
@article{tan_spiking_2020,
	title = {Spiking {Neural} {Networks}: {Background}, {Recent} {Development} and the {NeuCube} {Architecture}},
	volume = {52},
	issn = {1573-773X},
	shorttitle = {Spiking {Neural} {Networks}},
	url = {https://doi.org/10.1007/s11063-020-10322-8},
	doi = {10.1007/s11063-020-10322-8},
	abstract = {This paper reviews recent developments in the still-off-the-mainstream information and data processing area of spiking neural networks (SNN)—the third generation of artificial neural networks. We provide background information about the functioning of biological neurons, discussing the most important and commonly used mathematical neural models. Most relevant information processing techniques, learning algorithms, and applications of spiking neurons are described and discussed, focusing on feasibility and biological plausibility of the methods. Specifically, we describe in detail the functioning and organization of the latest version of a 3D spatio-temporal SNN-based data machine framework called NeuCube, as well as it’s SNN-related submodules. All described submodules are accompanied with formal algorithmic formulations. The architecture is highly relevant for the analysis and interpretation of various types of spatio-temporal brain data (STBD), like EEG, NIRS, fMRI, but we highlight some of the recent both STBD- and non-STBD-based applications. Finally, we summarise and discuss some open research problems that can be addressed in the future. These include, but are not limited to: application in the area of EEG-based BCI through transfer learning; application in the area of affective computing through the extension of the NeuCube framework which would allow for a biologically plausible SNN-based integration of central and peripheral nervous system measures. Matlab implementation of the NeuCube’s SNN-related module is available for research and teaching purposes.},
	language = {en},
	number = {2},
	urldate = {2021-10-01},
	journal = {Neural Processing Letters},
	author = {Tan, Clarence and Šarlija, Marko and Kasabov, Nikola},
	month = oct,
	year = {2020},
	pages = {1675--1701},
	file = {Springer Full Text PDF:/mnt/work-data/weverson/Zotero/storage/WF3XYKVN/Tan et al. - 2020 - Spiking Neural Networks Background, Recent Develo.pdf:application/pdf},
}

@inproceedings{kumarasinghe_espannet_2019,
	title = {{eSPANNet}: {Evolving} {Spike} {Pattern} {Association} {Neural} {Network} for {Spike}-based {Supervised} {Incremental} {Learning} and {Its} {Application} for {Single}-trial {Brain} {Computer} {Interfaces}},
	shorttitle = {{eSPANNet}},
	doi = {10.1109/IJCNN.2019.8852213},
	abstract = {Objective:Due to the non-stationarity and high trialto-trial variability, online event prediction from biomedical signals is challenging. This is more significant when it is applied to neurological rehabilitation where the person incrementally learns to regain the control of movement. eSPANNet is a computational model inspired by the incremental learning for motor control in living nervous systems. It is inspired by the concept of 'population vectors' which have been experimentally proven by several computational neuroscience studies. In this paper, we present a proof-of-concept study on the proposed computational model. Our goal is to utilize the polychronization effect of Spiking Neural Networks to develop a better neural decoder for Brain-Computer Interfaces. Methods: The eSPANNet model contains a network of Spike Pattern Association Neurons, a spiking neuron model which is able to emit spikes at the desired time-point. Results: The proposed approach was experimentally validated using the finger flexion prediction dataset from the fourth BCI competition. The results show that eSPANNet results in 1) a higher classification accuracy, sensitivity and F1 score compared to several other multi-class classifiers and, 2) a better approximation of the actual movement compared to several regression analysis based approaches. Conclusion and Significance: The novelty of our algorithm is the ability to learn which inputs to focus on in an online manner. We suggest that the eSPANNet is a better BCI decoder due to its i) incremental and life-long learning, ii) compatibility with the neuromorphic platforms and, iii) ability to address the non-stationarity of brain data.},
	booktitle = {2019 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Kumarasinghe, Kaushalya and Taylor, Denise and Kasabov, Nikola},
	month = jul,
	year = {2019},
	note = {ISSN: 2161-4407},
	keywords = {Biological neural networks, Brain modeling, Computational modeling, Neurons, Sociology, Statistics, Training},
	pages = {1--8},
	file = {IEEE Xplore Full Text PDF:/mnt/work-data/weverson/Zotero/storage/4I4DPVG4/Kumarasinghe et al. - 2019 - eSPANNet Evolving Spike Pattern Association Neura.pdf:application/pdf},
}

@article{kumarasinghe_brain-inspired_2021,
	title = {Brain-inspired spiking neural networks for decoding and understanding muscle activity and kinematics from electroencephalography signals during hand movements},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-81805-4},
	doi = {10.1038/s41598-021-81805-4},
	abstract = {Compared to the abilities of the animal brain, many Artificial Intelligence systems have limitations which emphasise the need for a Brain-Inspired Artificial Intelligence paradigm. This paper proposes a novel Brain-Inspired Spiking Neural Network (BI-SNN) model for incremental learning of spike sequences. BI-SNN maps spiking activity from input channels into a high dimensional source-space which enhances the evolution of polychronising spiking neural populations. We applied the BI-SNN to predict muscle activity and kinematics from electroencephalography signals during upper limb functional movements. The BI-SNN extends our previously proposed eSPANNet computational model by integrating it with the ‘NeuCube’ brain-inspired SNN architecture. We show that BI-SNN can successfully predict continuous muscle activity and kinematics of upper-limb. The experimental results confirmed that the BI-SNN resulted in strongly correlated population activity and demonstrated the feasibility for real-time prediction. In contrast to the majority of Brain–Computer Interfaces (BCIs) that constitute a ‘black box’, BI-SNN provide quantitative and visual feedback about the related brain activity. This study is one of the first attempts to examine the feasibility of finding neural correlates of muscle activity and kinematics from electroencephalography using a brain-inspired computational paradigm. The findings suggest that BI-SNN is a better neural decoder for non-invasive BCI.},
	language = {en},
	number = {1},
	urldate = {2021-10-01},
	journal = {Scientific Reports},
	author = {Kumarasinghe, Kaushalya and Kasabov, Nikola and Taylor, Denise},
	month = jan,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Brain–machine interface;Computer science;Electroencephalography – EEG;Learning algorithms;Network models;Neural decoding;Sensorimotor processing
Subject\_term\_id: brain-machine-interface;computer-science;electroencephalography-eeg;learning-algorithms;network-models;neural-decoding;sensorimotor-processing},
	pages = {2486},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/3327MBD9/Kumarasinghe et al. - 2021 - Brain-inspired spiking neural networks for decodin.pdf:application/pdf},
}

@article{lu_autonomous_2021,
	title = {An autonomous learning mobile robot using biological reward modulate {STDP}},
	volume = {458},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231221009310},
	doi = {10.1016/j.neucom.2021.06.027},
	abstract = {Recent studies have shown that biologically inspired Spiking Neural Networks (SNNs) has potentials for the mobile robot controls. Based on SNNs, an autonomous learning paradigm for controlling mobile robots is proposed in this work, which can learn specific tasks autonomously. A reward modulated spike-timing-dependent plasticity (R-STDP) learning algorithm is designed to aid implementing the autonomous learning paradigm. It can train the SNN under different environmental states and conditions. The obstacle avoidance in the synthetic and real environments is used as a robotic task example to verify the effectiveness of the proposed paradigm. Results show that the mobile robot can learn autonomously under different environmental conditions and is able to avoid obstacles after learning processes complete.},
	language = {en},
	urldate = {2021-10-01},
	journal = {Neurocomputing},
	author = {Lu, Hao and Liu, Junxiu and Luo, Yuling and Hua, Yifan and Qiu, Senhui and Huang, Yongchuang},
	month = oct,
	year = {2021},
	keywords = {Autonomous learning, Reinforcement learning, Reward, Spike-timing-dependent plasticity, Spiking neural networks},
	pages = {308--318},
	file = {Lu et al. - 2021 - An autonomous learning mobile robot using biologic.pdf:/mnt/work-data/weverson/Zotero/storage/PN277HHQ/Lu et al. - 2021 - An autonomous learning mobile robot using biologic.pdf:application/pdf},
}

@article{galanis_efficient_2021,
	title = {Efficient {Deployment} of {Spiking} {Neural} {Networks} on {SpiNNaker} {Neuromorphic} {Platform}},
	volume = {68},
	issn = {1558-3791},
	doi = {10.1109/TCSII.2020.3047425},
	abstract = {Spiking Neural Networks (SNNs) have emerged as serious competitors of the traditional Convolutional Neural Networks (CNNs), as they unlock new potential of implementing less complex and more energy efficient neural networks. Current deep CNNs can be converted to SNNs for fast deployment on neuromorphic devices, however existing methods do not investigate the impact of hardware-related parameters that directly affect the accuracy of an SNN. In this brief, we target the SpiNNaker neuromorphic platform and we demonstrate a fast exploration framework that effectively decides the configuration of the target board, in order to achieve the highest possible accuracy. Experimental results show that our method reaches 98.85\% SNN accuracy on MNIST dataset, while reducing the exploration time by a factor of \$3{\textbackslash}times \$ compared to exhaustive search.},
	number = {6},
	journal = {IEEE Transactions on Circuits and Systems II: Express Briefs},
	author = {Galanis, Ioannis and Anagnostopoulos, Iraklis and Nguyen, Chinh and Bares, Guillermo},
	month = jun,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Circuits and Systems II: Express Briefs},
	keywords = {Synapses, Biological neural networks, Neurons, Artificial neural networks, Computer architecture, neural network hardware, neuromorphics, Neuromorphics, Routing, Time factors},
	pages = {1937--1941},
	file = {IEEE Xplore Full Text PDF:/mnt/work-data/weverson/Zotero/storage/B7HQF8YI/Galanis et al. - 2021 - Efficient Deployment of Spiking Neural Networks on.pdf:application/pdf},
}

@article{furber_spinnaker_2014,
	title = {The {SpiNNaker} {Project}},
	volume = {102},
	issn = {1558-2256},
	doi = {10.1109/JPROC.2014.2304638},
	abstract = {The spiking neural network architecture (SpiNNaker) project aims to deliver a massively parallel million-core computer whose interconnect architecture is inspired by the connectivity characteristics of the mammalian brain, and which is suited to the modeling of large-scale spiking neural networks in biological real time. Specifically, the interconnect allows the transmission of a very large number of very small data packets, each conveying explicitly the source, and implicitly the time, of a single neural action potential or “spike.” In this paper, we review the current state of the project, which has already delivered systems with up to 2500 processors, and present the real-time event-driven programming model that supports flexible access to the resources of the machine and has enabled its use by a wide range of collaborators around the world.},
	number = {5},
	journal = {Proceedings of the IEEE},
	author = {Furber, Steve B. and Galluppi, Francesco and Temple, Steve and Plana, Luis A.},
	month = may,
	year = {2014},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Neuroscience, Brain modeling, Computational modeling, Computer architecture, neural network hardware, multicast algorithms, multiprocessor interconnection networks, Multitasking, Neural networks, parallel programming, Parallel programming, Program processors},
	pages = {652--665},
	file = {IEEE Xplore Full Text PDF:/mnt/work-data/weverson/Zotero/storage/RNHDAW8M/Furber et al. - 2014 - The SpiNNaker Project.pdf:application/pdf},
}

@inproceedings{indiveri_spike-based_2007,
	title = {Spike-based learning in {VLSI} networks of integrate-and-fire neurons},
	doi = {10.1109/ISCAS.2007.378290},
	abstract = {As the number of VLSI implementations of spike-based neural networks is steadily increasing, and the development of spike-based multi-chip systems is becoming more popular it is important to design spike-based learning algorithms and circuits, compatible with existing solutions, that endow these systems with adaptation and classification capabilities. We propose a spike-based learning algorithm that is highly effective in classifying complex patterns in semi-supervised fashion, and present neuromorphic circuits that support its VLSI implementation. We describe the architecture of a spike-based learning neural network, the analog circuits that implement the synaptic learning mechanism, and present results from a prototype VLSI chip comprising a full network of integrate-and-fire neurons and plastic synapses. We demonstrate how the VLSI circuits proposed reproduce the learning model's properties and fulfil its basic requirements for classifying complex patterns of mean firing rates.},
	booktitle = {2007 {IEEE} {International} {Symposium} on {Circuits} and {Systems}},
	author = {Indiveri, Giacomo and Fusi, Stefano},
	month = may,
	year = {2007},
	note = {ISSN: 2158-1525},
	keywords = {Timing, Neurons, Neural networks, Circuits, Large-scale systems, Learning systems, Protection, Sensor arrays, Silicon, Very large scale integration},
	pages = {3371--3374},
	file = {IEEE Xplore Full Text PDF:/mnt/work-data/weverson/Zotero/storage/DDMHHQVH/Indiveri e Fusi - 2007 - Spike-based learning in VLSI networks of integrate.pdf:application/pdf},
}

@article{huh_gradient_2017,
	title = {Gradient {Descent} for {Spiking} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1706.04698},
	abstract = {Much of studies on neural computation are based on network models of static neurons that produce analog output, despite the fact that information processing in the brain is predominantly carried out by dynamic neurons that produce discrete pulses called spikes. Research in spike-based computation has been impeded by the lack of efﬁcient supervised learning algorithm for spiking networks. Here, we present a gradient descent method for optimizing spiking network models by introducing a differentiable formulation of spiking networks and deriving the exact gradient calculation. For demonstration, we trained recurrent spiking networks on two dynamic tasks: one that requires optimizing fast (≈ millisecond) spike-based interactions for efﬁcient encoding of information, and a delayed-memory XOR task over extended duration (≈ second). The results show that our method indeed optimizes the spiking network dynamics on the time scale of individual spikes as well as the behavioral time scales. In conclusion, our result offers a general purpose supervised learning algorithm for spiking neural networks, thus advancing further investigations on spike-based computation.},
	language = {en},
	urldate = {2021-10-01},
	journal = {arXiv:1706.04698 [cs, q-bio, stat]},
	author = {Huh, Dongsung and Sejnowski, Terrence J.},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.04698},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
	file = {Huh e Sejnowski - 2017 - Gradient Descent for Spiking Neural Networks.pdf:/mnt/work-data/weverson/Zotero/storage/WTHYTYSC/Huh e Sejnowski - 2017 - Gradient Descent for Spiking Neural Networks.pdf:application/pdf},
}

@article{brette_philosophy_2015,
	title = {Philosophy of the {Spike}: {Rate}-{Based} vs. {Spike}-{Based} {Theories} of the {Brain}},
	volume = {9},
	issn = {1662-5137},
	shorttitle = {Philosophy of the {Spike}},
	url = {https://www.frontiersin.org/article/10.3389/fnsys.2015.00151},
	doi = {10.3389/fnsys.2015.00151},
	abstract = {Does the brain use a firing rate code or a spike timing code? Considering this controversial question from an epistemological perspective, I argue that progress has been hampered by its problematic phrasing. It takes the perspective of an external observer looking at whether those two observables vary with stimuli, and thereby misses the relevant question: which one has a causal role in neural activity? When rephrased in a more meaningful way, the rate-based view appears as an ad hoc methodological postulate, one that is practical but with virtually no empirical or theoretical support.},
	urldate = {2021-10-01},
	journal = {Frontiers in Systems Neuroscience},
	author = {Brette, Romain},
	year = {2015},
	pages = {151},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/XZCCFUA8/Brette - 2015 - Philosophy of the Spike Rate-Based vs. Spike-Base.pdf:application/pdf},
}

@article{teramae_optimal_2012,
	title = {Optimal spike-based communication in excitable networks with strong-sparse and weak-dense links},
	volume = {2},
	copyright = {2012 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/srep00485},
	doi = {10.1038/srep00485},
	abstract = {The connectivity of complex networks and functional implications has been attracting much interest in many physical, biological and social systems. However, the significance of the weight distributions of network links remains largely unknown except for uniformly- or Gaussian-weighted links. Here, we show analytically and numerically, that recurrent neural networks can robustly generate internal noise optimal for spike transmission between neurons with the help of a long-tailed distribution in the weights of recurrent connections. The structure of spontaneous activity in such networks involves weak-dense connections that redistribute excitatory activity over the network as noise sources to optimally enhance the responses of individual neurons to input at sparse-strong connections, thus opening multiple signal transmission pathways. Electrophysiological experiments confirm the importance of a highly broad connectivity spectrum supported by the model. Our results identify a simple network mechanism for internal noise generation by highly inhomogeneous connection strengths supporting both stability and optimal communication.},
	language = {en},
	number = {1},
	urldate = {2021-10-01},
	journal = {Scientific Reports},
	author = {Teramae, Jun-nosuke and Tsubo, Yasuhiro and Fukai, Tomoki},
	month = jul,
	year = {2012},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_y
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Physics;Statistical physics, thermodynamics and nonlinear dynamics;Synaptic transmission
Subject\_term\_id: physics;statistical-physics-thermodynamics-and-nonlinear-dynamics;synaptic-transmission},
	pages = {485},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/T6CHXXA5/Teramae et al. - 2012 - Optimal spike-based communication in excitable net.pdf:application/pdf},
}

@article{lee_enabling_2020,
	title = {Enabling {Spike}-{Based} {Backpropagation} for {Training} {Deep} {Neural} {Network} {Architectures}},
	volume = {14},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/article/10.3389/fnins.2020.00119},
	doi = {10.3389/fnins.2020.00119},
	abstract = {Spiking Neural Networks (SNNs) have recently emerged as a prominent neural computing paradigm. However, the typical shallow SNN architectures have limited capacity for expressing complex representations while training deep SNNs using input spikes has not been successful so far. Diverse methods have been proposed to get around this issue such as converting off-the-shelf trained deep Artificial Neural Networks (ANNs) to SNNs. However, the ANN-SNN conversion scheme fails to capture the temporal dynamics of a spiking system. On the other hand, it is still a difficult problem to directly train deep SNNs using input spike events due to the discontinuous, non-differentiable nature of the spike generation function. To overcome this problem, we propose an approximate derivative method that accounts for the leaky behavior of LIF neurons. This method enables training deep convolutional SNNs directly (with input spike events) using spike-based backpropagation. Our experiments show the effectiveness of the proposed spike-based learning on deep networks (VGG and Residual architectures) by achieving the best classification accuracies in MNIST, SVHN, and CIFAR-10 datasets compared to other SNNs trained with a spike-based learning. Moreover, we analyze sparse event-based computations to demonstrate the efficacy of the proposed SNN training method for inference operation in the spiking domain.},
	urldate = {2021-10-01},
	journal = {Frontiers in Neuroscience},
	author = {Lee, Chankyu and Sarwar, Syed Shakib and Panda, Priyadarshini and Srinivasan, Gopalakrishnan and Roy, Kaushik},
	year = {2020},
	pages = {119},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/U27JIEMK/Lee et al. - 2020 - Enabling Spike-Based Backpropagation for Training .pdf:application/pdf},
}

@article{thorpe_spike-based_2001,
	title = {Spike-based strategies for rapid processing},
	volume = {14},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608001000831},
	doi = {10.1016/S0893-6080(01)00083-1},
	abstract = {Most experimental and theoretical studies of brain function assume that neurons transmit information as a rate code, but recent studies on the speed of visual processing impose temporal constraints that appear incompatible with such a coding scheme. Other coding schemes that use the pattern of spikes across a population a neurons may be much more efficient. For example, since strongly activated neurons tend to fire first, one can use the order of firing as a code. We argue that Rank Order Coding is not only very efficient, but also easy to implement in biological hardware: neurons can be made sensitive to the order of activation of their inputs by including a feed-forward shunting inhibition mechanism that progressively desensitizes the neuronal population during a wave of afferent activity. In such a case, maximum activation will only be produced when the afferent inputs are activated in the order of their synaptic weights.},
	language = {en},
	number = {6},
	urldate = {2021-10-01},
	journal = {Neural Networks},
	author = {Thorpe, Simon and Delorme, Arnaud and Van Rullen, Rufin},
	month = jul,
	year = {2001},
	keywords = {Information, Latency, Rank Order Coding, Rapid visual processing, Retina, Spikes},
	pages = {715--725},
	file = {ScienceDirect Snapshot:/mnt/work-data/weverson/Zotero/storage/XMM4NWNP/S0893608001000831.html:text/html;ScienceDirect Full Text PDF:/mnt/work-data/weverson/Zotero/storage/JZHBCNAD/Thorpe et al. - 2001 - Spike-based strategies for rapid processing.pdf:application/pdf},
}

@article{kaya_large_2018,
	title = {A large electroencephalographic motor imagery dataset for electroencephalographic brain computer interfaces},
	volume = {5},
	copyright = {2018 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata2018211},
	doi = {10.1038/sdata.2018.211},
	abstract = {Recent advancements in brain computer interfaces (BCI) have demonstrated control of robotic systems by mental processes alone. Together with invasive BCI, electroencephalographic (EEG) BCI represent an important direction in the development of BCI systems. In the context of EEG BCI, the processing of EEG data is the key challenge. Unfortunately, advances in that direction have been complicated by a lack of large and uniform datasets that could be used to design and evaluate different data processing approaches. In this work, we release a large set of EEG BCI data collected during the development of a slow cortical potentials-based EEG BCI. The dataset contains 60 h of EEG recordings, 13 participants, 75 recording sessions, 201 individual EEG BCI interaction session-segments, and over 60 000 examples of motor imageries in 4 interaction paradigms. The current dataset presents one of the largest EEG BCI datasets publically available to date.},
	language = {en},
	number = {1},
	urldate = {2021-10-01},
	journal = {Scientific Data},
	author = {Kaya, Murat and Binli, Mustafa Kemal and Ozbay, Erkan and Yanar, Hilmi and Mishchenko, Yuriy},
	month = oct,
	year = {2018},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_publicdomain
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Biomedical engineering;Computational neuroscience
Subject\_term\_id: biomedical-engineering;computational-neuroscience},
	pages = {180211},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/WDWSQS7A/Kaya et al. - 2018 - A large electroencephalographic motor imagery data.pdf:application/pdf},
}

@article{okamoto_three-dimensional_2004,
	title = {Three-dimensional probabilistic anatomical cranio-cerebral correlation via the international 10–20 system oriented for transcranial functional brain mapping},
	volume = {21},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811903005366},
	doi = {10.1016/j.neuroimage.2003.08.026},
	abstract = {The recent advent of multichannel near-infrared spectroscopy (NIRS) has expanded its technical potential for human brain mapping. However, NIRS measurement has a technical drawback in that it measures cortical activities from the head surface without anatomical information of the object to be measured. This problem is also found in transcranial magnetic stimulation (TMS) that transcranially activates or inactivates the cortical surface. To overcome this drawback, we examined cranio-cerebral correlation using magnetic resonance imaging (MRI) via the guidance of the international 10–20 system for electrode placement, which had originally been developed for electroencephalography. We projected the 10–20 standard cranial positions over the cerebral cortical surface. After examining the cranio-cerebral correspondence for 17 healthy adults, we normalized the 10–20 cortical projection points of the subjects to the standard Montreal Neurological Institute (MNI) and Talairach stereotactic coordinates and obtained their probabilistic distributions. We also expressed the anatomical structures for the 10–20 cortical projection points probabilistically. Next, we examined the distance between the cortical surface and the head surface along the scalp and created a cortical surface depth map. We found that the locations of 10–20 cortical projection points in the standard MNI or Talairach space could be estimated with an average standard deviation of 8 mm. This study provided an initial step toward establishing a three-dimensional probabilistic anatomical platform that enables intra- and intermodal comparisons of NIRS and TMS brain imaging data.},
	language = {en},
	number = {1},
	urldate = {2021-10-01},
	journal = {NeuroImage},
	author = {Okamoto, Masako and Dan, Haruka and Sakamoto, Kuniko and Takeo, Kazuhiro and Shimizu, Koji and Kohno, Satoru and Oda, Ichiro and Isobe, Seiichiro and Suzuki, Tateo and Kohyama, Kaoru and Dan, Ippeita},
	month = jan,
	year = {2004},
	keywords = {Human brain mapping, Near-infrared spectroscopy, Probabilistic anatomical platform, Transcranial magnetic stimulation},
	pages = {99--111},
	file = {ScienceDirect Full Text PDF:/mnt/work-data/weverson/Zotero/storage/YLWKEJP5/Okamoto et al. - 2004 - Three-dimensional probabilistic anatomical cranio-.pdf:application/pdf;ScienceDirect Snapshot:/mnt/work-data/weverson/Zotero/storage/VZQJ9RWA/S1053811903005366.html:text/html},
}

@article{boerlin_spike-based_2011,
	title = {Spike-{Based} {Population} {Coding} and {Working} {Memory}},
	volume = {7},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1001080},
	doi = {10.1371/journal.pcbi.1001080},
	abstract = {Compelling behavioral evidence suggests that humans can make optimal decisions despite the uncertainty inherent in perceptual or motor tasks. A key question in neuroscience is how populations of spiking neurons can implement such probabilistic computations. In this article, we develop a comprehensive framework for optimal, spike-based sensory integration and working memory in a dynamic environment. We propose that probability distributions are inferred spike-per-spike in recurrently connected networks of integrate-and-fire neurons. As a result, these networks can combine sensory cues optimally, track the state of a time-varying stimulus and memorize accumulated evidence over periods much longer than the time constant of single neurons. Importantly, we propose that population responses and persistent working memory states represent entire probability distributions and not only single stimulus values. These memories are reflected by sustained, asynchronous patterns of activity which make relevant information available to downstream neurons within their short time window of integration. Model neurons act as predictive encoders, only firing spikes which account for new information that has not yet been signaled. Thus, spike times signal deterministically a prediction error, contrary to rate codes in which spike times are considered to be random samples of an underlying firing rate. As a consequence of this coding scheme, a multitude of spike patterns can reliably encode the same information. This results in weakly correlated, Poisson-like spike trains that are sensitive to initial conditions but robust to even high levels of external neural noise. This spike train variability reproduces the one observed in cortical sensory spike trains, but cannot be equated to noise. On the contrary, it is a consequence of optimal spike-based inference. In contrast, we show that rate-based models perform poorly when implemented with stochastically spiking neurons.},
	language = {en},
	number = {2},
	urldate = {2021-10-01},
	journal = {PLOS Computational Biology},
	author = {Boerlin, Martin and Denève, Sophie},
	month = feb,
	year = {2011},
	note = {Publisher: Public Library of Science},
	keywords = {Sensory perception, Memory, Working memory, Neurons, Neural networks, Action potentials, Neuronal tuning, Sensory cues},
	pages = {e1001080},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/YNGVFNJS/Boerlin e Denève - 2011 - Spike-Based Population Coding and Working Memory.pdf:application/pdf;Snapshot:/mnt/work-data/weverson/Zotero/storage/J3FNM6DA/article.html:text/html},
}

@article{tully_spike-based_2016,
	title = {Spike-{Based} {Bayesian}-{Hebbian} {Learning} of {Temporal} {Sequences}},
	volume = {12},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004954},
	doi = {10.1371/journal.pcbi.1004954},
	abstract = {Many cognitive and motor functions are enabled by the temporal representation and processing of stimuli, but it remains an open issue how neocortical microcircuits can reliably encode and replay such sequences of information. To better understand this, a modular attractor memory network is proposed in which meta-stable sequential attractor transitions are learned through changes to synaptic weights and intrinsic excitabilities via the spike-based Bayesian Confidence Propagation Neural Network (BCPNN) learning rule. We find that the formation of distributed memories, embodied by increased periods of firing in pools of excitatory neurons, together with asymmetrical associations between these distinct network states, can be acquired through plasticity. The model’s feasibility is demonstrated using simulations of adaptive exponential integrate-and-fire model neurons (AdEx). We show that the learning and speed of sequence replay depends on a confluence of biophysically relevant parameters including stimulus duration, level of background noise, ratio of synaptic currents, and strengths of short-term depression and adaptation. Moreover, sequence elements are shown to flexibly participate multiple times in the sequence, suggesting that spiking attractor networks of this type can support an efficient combinatorial code. The model provides a principled approach towards understanding how multiple interacting plasticity mechanisms can coordinate hetero-associative learning in unison.},
	language = {en},
	number = {5},
	urldate = {2021-10-01},
	journal = {PLOS Computational Biology},
	author = {Tully, Philip J. and Lindén, Henrik and Hennig, Matthias H. and Lansner, Anders},
	month = may,
	year = {2016},
	note = {Publisher: Public Library of Science},
	keywords = {Learning, Synapses, Neurons, Neural networks, Action potentials, Memory recall, Neuronal plasticity, Synaptic plasticity},
	pages = {e1004954},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/8LARUG83/Tully et al. - 2016 - Spike-Based Bayesian-Hebbian Learning of Temporal .pdf:application/pdf},
}

@article{heusser_episodic_2016,
	title = {Episodic sequence memory is supported by a theta–gamma phase code},
	volume = {19},
	copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn.4374},
	doi = {10.1038/nn.4374},
	abstract = {A core aspect of human episodic memory is the ability to recall events in the order that they were experienced. The authors found that successful memory for order is related to the precise timing of high frequency brain activity with respect to slower underlying rhythms.},
	language = {en},
	number = {10},
	urldate = {2021-10-01},
	journal = {Nature Neuroscience},
	author = {Heusser, Andrew C. and Poeppel, David and Ezzyat, Youssef and Davachi, Lila},
	month = oct,
	year = {2016},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 10
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Cognitive neuroscience;Learning and memory;Psychology
Subject\_term\_id: cognitive-neuroscience;learning-and-memory;psychology},
	pages = {1374--1380},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/HQ87DS3H/Heusser et al. - 2016 - Episodic sequence memory is supported by a theta–g.pdf:application/pdf},
}

@article{neymotin_ketamine_2011,
	title = {Ketamine {Disrupts} {Theta} {Modulation} of {Gamma} in a {Computer} {Model} of {Hippocampus}},
	volume = {31},
	copyright = {Copyright © 2011 the authors 0270-6474/11/3111733-11\$15.00/0},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/31/32/11733},
	doi = {10.1523/JNEUROSCI.0501-11.2011},
	abstract = {Abnormalities in oscillations have been suggested to play a role in schizophrenia. We studied theta-modulated gamma oscillations in a computer model of hippocampal CA3 in vivo with and without simulated application of ketamine, an NMDA receptor antagonist and psychotomimetic. Networks of 1200 multicompartment neurons [pyramidal, basket, and oriens-lacunosum moleculare (OLM) cells] generated theta and gamma oscillations from intrinsic network dynamics: basket cells primarily generated gamma and amplified theta, while OLM cells strongly contributed to theta. Extrinsic medial septal inputs paced theta and amplified both theta and gamma oscillations. Exploration of NMDA receptor reduction across all location combinations demonstrated that the experimentally observed ketamine effect occurred only with isolated reduction of NMDA receptors on OLMs. In the ketamine simulations, lower OLM activity reduced theta power and disinhibited pyramidal cells, resulting in increased basket cell activation and gamma power. Our simulations predict the following: (1) ketamine increases firing rates; (2) oscillations can be generated by intrinsic hippocampal circuits; (3) medial-septum inputs pace and augment oscillations; (4) pyramidal cells lead basket cells at the gamma peak but lag at trough; (5) basket cells amplify theta rhythms; (6) ketamine alters oscillations due to primary blockade at OLM NMDA receptors; (7) ketamine alters phase relationships of cell firing; (8) ketamine reduces network responsivity to the environment; (9) ketamine effect could be reversed by providing a continuous inward current to OLM cells. We suggest that this last prediction has implications for a possible novel treatment for cognitive deficits of schizophrenia by targeting OLM cells.},
	language = {en},
	number = {32},
	urldate = {2021-10-01},
	journal = {Journal of Neuroscience},
	author = {Neymotin, Samuel A. and Lazarewicz, Maciej T. and Sherif, Mohamed and Contreras, Diego and Finkel, Leif H. and Lytton, William W.},
	month = aug,
	year = {2011},
	pmid = {21832203},
	note = {Publisher: Society for Neuroscience
Section: Articles},
	pages = {11733--11743},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/2YJFAKU7/Neymotin et al. - 2011 - Ketamine Disrupts Theta Modulation of Gamma in a C.pdf:application/pdf},
}

@article{ratcliff_diffusion_2016,
	title = {Diffusion {Decision} {Model}: {Current} {Issues} and {History}},
	volume = {20},
	issn = {1364-6613},
	shorttitle = {Diffusion {Decision} {Model}},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661316000255},
	doi = {10.1016/j.tics.2016.01.007},
	abstract = {There is growing interest in diffusion models to represent the cognitive and neural processes of speeded decision making. Sequential-sampling models like the diffusion model have a long history in psychology. They view decision making as a process of noisy accumulation of evidence from a stimulus. The standard model assumes that evidence accumulates at a constant rate during the second or two it takes to make a decision. This process can be linked to the behaviors of populations of neurons and to theories of optimality. Diffusion models have been used successfully in a range of cognitive tasks and as psychometric tools in clinical research to examine individual differences. In this review, we relate the models to both earlier and more recent research in psychology.},
	language = {en},
	number = {4},
	urldate = {2021-10-01},
	journal = {Trends in Cognitive Sciences},
	author = {Ratcliff, Roger and Smith, Philip L. and Brown, Scott D. and McKoon, Gail},
	month = apr,
	year = {2016},
	keywords = {diffusion model, nonstationarity, optimality, response time},
	pages = {260--281},
	file = {ScienceDirect Full Text PDF:/mnt/work-data/weverson/Zotero/storage/X4M6I8J2/Ratcliff et al. - 2016 - Diffusion Decision Model Current Issues and Histo.pdf:application/pdf},
}

@article{herz_modeling_2006,
	title = {Modeling {Single}-{Neuron} {Dynamics} and {Computations}: {A} {Balance} of {Detail} and {Abstraction}},
	volume = {314},
	shorttitle = {Modeling {Single}-{Neuron} {Dynamics} and {Computations}},
	url = {https://www.science.org/doi/10.1126/science.1127240},
	doi = {10.1126/science.1127240},
	number = {5796},
	urldate = {2021-10-01},
	journal = {Science},
	author = {Herz, Andreas V. M. and Gollisch, Tim and Machens, Christian K. and Jaeger, Dieter},
	month = oct,
	year = {2006},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {80--85},
	file = {Herz et al. - 2006 - Modeling Single-Neuron Dynamics and Computations .pdf:/mnt/work-data/weverson/Zotero/storage/7LSPDVAL/Herz et al. - 2006 - Modeling Single-Neuron Dynamics and Computations .pdf:application/pdf},
}

@article{prat-ortega_flexible_2021,
	title = {Flexible categorization in perceptual decision making},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-21501-z},
	doi = {10.1038/s41467-021-21501-z},
	abstract = {Perceptual decisions rely on accumulating sensory evidence. This computation has been studied using either drift diffusion models or neurobiological network models exhibiting winner-take-all attractor dynamics. Although both models can account for a large amount of data, it remains unclear whether their dynamics are qualitatively equivalent. Here we show that in the attractor model, but not in the drift diffusion model, an increase in the stimulus fluctuations or the stimulus duration promotes transitions between decision states. The increase in the number of transitions leads to a crossover between weighting mostly early evidence (primacy) to weighting late evidence (recency), a prediction we validate with psychophysical data. Between these two limiting cases, we found a novel flexible categorization regime, in which fluctuations can reverse initially-incorrect categorizations. This reversal asymmetry results in a non-monotonic psychometric curve, a distinctive feature of the attractor model. Our findings point to correcting decision reversals as an important feature of perceptual decision making.},
	language = {en},
	number = {1},
	urldate = {2021-10-01},
	journal = {Nature Communications},
	author = {Prat-Ortega, Genís and Wimmer, Klaus and Roxin, Alex and de la Rocha, Jaime},
	month = feb,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Cognitive neuroscience;Computational neuroscience;Network models;Neural circuits;Sensory processing
Subject\_term\_id: cognitive-neuroscience;computational-neuroscience;network-models;neural-circuit;sensory-processing},
	pages = {1283},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/HPWDR37R/Prat-Ortega et al. - 2021 - Flexible categorization in perceptual decision mak.pdf:application/pdf},
}

@book{miller_introductory_2018,
	address = {Cambridge, MA},
	series = {Computational neuroscience series},
	title = {An {Introductory} {Course} in {Computational} {Neuroscience}},
	copyright = {2018 Massachussets Institute of Technology},
	isbn = {978-0-262-03825-6},
	url = {https://mitpress.ublish.com/book/intro-computational-neuroscience},
	language = {en},
	urldate = {2021-10-05},
	publisher = {The MIT Press},
	author = {Miller, Paul},
	year = {2018},
	file = {An Introductory Course in Computational Neuroscience | The MIT Press:/mnt/work-data/weverson/Zotero/storage/4WE73PLQ/intro-computational-neuroscience.html:text/html},
}

@book{adkins_ordinary_2012,
	address = {New York},
	edition = {1st ed. 2012},
	series = {Undergraduate {Texts} in {Mathematics}},
	title = {Ordinary {Differential} {Equations}},
	copyright = {Springer Science+Business Media, LLC, part of Springer Nature},
	isbn = {978-1-4614-3618-8},
	url = {https://www.springer.com/gp/book/9781461436171},
	abstract = {Unlike most texts in differential equations, this textbook gives an early presentation of the Laplace transform, which is then used to motivate and develop many of the remaining differential equation concepts for which it is particularly well suited. For example, the standard solution methods for constant coefficient linear differential equations are immediate and simplified, and solution methods for constant coefficient systems are streamlined. By introducing the Laplace transform early in the text, students become proficient in its use while at the same time learning the standard topics in differential equations. The text also includes proofs of several important theorems that are not usually given in introductory texts. These include a proof of the injectivity of the Laplace transform and a proof of the existence and uniqueness theorem for linear constant coefficient differential equations.Along with its unique traits, this text contains all the topics needed for a standard three- or four-hour, sophomore-level differential equations course for students majoring in science or engineering. These topics include: first order differential equations, general linear differential equations with constant coefficients, second order linear differential equations with variable coefficients, power series methods, and linear systems of differential equations. It is assumed that the reader has had the equivalent of a one-year course in college calculus.},
	language = {en},
	urldate = {2021-10-05},
	publisher = {Springer-Verlag},
	author = {Adkins, William and Davidson, Mark G.},
	year = {2012},
	doi = {10.1007/978-1-4614-3618-8},
}

@book{montgomery_estatistica_2018,
	address = {Rio de Janeiro},
	edition = {6},
	title = {Estatística aplicada e probabilidade para engenheiros},
	copyright = {LTC - Livros Técnicos e Científicos Editora Ltda.},
	isbn = {978-85-216-3241-2},
	language = {pt-br},
	publisher = {LTC},
	author = {Montgomery, Douglas C. and Runger, George C.},
	year = {2018},
}

@article{yuste_cortex_2005,
	title = {The cortex as a central pattern generator},
	volume = {6},
	copyright = {2005 Nature Publishing Group},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/nrn1686},
	doi = {10.1038/nrn1686},
	abstract = {Vertebrate spinal cord and brainstem central pattern generator (CPG) circuits share profound similarities with neocortical circuits. CPGs can produce meaningful functional output in the absence of sensory inputs. Neocortical circuits could be considered analogous to CPGs as they have rich spontaneous dynamics that, similar to CPGs, are powerfully modulated or engaged by sensory inputs, but can also generate output in their absence. We find compelling evidence for this argument at the anatomical, biophysical, developmental, dynamic and pathological levels of analysis. Although it is possible that cortical circuits are particularly plastic types of CPG ('learning CPGs'), we argue that present knowledge about CPGs is likely to foretell the basic principles of the organization and dynamic function of cortical circuits.},
	language = {en},
	number = {6},
	urldate = {2021-10-05},
	journal = {Nature Reviews Neuroscience},
	author = {Yuste, Rafael and MacLean, Jason N. and Smith, Jeffrey and Lansner, Anders},
	month = jun,
	year = {2005},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 6
Primary\_atype: Reviews
Publisher: Nature Publishing Group},
	pages = {477--483},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/VXKD82QN/Yuste et al. - 2005 - The cortex as a central pattern generator.pdf:application/pdf;Snapshot:/mnt/work-data/weverson/Zotero/storage/6PFR9FBZ/nrn1686.html:text/html},
}

@article{roy_towards_2019,
	title = {Towards spike-based machine intelligence with neuromorphic computing},
	volume = {575},
	copyright = {2019 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1677-2},
	doi = {10.1038/s41586-019-1677-2},
	abstract = {Guided by brain-like ‘spiking’ computational frameworks, neuromorphic computing—brain-inspired computing for machine intelligence—promises to realize artificial intelligence while reducing the energy requirements of computing platforms. This interdisciplinary field began with the implementation of silicon circuits for biological neural routines, but has evolved to encompass the hardware implementation of algorithms with spike-based encoding and event-driven representations. Here we provide an overview of the developments in neuromorphic computing for both algorithms and hardware and highlight the fundamentals of learning and hardware frameworks. We discuss the main challenges and the future prospects of neuromorphic computing, with emphasis on algorithm–hardware codesign.},
	language = {en},
	number = {7784},
	urldate = {2021-10-05},
	journal = {Nature},
	author = {Roy, Kaushik and Jaiswal, Akhilesh and Panda, Priyadarshini},
	month = nov,
	year = {2019},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 7784
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Electrical and electronic engineering;Nanoscience and technology
Subject\_term\_id: electrical-and-electronic-engineering;nanoscience-and-technology},
	pages = {607--617},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/GREZG9WP/Roy et al. - 2019 - Towards spike-based machine intelligence with neur.pdf:application/pdf;Snapshot:/mnt/work-data/weverson/Zotero/storage/5R8X9XAH/s41586-019-1677-2.html:text/html},
}

@article{syahid_simulation_2021,
	title = {Simulation of spiking activities neuron models using the {Euler} method},
	volume = {1951},
	issn = {1742-6596},
	url = {https://doi.org/10.1088/1742-6596/1951/1/012065},
	doi = {10.1088/1742-6596/1951/1/012065},
	abstract = {Simulation of neuron spiking activity models has been carried out using the Euler method. This study aims to simulate spiking activity in a neuron model. The neuron model used is the Hodgkin-Huxley neuron model, Integrate and Fire neuron model, Wilson neuron model, and Izhikevich neuron model. The research was conducted by implementing the mathematical equations of each neuron model used and then recording the membrane potential changes from time to time using the Euler method in MATLAB. The different forms of spiking activity were done by varying the variable’s value in each mathematical equation of a neuron model that describes the processing of action potentials (spikes) influenced by ion channel activity. The results showed that the Integrate and Fire neuron models produce regular spiking (RS), Hodgkin-Huxley neuron models have regular spiking (RS) forms, Wilson neuron models produce regular spiking (RS), fast-spiking (FS), and intrinsic bursting (IB), Izhikevich neuron model produces regular spiking (RS), fast-spiking (FS), intrinsic bursting (IB), chattering neurons (CH), and low threshold spiking (LTS). The complexity of the variables used and the spiking activity generated by each neuron model can provide an overview of computational efficiency and proximity to actual biological neurons.},
	language = {en},
	number = {1},
	urldate = {2021-10-05},
	journal = {Journal of Physics: Conference Series},
	author = {Syahid, A. and Yuniati, A.},
	month = jun,
	year = {2021},
	note = {Publisher: IOP Publishing},
	pages = {012065},
	file = {IOP Full Text PDF:/mnt/work-data/weverson/Zotero/storage/WXEHRCFW/Syahid e Yuniati - 2021 - Simulation of spiking activities neuron models usi.pdf:application/pdf},
}

@book{carnevale_neuron_2006,
	address = {Cambridge},
	title = {The {NEURON} {Book}},
	isbn = {978-0-521-84321-8},
	url = {https://www.cambridge.org/core/books/neuron-book/7C8D9BD861D288E658BEB652F593F273},
	abstract = {The authoritative reference on NEURON, the simulation environment for modeling biological neurons and neural networks that enjoys wide use in the experimental and computational neuroscience communities. This book shows how to use NEURON to construct and apply empirically based models. Written primarily for neuroscience investigators, teachers, and students, it assumes no previous knowledge of computer programming or numerical methods. Readers with a background in the physical sciences or mathematics, who have some knowledge about brain cells and circuits and are interested in computational modeling, will also find it helpful. The NEURON Book covers material that ranges from the inner workings of this program, to practical considerations involved in specifying the anatomical and biophysical properties that are to be represented in models. It uses a problem-solving approach, with many working examples that readers can try for themselves.},
	urldate = {2021-10-05},
	publisher = {Cambridge University Press},
	author = {Carnevale, Nicholas T. and Hines, Michael L.},
	year = {2006},
	doi = {10.1017/CBO9780511541612},
	file = {Snapshot:/mnt/work-data/weverson/Zotero/storage/UJPFJVKF/7C8D9BD861D288E658BEB652F593F273.html:text/html},
}

@article{stimberg_brian_2019,
	title = {Brian 2, an intuitive and efficient neural simulator},
	volume = {8},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.47314},
	doi = {10.7554/eLife.47314},
	abstract = {Brian 2 allows scientists to simply and efficiently simulate spiking neural network models. These models can feature novel dynamical equations, their interactions with the environment, and experimental protocols. To preserve high performance when defining new models, most simulators offer two options: low-level programming or description languages. The first option requires expertise, is prone to errors, and is problematic for reproducibility. The second option cannot describe all aspects of a computational experiment, such as the potentially complex logic of a stimulation protocol. Brian addresses these issues using runtime code generation. Scientists write code with simple and concise high-level descriptions, and Brian transforms them into efficient low-level code that can run interleaved with their code. We illustrate this with several challenging examples: a plastic model of the pyloric network, a closed-loop sensorimotor model, a programmatic exploration of a neuron model, and an auditory model with real-time input.},
	urldate = {2021-10-05},
	journal = {eLife},
	author = {Stimberg, Marcel and Brette, Romain and Goodman, Dan FM},
	editor = {Skinner, Frances K and Calabrese, Ronald L and Skinner, Frances K and Zeldenrust, Fleur and Gerkin, Richard C},
	month = aug,
	year = {2019},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {computational neuroscience, simulation, software},
	pages = {e47314},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/48RJHJ9E/Stimberg et al. - 2019 - Brian 2, an intuitive and efficient neural simulat.pdf:application/pdf},
}

@article{virgilio_g_spiking_2020,
	title = {Spiking {Neural} {Networks} applied to the classification of motor tasks in {EEG} signals},
	volume = {122},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608019303193},
	doi = {10.1016/j.neunet.2019.09.037},
	abstract = {Motivated by the recent progress of Spiking Neural Network (SNN) models in pattern recognition, we report on the development and evaluation of brain signal classifiers based on SNNs. The work shows the capabilities of this type of Spiking Neurons in the recognition of motor imagery tasks from EEG signals and compares their performance with other traditional classifiers commonly used in this application. This work includes two stages: the first stage consists of comparing the performance of the SNN models against some traditional neural network models. The second stage, compares the SNN models performance in two input conditions: input features with constant values and input features with temporal information. The EEG signals employed in this work represent five motor imagery tasks: i.e. rest, left hand, right hand, foot and tongue movements. These EEG signals were obtained from a public database provided by the Technological University of Graz (Brunner et al., 2008). The feature extraction stage was performed by applying two algorithms: power spectral density and wavelet decomposition. Likewise, this work uses raw EEG signals for the second stage of the problem solution. All of the models were evaluated in the classification between two motor imagery tasks. This work demonstrates that with a smaller number of Spiking neurons, simple problems can be solved. Better results are obtained by using patterns with temporal information, thereby exploiting the capabilities of the SNNs.},
	language = {en},
	urldate = {2021-10-11},
	journal = {Neural Networks},
	author = {Virgilio G., Carlos D. and Sossa A., Juan H. and Antelis, Javier M. and Falcón, Luis E.},
	month = feb,
	year = {2020},
	keywords = {EEG signals, Izhikevich model, Motor imagery, Power Spectral Density, Spiking Neural Network, Wavelet Decomposition},
	pages = {130--143},
	file = {ScienceDirect Snapshot:/mnt/work-data/weverson/Zotero/storage/G3CAWEIB/S0893608019303193.html:text/html},
}

@article{crisp_models_2019,
	title = {Models for {Spiking} {Neurons}: {Integrate}-and-{Fire} {Units} and {Relaxation} {Oscillators}},
	volume = {17},
	issn = {1544-2896},
	shorttitle = {Models for {Spiking} {Neurons}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6650253/},
	abstract = {Relaxation oscillators are nonlinear electronic circuits that produce a repetitive non-sinusoidal waveform when sufficient voltage is applied. In this fashion, they are reminiscent of integrate-and-fire neuron models, except that they also include components with hysteresis, and thus require no threshold rule to determine when an impulse has occurred or to return the voltage to its reset value. Here, I discuss the pros and cons of teaching elementary neurophysiology using first-order linear integrate-and-fire neurons versus relaxation oscillator circuits. I suggest that the shortcomings of both types of models are useful in order to foster a critical understanding of the neurophysiology underlying the firing dynamics of biological neurons.},
	number = {2},
	urldate = {2021-10-11},
	journal = {Journal of Undergraduate Neuroscience Education},
	author = {Crisp, Kevin},
	month = jun,
	year = {2019},
	pmid = {31360134},
	pmcid = {PMC6650253},
	pages = {E7--E12},
	file = {PubMed Central Full Text PDF:/mnt/work-data/weverson/Zotero/storage/DE2Y2JKP/Crisp - 2019 - Models for Spiking Neurons Integrate-and-Fire Uni.pdf:application/pdf},
}

@book{borges_python_2014,
	address = {São Paulo},
	title = {Python para {Desenvolvedores}: {Aborda} {Python} 3.3},
	copyright = {Novatec Editora Ltda.},
	isbn = {978-85-7522-405-2},
	url = {https://novatec.com.br/livros/python-para-desenvolvedores/},
	abstract = {Entenda os principais recursos da linguagem de programação Python, desde os conceitos mais básicos até recursos avançados, como a integração com outras linguagens. Indicado para desenvolvedores que já têm experiência e desejam um curso rápido e prático de Python sem ter de consultar volumosos livros e manuais.},
	language = {pt-br},
	urldate = {2021-10-11},
	publisher = {Novatec Editora},
	author = {Borges, Luiz Eduardo},
	year = {2014},
	file = {Snapshot:/mnt/work-data/weverson/Zotero/storage/Q2V5VKK7/python-para-desenvolvedores.html:text/html},
}

@article{wang_computational_2020,
	title = {Computational neuroscience: a frontier of the 21st century},
	volume = {7},
	issn = {2095-5138},
	shorttitle = {Computational neuroscience},
	url = {https://doi.org/10.1093/nsr/nwaa129},
	doi = {10.1093/nsr/nwaa129},
	number = {9},
	urldate = {2021-10-11},
	journal = {National Science Review},
	author = {Wang, Xiao-Jing and Hu, Hailan and Huang, Chengcheng and Kennedy, Henry and Li, Chengyu Tony and Logothetis, Nikos and Lu, Zhong-Lin and Luo, Qingming and Poo, Mu-ming and Tsao, Doris and Wu, Si and Wu, Zhaohui and Zhang, Xu and Zhou, Douglas},
	month = sep,
	year = {2020},
	pages = {1418--1422},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/KAD5UVQU/Wang et al. - 2020 - Computational neuroscience a frontier of the 21st.pdf:application/pdf;Snapshot:/mnt/work-data/weverson/Zotero/storage/IYUKCXQ5/5856589.html:text/html},
}

@article{nordlie_towards_2009,
	title = {Towards {Reproducible} {Descriptions} of {Neuronal} {Network} {Models}},
	volume = {5},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000456},
	doi = {10.1371/journal.pcbi.1000456},
	abstract = {Progress in science depends on the effective exchange of ideas among scientists. New ideas can be assessed and criticized in a meaningful manner only if they are formulated precisely. This applies to simulation studies as well as to experiments and theories. But after more than 50 years of neuronal network simulations, we still lack a clear and common understanding of the role of computational models in neuroscience as well as established practices for describing network models in publications. This hinders the critical evaluation of network models as well as their re-use. We analyze here 14 research papers proposing neuronal network models of different complexity and find widely varying approaches to model descriptions, with regard to both the means of description and the ordering and placement of material. We further observe great variation in the graphical representation of networks and the notation used in equations. Based on our observations, we propose a good model description practice, composed of guidelines for the organization of publications, a checklist for model descriptions, templates for tables presenting model structure, and guidelines for diagrams of networks. The main purpose of this good practice is to trigger a debate about the communication of neuronal network models in a manner comprehensible to humans, as opposed to machine-readable model description languages. We believe that the good model description practice proposed here, together with a number of other recent initiatives on data-, model-, and software-sharing, may lead to a deeper and more fruitful exchange of ideas among computational neuroscientists in years to come. We further hope that work on standardized ways of describing—and thinking about—complex neuronal networks will lead the scientific community to a clearer understanding of high-level concepts in network dynamics, and will thus lead to deeper insights into the function of the brain.},
	language = {en},
	number = {8},
	urldate = {2021-10-11},
	journal = {PLOS Computational Biology},
	author = {Nordlie, Eilen and Gewaltig, Marc-Oliver and Plesser, Hans Ekkehard},
	month = aug,
	year = {2009},
	note = {Publisher: Public Library of Science},
	keywords = {Synapses, Neurons, Neural networks, Computational neuroscience, Computer software, Network analysis, Scientists, Simulation and modeling},
	pages = {e1000456},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/QIAAMBCZ/Nordlie et al. - 2009 - Towards Reproducible Descriptions of Neuronal Netw.pdf:application/pdf;Snapshot:/mnt/work-data/weverson/Zotero/storage/NHYZJG5E/article.html:text/html},
}

@article{sandve_ten_2013,
	title = {Ten {Simple} {Rules} for {Reproducible} {Computational} {Research}},
	volume = {9},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003285},
	doi = {10.1371/journal.pcbi.1003285},
	language = {en},
	number = {10},
	urldate = {2021-10-11},
	journal = {PLOS Computational Biology},
	author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
	month = oct,
	year = {2013},
	note = {Publisher: Public Library of Science},
	keywords = {Archives, Computer and information sciences, Computer applications, Genome analysis, Habits, Replication studies, Reproducibility, Source code},
	pages = {e1003285},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/4Q3ZR4WH/Sandve et al. - 2013 - Ten Simple Rules for Reproducible Computational Re.pdf:application/pdf;Snapshot:/mnt/work-data/weverson/Zotero/storage/PF36H59Y/article.html:text/html},
}

@article{shen_interactive_2014,
	title = {Interactive notebooks: {Sharing} the code},
	volume = {515},
	copyright = {2014 Nature Publishing Group},
	issn = {1476-4687},
	shorttitle = {Interactive notebooks},
	url = {https://www.nature.com/articles/515151a},
	doi = {10.1038/515151a},
	abstract = {The free IPython notebook makes data analysis easier to record, understand and reproduce.},
	language = {en},
	number = {7525},
	urldate = {2021-10-11},
	journal = {Nature},
	author = {Shen, Helen},
	month = nov,
	year = {2014},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 7525
Primary\_atype: Special Features
Publisher: Nature Publishing Group
Subject\_term: Communication;Computational biology and bioinformatics;Information technology;Publishing
Subject\_term\_id: communication;computational-biology-and-bioinformatics;information-technology;publishing},
	pages = {151--152},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/J8HE5QA7/Shen - 2014 - Interactive notebooks Sharing the code.pdf:application/pdf},
}

@article{izhikevich_simple_2003,
	title = {Simple model of spiking neurons},
	volume = {14},
	issn = {1941-0093},
	doi = {10.1109/TNN.2003.820440},
	abstract = {A model is presented that reproduces spiking and bursting behavior of known types of cortical neurons. The model combines the biologically plausibility of Hodgkin-Huxley-type dynamics and the computational efficiency of integrate-and-fire neurons. Using this model, one can simulate tens of thousands of spiking cortical neurons in real time (1 ms resolution) using a desktop PC.},
	number = {6},
	journal = {IEEE Transactions on Neural Networks},
	author = {Izhikevich, E.M.},
	month = nov,
	year = {2003},
	note = {Conference Name: IEEE Transactions on Neural Networks},
	keywords = {Brain modeling, Computational modeling, Neurons, Large-scale systems, Bifurcation, Biological system modeling, Biology computing, Biomembranes, Mathematical analysis, Mathematical model},
	pages = {1569--1572},
	file = {IEEE Xplore Full Text PDF:/mnt/work-data/weverson/Zotero/storage/QHM9EXTV/Izhikevich - 2003 - Simple model of spiking neurons.pdf:application/pdf;IEEE Xplore Abstract Record:/mnt/work-data/weverson/Zotero/storage/UZ4LWWT7/1257420.html:text/html},
}

@article{saxe_if_2021,
	title = {If deep learning is the answer, what is the question?},
	volume = {22},
	copyright = {2020 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/s41583-020-00395-8},
	doi = {10.1038/s41583-020-00395-8},
	abstract = {Neuroscience research is undergoing a minor revolution. Recent advances in machine learning and artificial intelligence research have opened up new ways of thinking about neural computation. Many researchers are excited by the possibility that deep neural networks may offer theories of perception, cognition and action for biological brains. This approach has the potential to radically reshape our approach to understanding neural systems, because the computations performed by deep networks are learned from experience, and not endowed by the researcher. If so, how can neuroscientists use deep networks to model and understand biological brains? What is the outlook for neuroscientists who seek to characterize computations or neural codes, or who wish to understand perception, attention, memory and executive functions? In this Perspective, our goal is to offer a road map for systems neuroscience research in the age of deep learning. We discuss the conceptual and methodological challenges of comparing behaviour, learning dynamics and neural representations in artificial and biological systems, and we highlight new research questions that have emerged for neuroscience as a direct consequence of recent advances in machine learning.},
	language = {en},
	number = {1},
	urldate = {2021-10-19},
	journal = {Nature Reviews Neuroscience},
	author = {Saxe, Andrew and Nelli, Stephanie and Summerfield, Christopher},
	month = jan,
	year = {2021},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Learning algorithms;Network models
Subject\_term\_id: learning-algorithms;network-models},
	pages = {55--67},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/8BRQTZPZ/Saxe et al. - 2021 - If deep learning is the answer, what is the questi.pdf:application/pdf;Snapshot:/mnt/work-data/weverson/Zotero/storage/PDZLDJ5N/s41583-020-00395-8.html:text/html},
}

@article{tian_neural_2021,
	title = {A neural architecture search based framework for liquid state machine design},
	volume = {443},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231221003325},
	doi = {10.1016/j.neucom.2021.02.076},
	abstract = {Liquid state machines (LSMs), also known as the recurrent version of spiking neural networks, have garnered significant research interest owing to their high computational power, biological plausibility, simple structure, and low training complexity. By exploring the design space in network architectures and parameters, recent works have demonstrated the great potential for improving the accuracy of LSM models with low complexity. However, these works are based on manually defined network architectures or predefined parameters, which may ignore the potential optimization of the architectures and parameters of LSMs. In this study, we propose a neural architecture search-based framework to explore the architecture and parameter design space for the automatic dataset-oriented LSM models. To manage the exponentially increasing design space, we adopt a three-step search for LSMs, including dynamic multiple-liquid architecture search in multiple layers, variations in the number of neurons in each liquid, and parameter search such as percentage connectivity and excitatory neuron ratio within each liquid. In addition, we propose the use of a simulated annealing algorithm to implement three-step heuristic search. Two datasets, including the image dataset of NMNIST and speech dataset of FSDD, were used to test the effectiveness of the proposed framework. Simulation results demonstrated that our framework can produce the dataset-oriented optimal LSM models with high accuracy and low complexity. The best classification accuracy on the two datasets with only 1000 spiking neurons was observed to be 92.5\% and 84\%. Meanwhile, the network connections of discovered optimal multiple-liquid LSM models for the two datasets, on average, were reduced by 56.3\% and 60.2\% separately compared with a single LSM. Furthermore, the total number of neurons in the optimal multiple-liquid LSM models on the two datasets was reduced by 20\% with an accuracy loss of only 0.5\%.},
	language = {en},
	urldate = {2021-10-19},
	journal = {Neurocomputing},
	author = {Tian, Shuo and Qu, Lianhua and Wang, Lei and Hu, Kai and Li, Nan and Xu, Weixia},
	month = jul,
	year = {2021},
	keywords = {Design space, Heuristic search, Liquid state machine, Neural architecture search, Spiking neural network},
	pages = {174--182},
	file = {Versão submetida:/mnt/work-data/weverson/Zotero/storage/5LXD2YWC/Tian et al. - 2021 - A neural architecture search based framework for l.pdf:application/pdf},
}

@book{dayan_theoretical_2001,
	address = {Cambridge, Mass},
	series = {Computational neuroscience},
	title = {Theoretical neuroscience: computational and mathematical modeling of neural systems},
	isbn = {978-0-262-04199-7},
	shorttitle = {Theoretical neuroscience},
	publisher = {Massachusetts Institute of Technology Press},
	author = {Dayan, Peter and Abbott, L. F.},
	year = {2001},
	keywords = {Computational neuroscience, Computer simulation, Human information processing, Neural networks (Neurobiology)},
}

@book{gerstner_neuronal_2014,
	address = {Cambridge, United Kingdom},
	title = {Neuronal dynamics: from single neurons to networks and models of cognition},
	isbn = {978-1-107-06083-8 978-1-107-63519-7},
	shorttitle = {Neuronal dynamics},
	publisher = {Cambridge University Press},
	author = {Gerstner, Wulfram and Kistler, Werner M. and Naud, Richard and Paninski, Liam},
	year = {2014},
	keywords = {Neural networks (Neurobiology), Cognitive neuroscience, Neurobiology},
}

@article{lopez-osorio_neuromorphic_2021,
	title = {Neuromorphic adaptive spiking {CPG} towards bio-inspired locomotion of legged robots},
	url = {http://arxiv.org/abs/2101.09709},
	abstract = {In recent years, locomotion mechanisms exhibited by vertebrate animals have been the inspiration for the improvement in the performance of robotic systems. These mechanisms include the adaptability of their locomotion to any change registered in the environment through their biological sensors. In this regard, we aim to replicate such kind of adaptability in legged robots through a Spiking Central Pattern Generator. This Spiking Central Pattern Generator generates different locomotion (rhythmic) patterns which are driven by an external stimulus, that is, the output of a Force Sensitive Resistor connected to the robot to provide feedback. The Spiking Central Pattern Generator consists of a network of five populations of Leaky Integrate-and-Fire neurons designed with a specific topology in such a way that the rhythmic patterns can be generated and driven by the aforementioned external stimulus. Therefore, the locomotion of the end robotic platform (any-legged robot) can be adapted to the terrain by using any sensor as input. The Spiking Central Pattern Generator with adaptive learning has been numerically validated at software and hardware level, using the Brian 2 simulator and the SpiNNaker neuromorphic platform for the latest. In particular, our experiments clearly show an adaptation in the oscillation frequencies between the spikes produced in the populations of the Spiking Central Pattern Generator while the input stimulus varies. To validate the robustness and adaptability of the Spiking Central Pattern Generator, we have performed several tests by variating the output of the sensor. These experiments were carried out in Brian 2 and SpiNNaker; both implementations showed a similar behavior with a Pearson correlation coefficient of 0.905.},
	urldate = {2021-10-21},
	journal = {arXiv:2101.09709 [cs]},
	author = {Lopez-Osorio, Pablo and Patino-Saucedo, Alberto and Dominguez-Morales, Juan P. and Rostro-Gonzalez, Horacio and Perez-Peña, Fernando},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.09709
version: 1},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/mnt/work-data/weverson/Zotero/storage/L2T8M5EZ/Lopez-Osorio et al. - 2021 - Neuromorphic adaptive spiking CPG towards bio-insp.pdf:application/pdf;arXiv.org Snapshot:/mnt/work-data/weverson/Zotero/storage/9N32HCNU/2101.html:text/html},
}

@article{koulakov_model_2002,
	title = {Model for a robust neural integrator},
	volume = {5},
	copyright = {2002 Nature Publishing Group},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn893},
	doi = {10.1038/nn893},
	abstract = {Integrator circuits in the brain show persistent firing that reflects the sum of previous excitatory and inhibitory inputs from external sources. Integrator circuits have been implicated in parametric working memory, decision making and motor control. Previous work has shown that stable integrator function can be achieved by an excitatory recurrent neural circuit, provided synaptic strengths are tuned with extreme precision (better than 1\% accuracy). Here we show that integrator circuits can function without fine tuning if the neuronal units have bistable properties. Two specific mechanisms of bistability are analyzed, one based on local recurrent excitation, and the other on the voltage-dependence of the NMDA (N-methyl-D-aspartate) channel. Neither circuit requires fine tuning to perform robust integration, and the latter actually exploits the variability of neuronal conductances.},
	language = {en},
	number = {8},
	urldate = {2021-10-21},
	journal = {Nature Neuroscience},
	author = {Koulakov, Alexei A. and Raghavachari, Sridhar and Kepecs, Adam and Lisman, John E.},
	month = aug,
	year = {2002},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 8
Primary\_atype: Research
Publisher: Nature Publishing Group},
	pages = {775--782},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/H2DGA5E2/Koulakov et al. - 2002 - Model for a robust neural integrator.pdf:application/pdf},
}

@article{lillicrap_backpropagation_2020,
	title = {Backpropagation and the brain},
	volume = {21},
	copyright = {2020 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/s41583-020-0277-3},
	doi = {10.1038/s41583-020-0277-3},
	abstract = {During learning, the brain modifies synapses to improve behaviour. In the cortex, synapses are embedded within multilayered networks, making it difficult to determine the effect of an individual synaptic modification on the behaviour of the system. The backpropagation algorithm solves this problem in deep artificial neural networks, but historically it has been viewed as biologically problematic. Nonetheless, recent developments in neuroscience and the successes of artificial neural networks have reinvigorated interest in whether backpropagation offers insights for understanding learning in the cortex. The backpropagation algorithm learns quickly by computing synaptic updates using feedback connections to deliver error signals. Although feedback connections are ubiquitous in the cortex, it is difficult to see how they could deliver the error signals required by strict formulations of backpropagation. Here we build on past and recent developments to argue that feedback connections may instead induce neural activities whose differences can be used to locally approximate these signals and hence drive effective learning in deep networks in the brain.},
	language = {en},
	number = {6},
	urldate = {2021-10-21},
	journal = {Nature Reviews Neuroscience},
	author = {Lillicrap, Timothy P. and Santoro, Adam and Marris, Luke and Akerman, Colin J. and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 6
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Cortex;Learning algorithms;Long-term potentiation;Network models;Neurophysiology
Subject\_term\_id: cortex;learning-algorithms;long-term-potentiation;network-models;neurophysiology},
	pages = {335--346},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/2CFBZD2J/Lillicrap et al. - 2020 - Backpropagation and the brain.pdf:application/pdf},
}

@incollection{graham_modeling_2013,
	address = {New York, NY},
	title = {Modeling the {Axon}},
	isbn = {978-1-4614-7320-6},
	url = {https://doi.org/10.1007/978-1-4614-7320-6_241-1},
	language = {en},
	urldate = {2021-10-21},
	booktitle = {Encyclopedia of {Computational} {Neuroscience}},
	publisher = {Springer},
	author = {Graham, Bruce},
	editor = {Jaeger, Dieter and Jung, Ranu},
	year = {2013},
	doi = {10.1007/978-1-4614-7320-6_241-1},
	keywords = {Action Potential Propagation, Branch Point, Daughter Branch, Myelinated Axon, Sodium Current},
	pages = {1--9},
	file = {Graham - 2013 - Modeling the Axon.pdf:/mnt/work-data/weverson/Zotero/storage/Q3HJ48Z9/Graham - 2013 - Modeling the Axon.pdf:application/pdf},
}

@book{wasserman_all_2010,
	address = {New York Berlin Heidelberg},
	edition = {Corr. 2. print., [repr.]},
	series = {Springer texts in statistics},
	title = {All of statistics: a concise course in statistical inference},
	isbn = {978-0-387-21736-9 978-1-4419-2322-6},
	shorttitle = {All of statistics},
	language = {eng},
	publisher = {Springer},
	author = {Wasserman, Larry},
	year = {2010},
}

@book{forsyth_probability_2018,
	address = {Cham},
	edition = {1st ed. 2018},
	title = {Probability and {Statistics} for {Computer} {Science}},
	isbn = {978-3-319-64410-3},
	abstract = {This textbook is aimed at computer science undergraduates late in sophomore or early in junior year, supplying a comprehensive background in qualitative and quantitative data analysis, probability, random variables, and statistical methods, including machine learning. With careful treatment of topics that fill the curricular needs for the course, Probability and Statistics for Computer Science features: - A treatment of random variables and expectations dealing primarily with the discrete case. - A practical treatment of simulation, showing how many interesting probabilities and expectations can be extracted, with particular emphasis on Markov chains. - A clear but crisp account of simple point inference strategies (maximum likelihood; Bayesian inference) in simple contexts. This is extended to cover some confidence intervals, samples and populations for random sampling with replacement, and the simplest hypothesis testing. - A chapter dealing with classification, explaining why it's useful; how to train SVM classifiers with stochastic gradient descent; and how to use implementations of more advanced methods such as random forests and nearest neighbors. - A chapter dealing with regression, explaining how to set up, use and understand linear regression and nearest neighbors regression in practical problems. - A chapter dealing with principal components analysis, developing intuition carefully, and including numerous practical examples. There is a brief description of multivariate scaling via principal coordinate analysis. - A chapter dealing with clustering via agglomerative methods and k-means, showing how to build vector quantized features for complex signals. Illustrated throughout, each main chapter includes many worked examples and other pedagogical elements such as boxed Procedures, Definitions, Useful Facts, and Remember This (short tips). Problems and Programming Exercises are at the end of each chapter, with a summary of what the reader should know. Instructor resources include a full set of model solutions for all problems, and an Instructor's Manual with accompanying presentation slides},
	publisher = {Springer International Publishing : Imprint: Springer},
	author = {Forsyth, David},
	year = {2018},
	doi = {10.1007/978-3-319-64410-3},
	keywords = {Statistics, Computer simulation, Mathematical statistics, Probability and Statistics in Computer Science, Statistics and Computing/Statistics Programs, Simulation and Modeling},
}

@book{linge_programming_2020,
	address = {Cham},
	edition = {2nd ed.},
	series = {Texts in {Computational} {Science} and {Engineering}},
	title = {Programming for computations - {Python}: a gentle introduction to numerical simulations with {Python}},
	isbn = {978-3-030-16876-6},
	shorttitle = {Programming for computations - {Python}},
	language = {eng},
	number = {15},
	publisher = {Springer},
	author = {Linge, Svein and Langtangen, Hans Petter},
	year = {2020},
}

@book{illowsky_introductory_2018,
	title = {Introductory statistics},
	isbn = {978-1-938168-20-8},
	url = {https://openstax.org/details/books/introductory-statistics},
	abstract = {Introductory Statistics is designed for the one-semester, introduction to statistics course and is geared toward students majoring in fields other than math or engineering. This text assumes students have been exposed to intermediate algebra, and it focuses on the applications of statistical knowledge rather than the theory behind it. The foundation of this textbook is Collaborative Statistics, by Barbara Illowsky and Susan Dean.--Provided by publisher.},
	language = {English},
	urldate = {2021-10-28},
	publisher = {OpenStax},
	author = {Illowsky, Barbara and Dean, Susan},
	year = {2018},
	note = {OCLC: 1154530098},
}

@book{ermentrout_mathematical_2010,
	address = {New York, NY},
	series = {Interdisciplinary applied mathematics},
	title = {Mathematical foundations of neuroscience},
	volume = {35},
	isbn = {978-0-387-87708-2},
	abstract = {This book applies methods from nonlinear dynamics to problems in neuroscience. It uses modern mathematical approaches to understand patterns of neuronal activity seen in experiments and models of neuronal behavior. The intended audience is researchers interested in applying mathematics to important problems in neuroscience, and neuroscientists who would like to understand how to create models, as well as the mathematical and computational methods for analyzing them. The authors take a very broad approach and use many different methods to solve and understand complex models of neurons and circuits. They explain and combine numerical, analytical, dynamical systems and perturbation methods to produce a modern approach to the types of model equations that arise in neuroscience. There are extensive chapters on the role of noise, multiple time scales and spatial interactions in generating complex activity patterns found in experiments ; The early chapters require little more than basic calculus and some elementary differential equations and can form the core of a computational neuroscience course. Later chapters can be used as a basis for a graduate class and as a source for current research in mathematical neuroscience ; The book contains a large number of illustrations, chapter summaries and hundreds of exercises which are motivated by issues that arise in biology, and involve both computation and analysis. --Book Jacket},
	language = {eng},
	publisher = {Springer},
	author = {Ermentrout, Bard and Terman, David Hillel},
	year = {2010},
}

@book{grun_analysis_2010,
	address = {New York},
	series = {Springer series in computational neuroscience},
	title = {Analysis of parallel spike trains},
	volume = {7},
	isbn = {978-1-4419-5674-3 978-1-4419-5675-0},
	publisher = {Springer},
	author = {Grün, Sonja and Rotter, Stefan},
	year = {2010},
	note = {OCLC: ocn528411076},
	keywords = {Models, Neurological, Electrophysiology, Computational neuroscience, Action Potentials, Methodology, methods, physiology},
}

@book{walrand_probability_2021,
	edition = {1st ed.},
	title = {Probability in {Electrical} {Engineering} and {Computer} {Science} {An} {Application}-{Driven} {Course}},
	isbn = {978-3-030-49995-2},
	url = {https://doi.org/10.1007/978-3-030-49995-2},
	abstract = {This revised textbook motivates and illustrates the techniques of applied probability by applications in electrical engineering and computer science (EECS). The author presents information processing and communication systems that use algorithms based on probabilistic models and techniques, including web searches, digital links, speech recognition, GPS, route planning, recommendation systems, classification, and estimation. He then explains how these applications work and, along the way, provides the readers with the understanding of the key concepts and methods of applied probability. Python labs enable the readers to experiment and consolidate their understanding. The book includes homework, solutions, and Jupyter notebooks. This edition includes new topics such as Boosting, Multi-armed bandits, statistical tests, social networks, queuing networks, and neural networks. The companion website now has many examples of Python demos and also Python labs used in Berkeley. Showcases techniques of applied probability with applications in EE and CS; Presents all topics with concrete applications so students see the relevance of the theory; Illustrates methods with Jupyter notebooks that use widgets to enable the users to modify parameters.},
	language = {English},
	urldate = {2021-11-02},
	author = {Walrand, Jean},
	year = {2021},
	note = {OCLC: 1266441293},
}

@book{sundnes_introduction_2020,
	address = {Cham, Switzerland},
	series = {Simula {SpringerBriefs} on computing},
	title = {Introduction to scientific programming with {Python}},
	volume = {6},
	isbn = {978-3-030-50356-7 978-3-030-50355-0},
	language = {eng},
	publisher = {Springer},
	author = {Sundnes, Joakim},
	year = {2020},
}

@book{langtangen_primer_2011,
	address = {Berlin; Heidelberg; New York},
	edition = {2nd ed},
	series = {Texts in computational science and engineering},
	title = {A primer on scientific programming with {Python}},
	isbn = {978-3-642-18366-9},
	abstract = {The book serves as a first introduction to computer programming of scientific applications, using the high-level Python language. The exposition is example- and problem-oriented, where the applications are taken from mathematics, numerical calculus, statistics, physics, biology, and finance. The book teaches "Matlab-style" and procedural programming as well as object-oriented programming. High school mathematics is a required background, and it is advantageous to study classical and numerical one-variable calculus in parallel with reading this book. Besides learning how to program computers, the reader will also learn how to solve mathematical problems, arising in various branches of science and engineering, with the aid of numerical methods and programming. By blending programming, mathematics and scientific applications, the book lays a solid foundation for practicing computational science},
	language = {eng},
	number = {6},
	publisher = {Springer},
	author = {Langtangen, Hans Petter},
	year = {2011},
}

@book{lee_python_2011,
	address = {London ; New York},
	edition = {1st ed.},
	series = {Undergraduate topics in computer science},
	title = {Python programming fundamentals},
	isbn = {978-1-84996-536-1 978-1-84996-537-8},
	abstract = {This classroom-tested and easy-to-follow textbook teaches the reader how to program using Python, an accessible language which can be learned incrementally. Through an extensive use of examples and practical exercises, students will learn to recognize and apply abstract patterns in programming, as well as how to inspect the state of a program using a debugger tool},
	language = {eng},
	publisher = {Springer},
	author = {Lee, Kent D.},
	year = {2011},
	keywords = {Python (Computer program language), Python (programmeertaal)},
}

@book{newman_neurophysiology_1980,
	address = {Dordrecht},
	edition = {1st ed.},
	series = {Monographs in {Modern} {Neurobiology}},
	title = {Neurophysiology.},
	isbn = {978-94-011-6681-2},
	language = {English},
	publisher = {Springer},
	author = {Newman, P. P},
	year = {1980},
	note = {OCLC: 958542488},
}

@article{burkitt_review_2006,
	title = {A {Review} of the {Integrate}-and-fire {Neuron} {Model}: {I}. {Homogeneous} {Synaptic} {Input}},
	volume = {95},
	issn = {1432-0770},
	shorttitle = {A {Review} of the {Integrate}-and-fire {Neuron} {Model}},
	url = {https://doi.org/10.1007/s00422-006-0068-6},
	doi = {10.1007/s00422-006-0068-6},
	abstract = {The integrate-and-fire neuron model is one of the most widely used models for analyzing the behavior of neural systems. It describes the membrane potential of a neuron in terms of the synaptic inputs and the injected current that it receives. An action potential (spike) is generated when the membrane potential reaches a threshold, but the actual changes associated with the membrane voltage and conductances driving the action potential do not form part of the model. The synaptic inputs to the neuron are considered to be stochastic and are described as a temporally homogeneous Poisson process. Methods and results for both current synapses and conductance synapses are examined in the diffusion approximation, where the individual contributions to the postsynaptic potential are small. The focus of this review is upon the mathematical techniques that give the time distribution of output spikes, namely stochastic differential equations and the Fokker–Planck equation. The integrate-and-fire neuron model has become established as a canonical model for the description of spiking neurons because it is capable of being analyzed mathematically while at the same time being sufficiently complex to capture many of the essential features of neural processing. A number of variations of the model are discussed, together with the relationship with the Hodgkin–Huxley neuron model and the comparison with electrophysiological data. A brief overview is given of two issues in neural information processing that the integrate-and-fire neuron model has contributed to – the irregular nature of spiking in cortical neurons and neural gain modulation.},
	language = {en},
	number = {1},
	urldate = {2021-11-09},
	journal = {Biological Cybernetics},
	author = {Burkitt, A. N.},
	month = jul,
	year = {2006},
	pages = {1--19},
	file = {Springer Full Text PDF:/mnt/work-data/weverson/Zotero/storage/UAYSDDAE/Burkitt - 2006 - A Review of the Integrate-and-fire Neuron Model I.pdf:application/pdf},
}

@article{brunel_lapicques_2007,
	title = {Lapicque’s 1907 paper: from frogs to integrate-and-fire},
	volume = {97},
	issn = {1432-0770},
	shorttitle = {Lapicque’s 1907 paper},
	url = {https://doi.org/10.1007/s00422-007-0190-0},
	doi = {10.1007/s00422-007-0190-0},
	abstract = {Exactly 100 years ago, Louis Lapicque published a paper on the excitability of nerves that is often cited in the context of integrate-and-fire neurons. We discuss Lapicque’s contributions along with a translation of the original publication.},
	language = {en},
	number = {5},
	urldate = {2021-11-09},
	journal = {Biological Cybernetics},
	author = {Brunel, Nicolas and van Rossum, Mark C. W.},
	month = dec,
	year = {2007},
	pages = {337--339},
	file = {Springer Full Text PDF:/mnt/work-data/weverson/Zotero/storage/NELCX79W/Brunel e van Rossum - 2007 - Lapicque’s 1907 paper from frogs to integrate-and.pdf:application/pdf},
}

@article{lapicque_recherches_1907,
	title = {Recherches quantitatives sur l’excitation électrique des nerfs traitée comme une polarization},
	volume = {9},
	language = {fr},
	journal = {Journal of Physiol Pathol Générale},
	author = {Lapicque, Louis},
	year = {1907},
	pages = {620--635},
}

@article{brunel_quantitative_2007,
	title = {Quantitative investigations of electrical nerve excitation treated as polarization},
	volume = {97},
	issn = {1432-0770},
	url = {https://doi.org/10.1007/s00422-007-0189-6},
	doi = {10.1007/s00422-007-0189-6},
	language = {en},
	number = {5},
	urldate = {2021-11-09},
	journal = {Biological Cybernetics},
	author = {Brunel, Nicolas and van Rossum, Mark C. W.},
	month = dec,
	year = {2007},
	pages = {341--349},
	file = {Springer Full Text PDF:/mnt/work-data/weverson/Zotero/storage/ZS7LYBEF/Brunel e van Rossum - 2007 - Quantitative investigations of electrical nerve ex.pdf:application/pdf},
}

@article{brette_adaptive_2005,
	title = {Adaptive {Exponential} {Integrate}-and-{Fire} {Model} as an {Effective} {Description} of {Neuronal} {Activity}},
	volume = {94},
	issn = {0022-3077},
	url = {https://journals.physiology.org/doi/full/10.1152/jn.00686.2005},
	doi = {10.1152/jn.00686.2005},
	abstract = {We introduce a two-dimensional integrate-and-fire model that combines an exponential spike mechanism with an adaptation equation, based on recent theoretical findings. We describe a systematic method to estimate its parameters with simple electrophysiological protocols (current-clamp injection of pulses and ramps) and apply it to a detailed conductance-based model of a regular spiking neuron. Our simple model predicts correctly the timing of 96\% of the spikes (±2 ms) of the detailed model in response to injection of noisy synaptic conductances. The model is especially reliable in high-conductance states, typical of cortical activity in vivo, in which intrinsic conductances were found to have a reduced role in shaping spike trains. These results are promising because this simple model has enough expressive power to reproduce qualitatively several electrophysiological classes described in vitro.},
	number = {5},
	urldate = {2021-11-09},
	journal = {Journal of Neurophysiology},
	author = {Brette, Romain and Gerstner, Wulfram},
	month = nov,
	year = {2005},
	note = {Publisher: American Physiological Society},
	pages = {3637--3642},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/JDZNBVSY/Brette e Gerstner - 2005 - Adaptive Exponential Integrate-and-Fire Model as a.pdf:application/pdf},
}

@article{feng_is_2001,
	title = {Is the integrate-and-fire model good enough?—a review},
	volume = {14},
	issn = {0893-6080},
	shorttitle = {Is the integrate-and-fire model good enough?},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608001000740},
	doi = {10.1016/S0893-6080(01)00074-0},
	abstract = {We review some recent results on the behaviour of the integrate-and-fire (IF) model, the FitzHugh–Nagumo (FHN) model, a simplified version of the FHN (IF-FHN) model and the Hodgkin–Huxley (HH) model with correlated inputs. The effect of inhibitory inputs on the model behaviour is also taken into account. Here, inputs exclusively take the form of diffusion approximation and correlated inputs mean correlated synaptic inputs (2 The integrate-and-fire model and the Hodgkin–Huxley model, 3 Synaptic inputs). It is found that the IF and HH models respond to correlated inputs in totally opposite ways, but the IF-FHN model shows similar behaviour to the HH model. Increasing inhibitory input to single neuronal models, such as the FHN model and the HH model can sometimes increase their firing rates, which we termed inhibition-boosted firing (IBF). Using the IF model and the IF-FHN model, we theoretically explore how and when IBF can happen. The computational complexity of the IF-FHN model is very similar to the conventional IF model, but the former captures some interesting and essential features of biophysical models and could serve as a better model for spiking neuron computation.},
	language = {en},
	number = {6},
	urldate = {2021-11-09},
	journal = {Neural Networks},
	author = {Feng, Jianfeng},
	month = jul,
	year = {2001},
	keywords = {Inhibition boosted firing, The Fitzhugh-Nagumo model, The Hodgkin-Huxley model, The IF-FHN model, The integrate-and-fire model},
	pages = {955--975},
}

@article{abbott_lapicques_1999,
	title = {Lapicque’s introduction of the integrate-and-fire model neuron (1907)},
	volume = {50},
	issn = {03619230},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0361923099001616},
	doi = {10.1016/S0361-9230(99)00161-6},
	language = {en},
	number = {5-6},
	urldate = {2021-11-09},
	journal = {Brain Research Bulletin},
	author = {Abbott, L.F},
	month = nov,
	year = {1999},
	pages = {303--304},
	file = {Abbott - 1999 - Lapicque’s introduction of the integrate-and-fire .pdf:/mnt/work-data/weverson/Zotero/storage/VDE33E9W/Abbott - 1999 - Lapicque’s introduction of the integrate-and-fire .pdf:application/pdf},
}

@article{zeeman_extinction_1995,
	title = {Extinction in competitive {Lotka}-{Volterra} systems},
	volume = {123},
	issn = {0002-9939, 1088-6826},
	url = {https://www.ams.org/proc/1995-123-01/S0002-9939-1995-1264833-2/},
	doi = {10.1090/S0002-9939-1995-1264833-2},
	abstract = {It is well known that for the two species autonomous competitive Lotka-Volterra model with no fixed point in the open positive quadrant, one of the species is driven to extinction, whilst the other population stabilises at its own carrying capacity. In this paper we prove a generalisation of this result to arbitrary finite dimension. That is, for the n-species autonomous competitive Lotka-Volterra model, we exhibit simple algebraic criteria on the parameters which guarantee that all but one of the species is driven to extinction, whilst the one remaining population stabilises at its own carrying capacity.},
	language = {en},
	number = {1},
	urldate = {2022-01-04},
	journal = {Proceedings of the American Mathematical Society},
	author = {Zeeman, M. L.},
	year = {1995},
	pages = {87--96},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/UYQ2WJJW/Zeeman - 1995 - Extinction in competitive Lotka-Volterra systems.pdf:application/pdf;Snapshot:/mnt/work-data/weverson/Zotero/storage/TD8WCMXT/S0002-9939-1995-1264833-2.html:text/html},
}

@article{boeing_visual_2016,
	title = {Visual {Analysis} of {Nonlinear} {Dynamical} {Systems}: {Chaos}, {Fractals}, {Self}-{Similarity} and the {Limits} of {Prediction}},
	volume = {4},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Visual {Analysis} of {Nonlinear} {Dynamical} {Systems}},
	url = {https://www.mdpi.com/2079-8954/4/4/37},
	doi = {10.3390/systems4040037},
	abstract = {Nearly all nontrivial real-world systems are nonlinear dynamical systems. Chaos describes certain nonlinear dynamical systems that have a very sensitive dependence on initial conditions. Chaotic systems are always deterministic and may be very simple, yet they produce completely unpredictable and divergent behavior. Systems of nonlinear equations are difficult to solve analytically, and scientists have relied heavily on visual and qualitative approaches to discover and analyze the dynamics of nonlinearity. Indeed, few fields have drawn as heavily from visualization methods for their seminal innovations: from strange attractors, to bifurcation diagrams, to cobweb plots, to phase diagrams and embedding. Although the social sciences are increasingly studying these types of systems, seminal concepts remain murky or loosely adopted. This article has three aims. First, it argues for several visualization methods to critically analyze and understand the behavior of nonlinear dynamical systems. Second, it uses these visualizations to introduce the foundations of nonlinear dynamics, chaos, fractals, self-similarity and the limits of prediction. Finally, it presents Pynamical, an open-source Python package to easily visualize and explore nonlinear dynamical systems’ behavior.},
	language = {en},
	number = {4},
	urldate = {2022-01-04},
	journal = {Systems},
	author = {Boeing, Geoff},
	month = dec,
	year = {2016},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {attractor, bifurcation, chaos, dynamical systems, fractal, logistic map, nonlinear dynamics, prediction, python, visualization},
	pages = {37},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/5YQKU5B6/Boeing - 2016 - Visual Analysis of Nonlinear Dynamical Systems Ch.pdf:application/pdf;Snapshot:/mnt/work-data/weverson/Zotero/storage/P6E8FWNE/37.html:text/html},
}

@incollection{pham_artificial_2007,
	address = {Boston, MA},
	title = {Artificial {Neural} {Networks}},
	isbn = {978-0-387-37452-9},
	url = {https://doi.org/10.1007/0-387-37452-3_3},
	language = {en},
	urldate = {2022-02-09},
	booktitle = {Computational {Intelligence}: for {Engineering} and {Manufacturing}},
	publisher = {Springer US},
	author = {Pham, D. T. and Packianather, M. S. and Afify, A. A.},
	editor = {Andina, Diego and Pham, Duc Truong},
	year = {2007},
	doi = {10.1007/0-387-37452-3_3},
	pages = {67--92},
	file = {Springer Full Text PDF:/mnt/work-data/weverson/Zotero/storage/FZR9GW9P/Pham et al. - 2007 - Artificial Neural Networks.pdf:application/pdf},
}
