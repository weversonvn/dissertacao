
@article{clopath_cerebellar_2014,
	title = {A Cerebellar Learning Model of Vestibulo-Ocular Reflex Adaptation in Wild-Type and Mutant Mice},
	volume = {34},
	issn = {0270-6474},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6608186/},
	doi = {10.1523/JNEUROSCI.2791-13.2014},
	abstract = {Mechanisms of cerebellar motor learning are still poorly understood. The standard Marr–Albus–Ito theory posits that learning involves plasticity at the parallel fiber to Purkinje cell synapses under control of the climbing fiber input, which provides an error signal as in classical supervised learning paradigms. However, a growing body of evidence challenges this theory, in that additional sites of plasticity appear to contribute to motor adaptation. Here, we consider phase-reversal training of the vestibulo-ocular reflex ({VOR}), a simple form of motor learning for which a large body of experimental data is available in wild-type and mutant mice, in which the excitability of granule cells or inhibition of Purkinje cells was affected in a cell-specific fashion. We present novel electrophysiological recordings of Purkinje cell activity measured in naive wild-type mice subjected to this {VOR} adaptation task. We then introduce a minimal model that consists of learning at the parallel fibers to Purkinje cells with the help of the climbing fibers. Although the minimal model reproduces the behavior of the wild-type animals and is analytically tractable, it fails at reproducing the behavior of mutant mice and the electrophysiology data. Therefore, we build a detailed model involving plasticity at the parallel fibers to Purkinje cells' synapse guided by climbing fibers, feedforward inhibition of Purkinje cells, and plasticity at the mossy fiber to vestibular nuclei neuron synapse. The detailed model reproduces both the behavioral and electrophysiological data of both the wild-type and mutant mice and allows for experimentally testable predictions.},
	pages = {7203--7215},
	number = {21},
	journaltitle = {The Journal of Neuroscience},
	shortjournal = {J Neurosci},
	author = {Clopath, Claudia and Badura, Aleksandra and De Zeeuw, Chris I. and Brunel, Nicolas},
	urldate = {2023-08-25},
	date = {2014-05-21},
	pmid = {24849355},
	pmcid = {PMC6608186},
	file = {PubMed Central Full Text PDF:/home/weverson/Zotero/storage/PUX8CMWS/Clopath et al. - 2014 - A Cerebellar Learning Model of Vestibulo-Ocular Re.pdf:application/pdf},
}

@article{beggs_being_2012,
	title = {Being Critical of Criticality in the Brain},
	volume = {3},
	issn = {1664-042X},
	url = {https://www.frontiersin.org/articles/10.3389/fphys.2012.00163},
	abstract = {Relatively recent work has reported that networks of neurons can produce avalanches of activity whose sizes follow a power law distribution. This suggests that these networks may be operating near a critical point, poised between a phase where activity rapidly dies out and a phase where activity is amplified over time. The hypothesis that the electrical activity of neural networks in the brain is critical is potentially important, as many simulations suggest that information processing functions would be optimized at the critical point. This hypothesis, however, is still controversial. Here we will explain the concept of criticality and review the substantial objections to the criticality hypothesis raised by skeptics. Points and counter points are presented in dialog form.},
	journaltitle = {Frontiers in Physiology},
	author = {Beggs, John and Timme, Nicholas},
	urldate = {2023-08-25},
	date = {2012},
	file = {Full Text PDF:/home/weverson/Zotero/storage/HPAYE938/Beggs e Timme - 2012 - Being Critical of Criticality in the Brain.pdf:application/pdf},
}

@article{marshall_analysis_2016,
	title = {Analysis of Power Laws, Shape Collapses, and Neural Complexity: New Techniques and {MATLAB} Support via the {NCC} Toolbox},
	volume = {7},
	issn = {1664-042X},
	url = {https://www.frontiersin.org/articles/10.3389/fphys.2016.00250},
	shorttitle = {Analysis of Power Laws, Shape Collapses, and Neural Complexity},
	abstract = {Neural systems include interactions that occur across many scales. Two divergent methods for characterizing such interactions have drawn on the physical analysis of critical phenomena and the mathematical study of information. Inferring criticality in neural systems has traditionally rested on fitting power laws to the property distributions of “neural avalanches” (contiguous bursts of activity), but the fractal nature of avalanche shapes has recently emerged as another signature of criticality. On the other hand, neural complexity, an information theoretic measure, has been used to capture the interplay between the functional localization of brain regions and their integration for higher cognitive functions. Unfortunately, treatments of all three methods—power-law fitting, avalanche shape collapse, and neural complexity—have suffered from shortcomings. Empirical data often contain biases that introduce deviations from true power law in the tail and head of the distribution, but deviations in the tail have often been unconsidered; avalanche shape collapse has required manual parameter tuning; and the estimation of neural complexity has relied on small data sets or statistical assumptions for the sake of computational efficiency. In this paper we present technical advancements in the analysis of criticality and complexity in neural systems. We use maximum-likelihood estimation to automatically fit power laws with left and right cutoffs, present the first automated shape collapse algorithm, and describe new techniques to account for large numbers of neural variables and small data sets in the calculation of neural complexity. In order to facilitate future research in criticality and complexity, we have made the software utilized in this analysis freely available online in the {MATLAB} {NCC} (Neural Complexity and Criticality) Toolbox.},
	journaltitle = {Frontiers in Physiology},
	author = {Marshall, Najja and Timme, Nicholas M. and Bennett, Nicholas and Ripp, Monica and Lautzenhiser, Edward and Beggs, John M.},
	urldate = {2023-08-25},
	date = {2016},
	file = {Full Text PDF:/home/weverson/Zotero/storage/EVF9FESQ/Marshall et al. - 2016 - Analysis of Power Laws, Shape Collapses, and Neura.pdf:application/pdf},
}

@article{zimmern_why_2020,
	title = {Why Brain Criticality Is Clinically Relevant: A Scoping Review},
	volume = {14},
	issn = {1662-5110},
	url = {https://www.frontiersin.org/articles/10.3389/fncir.2020.00054},
	shorttitle = {Why Brain Criticality Is Clinically Relevant},
	abstract = {The past 25 years have seen a strong increase in the number of publications related to criticality in different areas of neuroscience. The potential of criticality to explain various brain properties, including optimal information processing, has made it an increasingly exciting area of investigation for neuroscientists. Recent reviews on this topic, sometimes termed brain criticality, make brief mention of clinical applications of these findings to several neurological disorders such as epilepsy, neurodegenerative disease, and neonatal hypoxia. Other clinicallyrelevant domains – including anesthesia, sleep medicine, developmental-behavioral pediatrics, and psychiatry – are seldom discussed in review papers of brain criticality. Thorough assessments of these application areas and their relevance for clinicians have also yet to be published. In this scoping review, studies of brain criticality involving human data of all ages are evaluated for their current and future clinical relevance. To make the results of these studies understandable to a more clinical audience, a review of the key concepts behind criticality (e.g., phase transitions, long-range temporal correlation, self-organized criticality, power laws, branching processes) precedes the discussion of human clinical studies. Open questions and forthcoming areas of investigation are also considered.},
	journaltitle = {Frontiers in Neural Circuits},
	author = {Zimmern, Vincent},
	urldate = {2023-08-25},
	date = {2020},
	file = {Full Text PDF:/home/weverson/Zotero/storage/656YGTZI/Zimmern - 2020 - Why Brain Criticality Is Clinically Relevant A Sc.pdf:application/pdf},
}

@article{lee_role_2018,
	title = {The Role of Network Science in the Study of Anesthetic State Transitions},
	volume = {129},
	issn = {0003-3022},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6191341/},
	doi = {10.1097/ALN.0000000000002228},
	abstract = {The heterogeneity of molecular mechanisms, target neural circuits, and neurophysiological effects of general anesthetics makes it difficult to develop a reliable and drug-invariant index of general anesthesia. No single brain region or mechanism has been identified as the neural correlate of consciousness, suggesting that consciousness might emerge through complex interactions of spatially and temporally distributed brain functions. The goal of this review article is to introduce the basic concepts of networks and explain why the application of network science to general anesthesia could be a pathway to discover a fundamental mechanism of anesthetic-induced unconsciousness. We review data suggesting that reduced network efficiency, constrained network repertoires, and changes in cortical dynamics create inhospitable conditions for information processing and transfer, which lead to unconsciousness. We propose that network science is not just a useful tool but a necessary theoretical framework and method to uncover common principles of anesthetic-induced unconsciousness.},
	pages = {1029--1044},
	number = {5},
	journaltitle = {Anesthesiology},
	shortjournal = {Anesthesiology},
	author = {Lee, {UnCheol} and Mashour, George A.},
	urldate = {2023-08-25},
	date = {2018-11},
	pmid = {29683806},
	pmcid = {PMC6191341},
	file = {PubMed Central Full Text PDF:/home/weverson/Zotero/storage/C4CG3Z9C/Lee e Mashour - 2018 - The Role of Network Science in the Study of Anesth.pdf:application/pdf},
}

@article{li_deep_2020,
	title = {A Deep Learning Approach for Mild Depression Recognition Based on Functional Connectivity Using Electroencephalography},
	volume = {14},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2020.00192},
	abstract = {Early detection remains a significant challenge for the treatment of depression. In our work, we proposed a novel approach to mild depression recognition using electroencephalography ({EEG}). First, we explored abnormal organization in the functional connectivity network of mild depression using graph theory. Second, we proposed a novel classification model for recognizing mild depression. Considering the powerful ability of {CNN} to process two-dimensional data, we applied {CNN} separately to the two-dimensional data form of the functional connectivity matrices from five {EEG} bands (delta, theta, alpha, beta, and gamma). In addition, inspired by recent breakthroughs in the ability of deep recurrent {CNNs} to classify mental load, we merged the functional connectivity matrices from the three {EEG} bands that performed the best into a three-channel image to classify mild depression-related and normal {EEG} signals using the {CNN}. The results of the graph theory analysis showed that the brain functional network of the mild depression group had a larger characteristic path length and a lower clustering coefficient than the healthy control group, showing deviation from the small-world network. The proposed classification model obtained a classification accuracy of 80.74\% for recognizing mild depression. The current study suggests that the combination of a {CNN} and functional connectivity matrix may provide a promising objective approach for diagnosing mild depression. Deep learning approaches such as this might have the potential to inform clinical practice and aid in research on psychiatric disorders.},
	journaltitle = {Frontiers in Neuroscience},
	author = {Li, Xiaowei and La, Rong and Wang, Ying and Hu, Bin and Zhang, Xuemin},
	urldate = {2023-08-25},
	date = {2020},
	file = {Full Text PDF:/home/weverson/Zotero/storage/SXJYNLRD/Li et al. - 2020 - A Deep Learning Approach for Mild Depression Recog.pdf:application/pdf},
}

@article{heiney_criticality_2021,
	title = {Criticality, Connectivity, and Neural Disorder: A Multifaceted Approach to Neural Computation},
	volume = {15},
	issn = {1662-5188},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7902700/},
	doi = {10.3389/fncom.2021.611183},
	shorttitle = {Criticality, Connectivity, and Neural Disorder},
	abstract = {It has been hypothesized that the brain optimizes its capacity for computation by self-organizing to a critical point. The dynamical state of criticality is achieved by striking a balance such that activity can effectively spread through the network without overwhelming it and is commonly identified in neuronal networks by observing the behavior of cascades of network activity termed “neuronal avalanches.” The dynamic activity that occurs in neuronal networks is closely intertwined with how the elements of the network are connected and how they influence each other's functional activity. In this review, we highlight how studying criticality with a broad perspective that integrates concepts from physics, experimental and theoretical neuroscience, and computer science can provide a greater understanding of the mechanisms that drive networks to criticality and how their disruption may manifest in different disorders. First, integrating graph theory into experimental studies on criticality, as is becoming more common in theoretical and modeling studies, would provide insight into the kinds of network structures that support criticality in networks of biological neurons. Furthermore, plasticity mechanisms play a crucial role in shaping these neural structures, both in terms of homeostatic maintenance and learning. Both network structures and plasticity have been studied fairly extensively in theoretical models, but much work remains to bridge the gap between theoretical and experimental findings. Finally, information theoretical approaches can tie in more concrete evidence of a network's computational capabilities. Approaching neural dynamics with all these facets in mind has the potential to provide a greater understanding of what goes wrong in neural disorders. Criticality analysis therefore holds potential to identify disruptions to healthy dynamics, granted that robust methods and approaches are considered.},
	pages = {611183},
	journaltitle = {Frontiers in Computational Neuroscience},
	shortjournal = {Front Comput Neurosci},
	author = {Heiney, Kristine and Huse Ramstad, Ola and Fiskum, Vegard and Christiansen, Nicholas and Sandvig, Axel and Nichele, Stefano and Sandvig, Ioanna},
	urldate = {2023-08-25},
	date = {2021-02-10},
	pmid = {33643017},
	pmcid = {PMC7902700},
	file = {PubMed Central Full Text PDF:/home/weverson/Zotero/storage/47SS3MND/Heiney et al. - 2021 - Criticality, Connectivity, and Neural Disorder A .pdf:application/pdf},
}

@article{hartley_identification_2014,
	title = {Identification of Criticality in Neuronal Avalanches: {II}. A Theoretical and Empirical Investigation of the Driven Case},
	volume = {4},
	rights = {2014 C. Hartley et al.; licensee Springer},
	issn = {2190-8567},
	url = {https://mathematical-neuroscience.springeropen.com/articles/10.1186/2190-8567-4-9},
	doi = {10.1186/2190-8567-4-9},
	shorttitle = {Identification of Criticality in Neuronal Avalanches},
	abstract = {The observation of apparent power laws in neuronal systems has led to the suggestion that the brain is at, or close to, a critical state and may be a self-organised critical system. Within the framework of self-organised criticality a separation of timescales is thought to be crucial for the observation of power-law dynamics and computational models are often constructed with this property. However, this is not necessarily a characteristic of physiological neural networks—external input does not only occur when the network is at rest/a steady state. In this paper we study a simple neuronal network model driven by a continuous external input (i.e. the model does not have an explicit separation of timescales from seeding the system only when in the quiescent state) and analytically tuned to operate in the region of a critical state (it reaches the critical regime exactly in the absence of input—the case studied in the companion paper to this article). The system displays avalanche dynamics in the form of cascades of neuronal firing separated by periods of silence. We observe partial scale-free behaviour in the distribution of avalanche size for low levels of external input. We analytically derive the distributions of waiting times and investigate their temporal behaviour in relation to different levels of external input, showing that the system’s dynamics can exhibit partial long-range temporal correlations. We further show that as the system approaches the critical state by two alternative ‘routes’, different markers of criticality (partial scale-free behaviour and long-range temporal correlations) are displayed. This suggests that signatures of criticality exhibited by a particular system in close proximity to a critical state are dependent on the region in parameter space at which the system (currently) resides.},
	pages = {1--42},
	number = {1},
	journaltitle = {The Journal of Mathematical Neuroscience},
	shortjournal = {J. Math. Neurosc.},
	author = {Hartley, Caroline and Taylor, Timothy J. and Kiss, Istvan Z. and Farmer, Simon F. and Berthouze, Luc},
	urldate = {2023-08-25},
	date = {2014-12},
	langid = {english},
	note = {Number: 1
Publisher: {SpringerOpen}},
	file = {Full Text PDF:/home/weverson/Zotero/storage/HXKYJM58/Hartley et al. - 2014 - Identification of Criticality in Neuronal Avalanch.pdf:application/pdf},
}

@article{perez-gonzalez_adaptation_2014,
	title = {Adaptation in the auditory system: an overview},
	volume = {8},
	issn = {1662-5145},
	url = {https://www.frontiersin.org/articles/10.3389/fnint.2014.00019},
	shorttitle = {Adaptation in the auditory system},
	abstract = {The early stages of the auditory system need to preserve the timing information of sounds in order to extract the basic features of acoustic stimuli. At the same time, different processes of neuronal adaptation occur at several levels to further process the auditory information. For instance, auditory nerve fiber responses already experience adaptation of their firing rates, a type of response that can be found in many other auditory nuclei and may be useful for emphasizing the onset of the stimuli. However, it is at higher levels in the auditory hierarchy where more sophisticated types of neuronal processing take place. For example, stimulus-specific adaptation, where neurons show adaptation to frequent, repetitive stimuli, but maintain their responsiveness to stimuli with different physical characteristics, thus representing a distinct kind of processing that may play a role in change and deviance detection. In the auditory cortex, adaptation takes more elaborate forms, and contributes to the processing of complex sequences, auditory scene analysis and attention. Here we review the multiple types of adaptation that occur in the auditory system, which are part of the pool of resources that the neurons employ to process the auditory scene, and are critical to a proper understanding of the neuronal mechanisms that govern auditory perception.},
	journaltitle = {Frontiers in Integrative Neuroscience},
	author = {Pérez-González, David and Malmierca, Manuel},
	urldate = {2023-08-25},
	date = {2014},
	file = {Full Text PDF:/home/weverson/Zotero/storage/W8K4XCHX/Pérez-González e Malmierca - 2014 - Adaptation in the auditory system an overview.pdf:application/pdf},
}

@article{hesse_self-organized_2014,
	title = {Self-organized criticality as a fundamental property of neural systems},
	volume = {8},
	issn = {1662-5137},
	url = {https://www.frontiersin.org/articles/10.3389/fnsys.2014.00166},
	abstract = {The neural criticality hypothesis states that the brain may be poised in a critical state at a boundary between different types of dynamics. Theoretical and experimental studies show that critical systems often exhibit optimal computational properties, suggesting the possibility that criticality has been evolutionarily selected as a useful trait for our nervous system. Evidence for criticality has been found in cell cultures, brain slices, and anesthetized animals. Yet, inconsistent results were reported for recordings in awake animals and humans, and current results point to open questions about the exact nature and mechanism of criticality, as well as its functional role. Therefore, the criticality hypothesis has remained a controversial proposition. Here, we provide an account of the mathematical and physical foundations of criticality. In the light of this conceptual framework, we then review and discuss recent experimental studies with the aim of identifying important next steps to be taken and connections to other fields that should be explored.},
	journaltitle = {Frontiers in Systems Neuroscience},
	author = {Hesse, Janina and Gross, Thilo},
	urldate = {2023-08-25},
	date = {2014},
	file = {Full Text PDF:/home/weverson/Zotero/storage/IL34RKHZ/Hesse e Gross - 2014 - Self-organized criticality as a fundamental proper.pdf:application/pdf},
}

@article{freeman_tutorial_1992,
	title = {{TUTORIAL} {ON} {NEUROBIOLOGY}: {FROM} {SINGLE} {NEURONS} {TO} {BRAIN} {CHAOS}},
	volume = {02},
	issn = {0218-1274, 1793-6551},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0218127492000653},
	doi = {10.1142/S0218127492000653},
	shorttitle = {{TUTORIAL} {ON} {NEUROBIOLOGY}},
	abstract = {Those classical models are reviewed that are most widely used by neurobiologists to explain the dynamics of neurons and neuron populations, and by modelers to implement artificial neural networks. Each neuron has input fibers called dendrites that integrate and an axon that transmits the output. The differing fiber architectures reflect these dissimilar dynamic operations. The basic tools to describe them are the {RC} model of the membrane, the core conductor model of the fibers, the Hodgkin–Huxley model of the trigger zone, and the modifiable synapse. Populations additionally require description of macroscopic state variables, the types of nonlinearity (most importantly the sigmoid curve and the dynamic range compression at the input to the cortex), and the types and strengths of connections. The properties of these neural masses can be characterized with the tools of nonlinear dynamics. These include description of point, limit cycle, and chaotic attractors for the cerebal cortex, as well as the types and mechanisms for the state transitions between basins of attraction during learning and perception.},
	pages = {451--482},
	number = {3},
	journaltitle = {International Journal of Bifurcation and Chaos},
	shortjournal = {Int. J. Bifurcation Chaos},
	author = {Freeman, Walter J.},
	urldate = {2023-08-25},
	date = {1992-09},
	langid = {english},
}

@article{miller_itinerancy_2016,
	title = {Itinerancy between attractor states in neural systems},
	volume = {40},
	issn = {0959-4388},
	url = {https://www.sciencedirect.com/science/article/pii/S0959438816300642},
	doi = {10.1016/j.conb.2016.05.005},
	series = {Systems neuroscience},
	abstract = {Converging evidence from neural, perceptual and simulated data suggests that discrete attractor states form within neural circuits through learning and development. External stimuli may bias neural activity to one attractor state or cause activity to transition between several discrete states. Evidence for such transitions, whose timing can vary across trials, is best accrued through analyses that avoid any trial-averaging of data. One such method, hidden Markov modeling, has been effective in this context, revealing state transitions in many neural circuits during many tasks. Concurrently, modeling efforts have revealed computational benefits of stimulus processing via transitions between attractor states. This review describes the current state of the field, with comments on how its perceived limitations have been addressed.},
	pages = {14--22},
	journaltitle = {Current Opinion in Neurobiology},
	shortjournal = {Current Opinion in Neurobiology},
	author = {Miller, Paul},
	urldate = {2023-08-25},
	date = {2016-10-01},
	file = {Versão aceita:/home/weverson/Zotero/storage/MBC6KRXL/Miller - 2016 - Itinerancy between attractor states in neural syst.pdf:application/pdf},
}

@article{toker_simple_2020,
	title = {A simple method for detecting chaos in nature},
	volume = {3},
	rights = {2020 The Author(s)},
	issn = {2399-3642},
	url = {https://www.nature.com/articles/s42003-019-0715-9},
	doi = {10.1038/s42003-019-0715-9},
	abstract = {Chaos, or exponential sensitivity to small perturbations, appears everywhere in nature. Moreover, chaos is predicted to play diverse functional roles in living systems. A method for detecting chaos from empirical measurements should therefore be a key component of the biologist’s toolkit. But, classic chaos-detection tools are highly sensitive to measurement noise and break down for common edge cases, making it difficult to detect chaos in domains, like biology, where measurements are noisy. However, newer tools promise to overcome these limitations. Here, we combine several such tools into an automated processing pipeline, and show that our pipeline can detect the presence (or absence) of chaos in noisy recordings, even for difficult edge cases. As a first-pass application of our pipeline, we show that heart rate variability is not chaotic as some have proposed, and instead reflects a stochastic process in both health and disease. Our tool is easy-to-use and freely available.},
	pages = {1--13},
	number = {1},
	journaltitle = {Communications Biology},
	shortjournal = {Commun Biol},
	author = {Toker, Daniel and Sommer, Friedrich T. and D’Esposito, Mark},
	urldate = {2023-08-25},
	date = {2020-01-03},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational biophysics, Data processing},
	file = {Full Text PDF:/home/weverson/Zotero/storage/7EWJ25DS/Toker et al. - 2020 - A simple method for detecting chaos in nature.pdf:application/pdf},
}

@incollection{pham_artificial_2007,
	location = {Boston, {MA}},
	title = {Artificial Neural Networks},
	isbn = {978-0-387-37452-9},
	url = {https://doi.org/10.1007/0-387-37452-3_3},
	pages = {67--92},
	booktitle = {Computational Intelligence: for Engineering and Manufacturing},
	publisher = {Springer {US}},
	author = {Pham, D. T. and Packianather, M. S. and Afify, A. A.},
	editor = {Andina, Diego and Pham, Duc Truong},
	urldate = {2022-02-09},
	date = {2007},
	langid = {english},
	doi = {10.1007/0-387-37452-3_3},
	file = {Springer Full Text PDF:/home/weverson/Zotero/storage/FZR9GW9P/Pham et al. - 2007 - Artificial Neural Networks.pdf:application/pdf},
}

@article{boeing_visual_2016,
	title = {Visual Analysis of Nonlinear Dynamical Systems: Chaos, Fractals, Self-Similarity and the Limits of Prediction},
	volume = {4},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2079-8954/4/4/37},
	doi = {10.3390/systems4040037},
	shorttitle = {Visual Analysis of Nonlinear Dynamical Systems},
	abstract = {Nearly all nontrivial real-world systems are nonlinear dynamical systems. Chaos describes certain nonlinear dynamical systems that have a very sensitive dependence on initial conditions. Chaotic systems are always deterministic and may be very simple, yet they produce completely unpredictable and divergent behavior. Systems of nonlinear equations are difficult to solve analytically, and scientists have relied heavily on visual and qualitative approaches to discover and analyze the dynamics of nonlinearity. Indeed, few fields have drawn as heavily from visualization methods for their seminal innovations: from strange attractors, to bifurcation diagrams, to cobweb plots, to phase diagrams and embedding. Although the social sciences are increasingly studying these types of systems, seminal concepts remain murky or loosely adopted. This article has three aims. First, it argues for several visualization methods to critically analyze and understand the behavior of nonlinear dynamical systems. Second, it uses these visualizations to introduce the foundations of nonlinear dynamics, chaos, fractals, self-similarity and the limits of prediction. Finally, it presents Pynamical, an open-source Python package to easily visualize and explore nonlinear dynamical systems’ behavior.},
	pages = {37},
	number = {4},
	journaltitle = {Systems},
	author = {Boeing, Geoff},
	urldate = {2022-01-04},
	date = {2016-12},
	langid = {english},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {attractor, bifurcation, chaos, dynamical systems, fractal, logistic map, nonlinear dynamics, prediction, python, visualization},
	file = {Full Text PDF:/home/weverson/Zotero/storage/5YQKU5B6/Boeing - 2016 - Visual Analysis of Nonlinear Dynamical Systems Ch.pdf:application/pdf},
}

@article{zeeman_extinction_1995,
	title = {Extinction in competitive Lotka-Volterra systems},
	volume = {123},
	issn = {0002-9939, 1088-6826},
	url = {https://www.ams.org/proc/1995-123-01/S0002-9939-1995-1264833-2/},
	doi = {10.1090/S0002-9939-1995-1264833-2},
	abstract = {It is well known that for the two species autonomous competitive Lotka-Volterra model with no fixed point in the open positive quadrant, one of the species is driven to extinction, whilst the other population stabilises at its own carrying capacity. In this paper we prove a generalisation of this result to arbitrary finite dimension. That is, for the n-species autonomous competitive Lotka-Volterra model, we exhibit simple algebraic criteria on the parameters which guarantee that all but one of the species is driven to extinction, whilst the one remaining population stabilises at its own carrying capacity.},
	pages = {87--96},
	number = {1},
	journaltitle = {Proceedings of the American Mathematical Society},
	shortjournal = {Proc. Amer. Math. Soc.},
	author = {Zeeman, M. L.},
	urldate = {2022-01-04},
	date = {1995},
	langid = {english},
	file = {Full Text PDF:/home/weverson/Zotero/storage/UYQ2WJJW/Zeeman - 1995 - Extinction in competitive Lotka-Volterra systems.pdf:application/pdf},
}

@book{miller_introductory_2018,
	location = {Cambridge, {MA}},
	title = {An Introductory Course in Computational Neuroscience},
	rights = {2018 Massachussets Institute of Technology},
	isbn = {978-0-262-03825-6},
	url = {https://mitpress.ublish.com/book/intro-computational-neuroscience},
	series = {Computational neuroscience series},
	pagetotal = {383},
	publisher = {The {MIT} Press},
	author = {Miller, Paul},
	urldate = {2021-10-05},
	date = {2018},
	langid = {english},
}

@article{abbott_lapicques_1999,
	title = {Lapicque’s introduction of the integrate-and-fire model neuron (1907)},
	volume = {50},
	issn = {03619230},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0361923099001616},
	doi = {10.1016/S0361-9230(99)00161-6},
	pages = {303--304},
	number = {5},
	journaltitle = {Brain Research Bulletin},
	shortjournal = {Brain Research Bulletin},
	author = {Abbott, L.F},
	urldate = {2021-11-09},
	date = {1999-11},
	langid = {english},
	file = {Abbott - 1999 - Lapicque’s introduction of the integrate-and-fire .pdf:/home/weverson/Zotero/storage/VDE33E9W/Abbott - 1999 - Lapicque’s introduction of the integrate-and-fire .pdf:application/pdf},
}

@article{feng_is_2001,
	title = {Is the integrate-and-fire model good enough?—a review},
	volume = {14},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608001000740},
	doi = {10.1016/S0893-6080(01)00074-0},
	shorttitle = {Is the integrate-and-fire model good enough?},
	abstract = {We review some recent results on the behaviour of the integrate-and-fire ({IF}) model, the {FitzHugh}–Nagumo ({FHN}) model, a simplified version of the {FHN} ({IF}-{FHN}) model and the Hodgkin–Huxley ({HH}) model with correlated inputs. The effect of inhibitory inputs on the model behaviour is also taken into account. Here, inputs exclusively take the form of diffusion approximation and correlated inputs mean correlated synaptic inputs (2 The integrate-and-fire model and the Hodgkin–Huxley model, 3 Synaptic inputs). It is found that the {IF} and {HH} models respond to correlated inputs in totally opposite ways, but the {IF}-{FHN} model shows similar behaviour to the {HH} model. Increasing inhibitory input to single neuronal models, such as the {FHN} model and the {HH} model can sometimes increase their firing rates, which we termed inhibition-boosted firing ({IBF}). Using the {IF} model and the {IF}-{FHN} model, we theoretically explore how and when {IBF} can happen. The computational complexity of the {IF}-{FHN} model is very similar to the conventional {IF} model, but the former captures some interesting and essential features of biophysical models and could serve as a better model for spiking neuron computation.},
	pages = {955--975},
	number = {6},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Feng, Jianfeng},
	urldate = {2021-11-09},
	date = {2001-07-09},
	langid = {english},
	keywords = {Inhibition boosted firing, The Fitzhugh-Nagumo model, The Hodgkin-Huxley model, The {IF}-{FHN} model, The integrate-and-fire model},
	file = {Versão submetida:/home/weverson/Zotero/storage/YYCVIMPX/Feng - 2001 - Is the integrate-and-fire model good enough—a rev.pdf:application/pdf},
}

@article{brette_adaptive_2005,
	title = {Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity},
	volume = {94},
	issn = {0022-3077},
	url = {https://journals.physiology.org/doi/full/10.1152/jn.00686.2005},
	doi = {10.1152/jn.00686.2005},
	abstract = {We introduce a two-dimensional integrate-and-fire model that combines an exponential spike mechanism with an adaptation equation, based on recent theoretical findings. We describe a systematic method to estimate its parameters with simple electrophysiological protocols (current-clamp injection of pulses and ramps) and apply it to a detailed conductance-based model of a regular spiking neuron. Our simple model predicts correctly the timing of 96\% of the spikes (±2 ms) of the detailed model in response to injection of noisy synaptic conductances. The model is especially reliable in high-conductance states, typical of cortical activity in vivo, in which intrinsic conductances were found to have a reduced role in shaping spike trains. These results are promising because this simple model has enough expressive power to reproduce qualitatively several electrophysiological classes described in vitro.},
	pages = {3637--3642},
	number = {5},
	journaltitle = {Journal of Neurophysiology},
	author = {Brette, Romain and Gerstner, Wulfram},
	urldate = {2021-11-09},
	date = {2005-11-01},
	note = {Publisher: American Physiological Society},
	file = {Full Text PDF:/home/weverson/Zotero/storage/JDZNBVSY/Brette e Gerstner - 2005 - Adaptive Exponential Integrate-and-Fire Model as a.pdf:application/pdf},
}

@article{brunel_quantitative_2007,
	title = {Quantitative investigations of electrical nerve excitation treated as polarization},
	volume = {97},
	issn = {1432-0770},
	url = {https://doi.org/10.1007/s00422-007-0189-6},
	doi = {10.1007/s00422-007-0189-6},
	pages = {341--349},
	number = {5},
	journaltitle = {Biological Cybernetics},
	shortjournal = {Biol Cybern},
	author = {Brunel, Nicolas and van Rossum, Mark C. W.},
	urldate = {2021-11-09},
	date = {2007-12-01},
	langid = {english},
	file = {Springer Full Text PDF:/home/weverson/Zotero/storage/ZS7LYBEF/Brunel e van Rossum - 2007 - Quantitative investigations of electrical nerve ex.pdf:application/pdf},
}

@article{lapicque_recherches_1907,
	title = {Recherches quantitatives sur l’excitation électrique des nerfs traitée comme une polarization},
	volume = {9},
	pages = {620--635},
	journaltitle = {Journal of Physiol Pathol Générale},
	author = {Lapicque, Louis},
	date = {1907},
	langid = {french},
}

@article{brunel_lapicques_2007,
	title = {Lapicque’s 1907 paper: from frogs to integrate-and-fire},
	volume = {97},
	issn = {1432-0770},
	url = {https://doi.org/10.1007/s00422-007-0190-0},
	doi = {10.1007/s00422-007-0190-0},
	shorttitle = {Lapicque’s 1907 paper},
	abstract = {Exactly 100 years ago, Louis Lapicque published a paper on the excitability of nerves that is often cited in the context of integrate-and-fire neurons. We discuss Lapicque’s contributions along with a translation of the original publication.},
	pages = {337--339},
	number = {5},
	journaltitle = {Biological Cybernetics},
	shortjournal = {Biol Cybern},
	author = {Brunel, Nicolas and van Rossum, Mark C. W.},
	urldate = {2021-11-09},
	date = {2007-12-01},
	langid = {english},
	file = {Springer Full Text PDF:/home/weverson/Zotero/storage/NELCX79W/Brunel e van Rossum - 2007 - Lapicque’s 1907 paper from frogs to integrate-and.pdf:application/pdf},
}

@article{burkitt_review_2006,
	title = {A Review of the Integrate-and-fire Neuron Model: I. Homogeneous Synaptic Input},
	volume = {95},
	issn = {1432-0770},
	url = {https://doi.org/10.1007/s00422-006-0068-6},
	doi = {10.1007/s00422-006-0068-6},
	shorttitle = {A Review of the Integrate-and-fire Neuron Model},
	abstract = {The integrate-and-fire neuron model is one of the most widely used models for analyzing the behavior of neural systems. It describes the membrane potential of a neuron in terms of the synaptic inputs and the injected current that it receives. An action potential (spike) is generated when the membrane potential reaches a threshold, but the actual changes associated with the membrane voltage and conductances driving the action potential do not form part of the model. The synaptic inputs to the neuron are considered to be stochastic and are described as a temporally homogeneous Poisson process. Methods and results for both current synapses and conductance synapses are examined in the diffusion approximation, where the individual contributions to the postsynaptic potential are small. The focus of this review is upon the mathematical techniques that give the time distribution of output spikes, namely stochastic differential equations and the Fokker–Planck equation. The integrate-and-fire neuron model has become established as a canonical model for the description of spiking neurons because it is capable of being analyzed mathematically while at the same time being sufficiently complex to capture many of the essential features of neural processing. A number of variations of the model are discussed, together with the relationship with the Hodgkin–Huxley neuron model and the comparison with electrophysiological data. A brief overview is given of two issues in neural information processing that the integrate-and-fire neuron model has contributed to – the irregular nature of spiking in cortical neurons and neural gain modulation.},
	pages = {1--19},
	number = {1},
	journaltitle = {Biological Cybernetics},
	shortjournal = {Biol Cybern},
	author = {Burkitt, A. N.},
	urldate = {2021-11-09},
	date = {2006-07-01},
	langid = {english},
	file = {Springer Full Text PDF:/home/weverson/Zotero/storage/UAYSDDAE/Burkitt - 2006 - A Review of the Integrate-and-fire Neuron Model I.pdf:application/pdf},
}

@book{newman_neurophysiology_1980,
	location = {Dordrecht},
	edition = {1st ed.},
	title = {Neurophysiology.},
	isbn = {978-94-011-6681-2},
	series = {Monographs in Modern Neurobiology},
	pagetotal = {525},
	publisher = {Springer},
	author = {Newman, P. P},
	date = {1980},
	note = {{OCLC}: 958542488},
}

@book{langtangen_primer_2011,
	location = {Berlin; Heidelberg; New York},
	edition = {2nd ed},
	title = {A primer on scientific programming with Python},
	isbn = {978-3-642-18366-9},
	series = {Texts in computational science and engineering},
	abstract = {The book serves as a first introduction to computer programming of scientific applications, using the high-level Python language. The exposition is example- and problem-oriented, where the applications are taken from mathematics, numerical calculus, statistics, physics, biology, and finance. The book teaches "Matlab-style" and procedural programming as well as object-oriented programming. High school mathematics is a required background, and it is advantageous to study classical and numerical one-variable calculus in parallel with reading this book. Besides learning how to program computers, the reader will also learn how to solve mathematical problems, arising in various branches of science and engineering, with the aid of numerical methods and programming. By blending programming, mathematics and scientific applications, the book lays a solid foundation for practicing computational science},
	pagetotal = {706},
	number = {6},
	publisher = {Springer},
	author = {Langtangen, Hans Petter},
	date = {2011},
}

@book{linge_programming_2020,
	location = {Cham},
	edition = {2nd ed.},
	title = {Programming for computations - Python: a gentle introduction to numerical simulations with Python},
	isbn = {978-3-030-16876-6},
	series = {Texts in Computational Science and Engineering},
	shorttitle = {Programming for computations - Python},
	pagetotal = {323},
	number = {15},
	publisher = {Springer},
	author = {Linge, Svein and Langtangen, Hans Petter},
	date = {2020},
}

@book{lee_python_2011,
	location = {London ; New York},
	edition = {1st ed.},
	title = {Python programming fundamentals},
	isbn = {978-1-84996-536-1 978-1-84996-537-8},
	series = {Undergraduate topics in computer science},
	abstract = {This classroom-tested and easy-to-follow textbook teaches the reader how to program using Python, an accessible language which can be learned incrementally. Through an extensive use of examples and practical exercises, students will learn to recognize and apply abstract patterns in programming, as well as how to inspect the state of a program using a debugger tool},
	pagetotal = {241},
	publisher = {Springer},
	author = {Lee, Kent D.},
	date = {2011},
	keywords = {Python (Computer program language), Python (programmeertaal)},
}

@book{sundnes_introduction_2020,
	location = {Cham, Switzerland},
	title = {Introduction to scientific programming with Python},
	volume = {6},
	isbn = {978-3-030-50356-7 978-3-030-50355-0},
	series = {Simula {SpringerBriefs} on computing},
	pagetotal = {148},
	publisher = {Springer},
	author = {Sundnes, Joakim},
	date = {2020},
}

@book{illowsky_introductory_2018,
	title = {Introductory statistics},
	isbn = {978-1-938168-20-8},
	url = {https://openstax.org/details/books/introductory-statistics},
	abstract = {Introductory Statistics is designed for the one-semester, introduction to statistics course and is geared toward students majoring in fields other than math or engineering. This text assumes students have been exposed to intermediate algebra, and it focuses on the applications of statistical knowledge rather than the theory behind it. The foundation of this textbook is Collaborative Statistics, by Barbara Illowsky and Susan Dean.--Provided by publisher.},
	pagetotal = {913},
	publisher = {{OpenStax}},
	author = {Illowsky, Barbara and Dean, Susan},
	urldate = {2021-10-28},
	date = {2018},
	note = {{OCLC}: 1154530098},
}

@book{walrand_probability_2021,
	edition = {1st ed.},
	title = {Probability in Electrical Engineering and Computer Science An Application-Driven Course},
	isbn = {978-3-030-49995-2},
	url = {https://doi.org/10.1007/978-3-030-49995-2},
	abstract = {This revised textbook motivates and illustrates the techniques of applied probability by applications in electrical engineering and computer science ({EECS}). The author presents information processing and communication systems that use algorithms based on probabilistic models and techniques, including web searches, digital links, speech recognition, {GPS}, route planning, recommendation systems, classification, and estimation. He then explains how these applications work and, along the way, provides the readers with the understanding of the key concepts and methods of applied probability. Python labs enable the readers to experiment and consolidate their understanding. The book includes homework, solutions, and Jupyter notebooks. This edition includes new topics such as Boosting, Multi-armed bandits, statistical tests, social networks, queuing networks, and neural networks. The companion website now has many examples of Python demos and also Python labs used in Berkeley. Showcases techniques of applied probability with applications in {EE} and {CS}; Presents all topics with concrete applications so students see the relevance of the theory; Illustrates methods with Jupyter notebooks that use widgets to enable the users to modify parameters.},
	pagetotal = {380},
	author = {Walrand, Jean},
	urldate = {2021-11-02},
	date = {2021},
	note = {{OCLC}: 1266441293},
}

@book{grun_analysis_2010,
	location = {New York},
	title = {Analysis of parallel spike trains},
	volume = {7},
	isbn = {978-1-4419-5674-3 978-1-4419-5675-0},
	series = {Springer series in computational neuroscience},
	pagetotal = {441},
	publisher = {Springer},
	author = {Grün, Sonja and Rotter, Stefan},
	date = {2010},
	note = {{OCLC}: ocn528411076},
	keywords = {Action Potentials, Computational neuroscience, Electrophysiology, Methodology, methods, Models, Neurological, physiology},
}

@book{ermentrout_mathematical_2010,
	location = {New York, {NY}},
	title = {Mathematical foundations of neuroscience},
	volume = {35},
	isbn = {978-0-387-87708-2},
	series = {Interdisciplinary applied mathematics},
	abstract = {This book applies methods from nonlinear dynamics to problems in neuroscience. It uses modern mathematical approaches to understand patterns of neuronal activity seen in experiments and models of neuronal behavior. The intended audience is researchers interested in applying mathematics to important problems in neuroscience, and neuroscientists who would like to understand how to create models, as well as the mathematical and computational methods for analyzing them. The authors take a very broad approach and use many different methods to solve and understand complex models of neurons and circuits. They explain and combine numerical, analytical, dynamical systems and perturbation methods to produce a modern approach to the types of model equations that arise in neuroscience. There are extensive chapters on the role of noise, multiple time scales and spatial interactions in generating complex activity patterns found in experiments ; The early chapters require little more than basic calculus and some elementary differential equations and can form the core of a computational neuroscience course. Later chapters can be used as a basis for a graduate class and as a source for current research in mathematical neuroscience ; The book contains a large number of illustrations, chapter summaries and hundreds of exercises which are motivated by issues that arise in biology, and involve both computation and analysis. --Book Jacket},
	pagetotal = {422},
	publisher = {Springer},
	author = {Ermentrout, Bard and Terman, David Hillel},
	date = {2010},
}

@book{forsyth_probability_2018,
	location = {Cham},
	edition = {1st ed. 2018},
	title = {Probability and Statistics for Computer Science},
	isbn = {978-3-319-64410-3},
	abstract = {This textbook is aimed at computer science undergraduates late in sophomore or early in junior year, supplying a comprehensive background in qualitative and quantitative data analysis, probability, random variables, and statistical methods, including machine learning. With careful treatment of topics that fill the curricular needs for the course, Probability and Statistics for Computer Science features: - A treatment of random variables and expectations dealing primarily with the discrete case. - A practical treatment of simulation, showing how many interesting probabilities and expectations can be extracted, with particular emphasis on Markov chains. - A clear but crisp account of simple point inference strategies (maximum likelihood; Bayesian inference) in simple contexts. This is extended to cover some confidence intervals, samples and populations for random sampling with replacement, and the simplest hypothesis testing. - A chapter dealing with classification, explaining why it's useful; how to train {SVM} classifiers with stochastic gradient descent; and how to use implementations of more advanced methods such as random forests and nearest neighbors. - A chapter dealing with regression, explaining how to set up, use and understand linear regression and nearest neighbors regression in practical problems. - A chapter dealing with principal components analysis, developing intuition carefully, and including numerous practical examples. There is a brief description of multivariate scaling via principal coordinate analysis. - A chapter dealing with clustering via agglomerative methods and k-means, showing how to build vector quantized features for complex signals. Illustrated throughout, each main chapter includes many worked examples and other pedagogical elements such as boxed Procedures, Definitions, Useful Facts, and Remember This (short tips). Problems and Programming Exercises are at the end of each chapter, with a summary of what the reader should know. Instructor resources include a full set of model solutions for all problems, and an Instructor's Manual with accompanying presentation slides},
	pagetotal = {367},
	publisher = {Springer International Publishing : Imprint: Springer},
	author = {Forsyth, David},
	date = {2018},
	doi = {10.1007/978-3-319-64410-3},
	keywords = {Statistics, Mathematical statistics, Statistics and Computing/Statistics Programs, Computer simulation, Probability and Statistics in Computer Science, Simulation and Modeling},
}

@book{adkins_ordinary_2012,
	location = {New York},
	edition = {1st ed. 2012},
	title = {Ordinary Differential Equations},
	rights = {Springer Science+Business Media, {LLC}, part of Springer Nature},
	isbn = {978-1-4614-3618-8},
	url = {https://www.springer.com/gp/book/9781461436171},
	series = {Undergraduate Texts in Mathematics},
	abstract = {Unlike most texts in differential equations, this textbook gives an early presentation of the Laplace transform, which is then used to motivate and develop many of the remaining differential equation concepts for which it is particularly well suited. For example, the standard solution methods for constant coefficient linear differential equations are immediate and simplified, and solution methods for constant coefficient systems are streamlined. By introducing the Laplace transform early in the text, students become proficient in its use while at the same time learning the standard topics in differential equations. The text also includes proofs of several important theorems that are not usually given in introductory texts. These include a proof of the injectivity of the Laplace transform and a proof of the existence and uniqueness theorem for linear constant coefficient differential equations.Along with its unique traits, this text contains all the topics needed for a standard three- or four-hour, sophomore-level differential equations course for students majoring in science or engineering. These topics include: first order differential equations, general linear differential equations with constant coefficients, second order linear differential equations with variable coefficients, power series methods, and linear systems of differential equations. It is assumed that the reader has had the equivalent of a one-year course in college calculus.},
	pagetotal = {799},
	publisher = {Springer-Verlag},
	author = {Adkins, William and Davidson, Mark G.},
	urldate = {2021-10-05},
	date = {2012},
	langid = {english},
	doi = {10.1007/978-1-4614-3618-8},
}

@book{wasserman_all_2010,
	location = {New York Berlin Heidelberg},
	edition = {Corr. 2. print., [repr.]},
	title = {All of statistics: a concise course in statistical inference},
	isbn = {978-0-387-21736-9 978-1-4419-2322-6},
	series = {Springer texts in statistics},
	shorttitle = {All of statistics},
	pagetotal = {442},
	publisher = {Springer},
	author = {Wasserman, Larry},
	date = {2010},
}

@incollection{graham_modeling_2013,
	location = {New York, {NY}},
	title = {Modeling the Axon},
	isbn = {978-1-4614-7320-6},
	url = {https://doi.org/10.1007/978-1-4614-7320-6_241-1},
	pages = {1--9},
	booktitle = {Encyclopedia of Computational Neuroscience},
	publisher = {Springer},
	author = {Graham, Bruce},
	editor = {Jaeger, Dieter and Jung, Ranu},
	urldate = {2021-10-21},
	date = {2013},
	langid = {english},
	doi = {10.1007/978-1-4614-7320-6_241-1},
	keywords = {Action Potential Propagation, Branch Point, Daughter Branch, Myelinated Axon, Sodium Current},
	file = {Graham - 2013 - Modeling the Axon.pdf:/home/weverson/Zotero/storage/Q3HJ48Z9/Graham - 2013 - Modeling the Axon.pdf:application/pdf},
}

@article{lillicrap_backpropagation_2020,
	title = {Backpropagation and the brain},
	volume = {21},
	rights = {2020 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/s41583-020-0277-3},
	doi = {10.1038/s41583-020-0277-3},
	abstract = {During learning, the brain modifies synapses to improve behaviour. In the cortex, synapses are embedded within multilayered networks, making it difficult to determine the effect of an individual synaptic modification on the behaviour of the system. The backpropagation algorithm solves this problem in deep artificial neural networks, but historically it has been viewed as biologically problematic. Nonetheless, recent developments in neuroscience and the successes of artificial neural networks have reinvigorated interest in whether backpropagation offers insights for understanding learning in the cortex. The backpropagation algorithm learns quickly by computing synaptic updates using feedback connections to deliver error signals. Although feedback connections are ubiquitous in the cortex, it is difficult to see how they could deliver the error signals required by strict formulations of backpropagation. Here we build on past and recent developments to argue that feedback connections may instead induce neural activities whose differences can be used to locally approximate these signals and hence drive effective learning in deep networks in the brain.},
	pages = {335--346},
	number = {6},
	journaltitle = {Nature Reviews Neuroscience},
	shortjournal = {Nat Rev Neurosci},
	author = {Lillicrap, Timothy P. and Santoro, Adam and Marris, Luke and Akerman, Colin J. and Hinton, Geoffrey},
	urldate = {2021-10-21},
	date = {2020-06},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 6
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Cortex;Learning algorithms;Long-term potentiation;Network models;Neurophysiology
Subject\_term\_id: cortex;learning-algorithms;long-term-potentiation;network-models;neurophysiology},
	file = {Full Text PDF:/home/weverson/Zotero/storage/2CFBZD2J/Lillicrap et al. - 2020 - Backpropagation and the brain.pdf:application/pdf},
}

@article{koulakov_model_2002,
	title = {Model for a robust neural integrator},
	volume = {5},
	rights = {2002 Nature Publishing Group},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn893},
	doi = {10.1038/nn893},
	abstract = {Integrator circuits in the brain show persistent firing that reflects the sum of previous excitatory and inhibitory inputs from external sources. Integrator circuits have been implicated in parametric working memory, decision making and motor control. Previous work has shown that stable integrator function can be achieved by an excitatory recurrent neural circuit, provided synaptic strengths are tuned with extreme precision (better than 1\% accuracy). Here we show that integrator circuits can function without fine tuning if the neuronal units have bistable properties. Two specific mechanisms of bistability are analyzed, one based on local recurrent excitation, and the other on the voltage-dependence of the {NMDA} (N-methyl-D-aspartate) channel. Neither circuit requires fine tuning to perform robust integration, and the latter actually exploits the variability of neuronal conductances.},
	pages = {775--782},
	number = {8},
	journaltitle = {Nature Neuroscience},
	shortjournal = {Nat Neurosci},
	author = {Koulakov, Alexei A. and Raghavachari, Sridhar and Kepecs, Adam and Lisman, John E.},
	urldate = {2021-10-21},
	date = {2002-08},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 8
Primary\_atype: Research
Publisher: Nature Publishing Group},
	file = {Full Text PDF:/home/weverson/Zotero/storage/H2DGA5E2/Koulakov et al. - 2002 - Model for a robust neural integrator.pdf:application/pdf},
}

@article{lopez-osorio_neuromorphic_2021,
	title = {Neuromorphic adaptive spiking {CPG} towards bio-inspired locomotion of legged robots},
	url = {http://arxiv.org/abs/2101.09709},
	abstract = {In recent years, locomotion mechanisms exhibited by vertebrate animals have been the inspiration for the improvement in the performance of robotic systems. These mechanisms include the adaptability of their locomotion to any change registered in the environment through their biological sensors. In this regard, we aim to replicate such kind of adaptability in legged robots through a Spiking Central Pattern Generator. This Spiking Central Pattern Generator generates different locomotion (rhythmic) patterns which are driven by an external stimulus, that is, the output of a Force Sensitive Resistor connected to the robot to provide feedback. The Spiking Central Pattern Generator consists of a network of five populations of Leaky Integrate-and-Fire neurons designed with a specific topology in such a way that the rhythmic patterns can be generated and driven by the aforementioned external stimulus. Therefore, the locomotion of the end robotic platform (any-legged robot) can be adapted to the terrain by using any sensor as input. The Spiking Central Pattern Generator with adaptive learning has been numerically validated at software and hardware level, using the Brian 2 simulator and the {SpiNNaker} neuromorphic platform for the latest. In particular, our experiments clearly show an adaptation in the oscillation frequencies between the spikes produced in the populations of the Spiking Central Pattern Generator while the input stimulus varies. To validate the robustness and adaptability of the Spiking Central Pattern Generator, we have performed several tests by variating the output of the sensor. These experiments were carried out in Brian 2 and {SpiNNaker}; both implementations showed a similar behavior with a Pearson correlation coefficient of 0.905.},
	journaltitle = {{arXiv}:2101.09709 [cs]},
	author = {Lopez-Osorio, Pablo and Patino-Saucedo, Alberto and Dominguez-Morales, Juan P. and Rostro-Gonzalez, Horacio and Perez-Peña, Fernando},
	urldate = {2021-10-21},
	date = {2021-01-24},
	eprinttype = {arxiv},
	eprint = {2101.09709},
	note = {version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/weverson/Zotero/storage/L2T8M5EZ/Lopez-Osorio et al. - 2021 - Neuromorphic adaptive spiking CPG towards bio-insp.pdf:application/pdf},
}

@book{gerstner_neuronal_2014,
	location = {Cambridge, United Kingdom},
	title = {Neuronal dynamics: from single neurons to networks and models of cognition},
	isbn = {978-1-107-06083-8 978-1-107-63519-7},
	shorttitle = {Neuronal dynamics},
	pagetotal = {577},
	publisher = {Cambridge University Press},
	author = {Gerstner, Wulfram and Kistler, Werner M. and Naud, Richard and Paninski, Liam},
	date = {2014},
	keywords = {Cognitive neuroscience, Neural networks (Neurobiology), Neurobiology},
}

@book{dayan_theoretical_2001,
	location = {Cambridge, Mass},
	title = {Theoretical neuroscience: computational and mathematical modeling of neural systems},
	isbn = {978-0-262-04199-7},
	series = {Computational neuroscience},
	shorttitle = {Theoretical neuroscience},
	pagetotal = {460},
	publisher = {Massachusetts Institute of Technology Press},
	author = {Dayan, Peter and Abbott, L. F.},
	date = {2001},
	keywords = {Computational neuroscience, Computer simulation, Neural networks (Neurobiology), Human information processing},
}

@article{tian_neural_2021,
	title = {A neural architecture search based framework for liquid state machine design},
	volume = {443},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231221003325},
	doi = {10.1016/j.neucom.2021.02.076},
	abstract = {Liquid state machines ({LSMs}), also known as the recurrent version of spiking neural networks, have garnered significant research interest owing to their high computational power, biological plausibility, simple structure, and low training complexity. By exploring the design space in network architectures and parameters, recent works have demonstrated the great potential for improving the accuracy of {LSM} models with low complexity. However, these works are based on manually defined network architectures or predefined parameters, which may ignore the potential optimization of the architectures and parameters of {LSMs}. In this study, we propose a neural architecture search-based framework to explore the architecture and parameter design space for the automatic dataset-oriented {LSM} models. To manage the exponentially increasing design space, we adopt a three-step search for {LSMs}, including dynamic multiple-liquid architecture search in multiple layers, variations in the number of neurons in each liquid, and parameter search such as percentage connectivity and excitatory neuron ratio within each liquid. In addition, we propose the use of a simulated annealing algorithm to implement three-step heuristic search. Two datasets, including the image dataset of {NMNIST} and speech dataset of {FSDD}, were used to test the effectiveness of the proposed framework. Simulation results demonstrated that our framework can produce the dataset-oriented optimal {LSM} models with high accuracy and low complexity. The best classification accuracy on the two datasets with only 1000 spiking neurons was observed to be 92.5\% and 84\%. Meanwhile, the network connections of discovered optimal multiple-liquid {LSM} models for the two datasets, on average, were reduced by 56.3\% and 60.2\% separately compared with a single {LSM}. Furthermore, the total number of neurons in the optimal multiple-liquid {LSM} models on the two datasets was reduced by 20\% with an accuracy loss of only 0.5\%.},
	pages = {174--182},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Tian, Shuo and Qu, Lianhua and Wang, Lei and Hu, Kai and Li, Nan and Xu, Weixia},
	urldate = {2021-10-19},
	date = {2021-07-05},
	langid = {english},
	keywords = {Design space, Heuristic search, Liquid state machine, Neural architecture search, Spiking neural network},
	file = {Versão submetida:/home/weverson/Zotero/storage/5LXD2YWC/Tian et al. - 2021 - A neural architecture search based framework for l.pdf:application/pdf},
}

@article{saxe_if_2021,
	title = {If deep learning is the answer, what is the question?},
	volume = {22},
	rights = {2020 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/s41583-020-00395-8},
	doi = {10.1038/s41583-020-00395-8},
	abstract = {Neuroscience research is undergoing a minor revolution. Recent advances in machine learning and artificial intelligence research have opened up new ways of thinking about neural computation. Many researchers are excited by the possibility that deep neural networks may offer theories of perception, cognition and action for biological brains. This approach has the potential to radically reshape our approach to understanding neural systems, because the computations performed by deep networks are learned from experience, and not endowed by the researcher. If so, how can neuroscientists use deep networks to model and understand biological brains? What is the outlook for neuroscientists who seek to characterize computations or neural codes, or who wish to understand perception, attention, memory and executive functions? In this Perspective, our goal is to offer a road map for systems neuroscience research in the age of deep learning. We discuss the conceptual and methodological challenges of comparing behaviour, learning dynamics and neural representations in artificial and biological systems, and we highlight new research questions that have emerged for neuroscience as a direct consequence of recent advances in machine learning.},
	pages = {55--67},
	number = {1},
	journaltitle = {Nature Reviews Neuroscience},
	shortjournal = {Nat Rev Neurosci},
	author = {Saxe, Andrew and Nelli, Stephanie and Summerfield, Christopher},
	urldate = {2021-10-19},
	date = {2021-01},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Learning algorithms;Network models
Subject\_term\_id: learning-algorithms;network-models},
	file = {Full Text PDF:/home/weverson/Zotero/storage/8BRQTZPZ/Saxe et al. - 2021 - If deep learning is the answer, what is the questi.pdf:application/pdf},
}

@article{izhikevich_simple_2003,
	title = {Simple model of spiking neurons},
	volume = {14},
	issn = {1941-0093},
	doi = {10.1109/TNN.2003.820440},
	abstract = {A model is presented that reproduces spiking and bursting behavior of known types of cortical neurons. The model combines the biologically plausibility of Hodgkin-Huxley-type dynamics and the computational efficiency of integrate-and-fire neurons. Using this model, one can simulate tens of thousands of spiking cortical neurons in real time (1 ms resolution) using a desktop {PC}.},
	pages = {1569--1572},
	number = {6},
	journaltitle = {{IEEE} Transactions on Neural Networks},
	author = {Izhikevich, E.M.},
	date = {2003-11},
	note = {Conference Name: {IEEE} Transactions on Neural Networks},
	keywords = {Bifurcation, Biological system modeling, Biology computing, Biomembranes, Brain modeling, Computational modeling, Large-scale systems, Mathematical analysis, Mathematical model, Neurons},
	file = {IEEE Xplore Full Text PDF:/home/weverson/Zotero/storage/QHM9EXTV/Izhikevich - 2003 - Simple model of spiking neurons.pdf:application/pdf},
}

@article{shen_interactive_2014,
	title = {Interactive notebooks: Sharing the code},
	volume = {515},
	rights = {2014 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/515151a},
	doi = {10.1038/515151a},
	shorttitle = {Interactive notebooks},
	abstract = {The free {IPython} notebook makes data analysis easier to record, understand and reproduce.},
	pages = {151--152},
	number = {7525},
	journaltitle = {Nature},
	author = {Shen, Helen},
	urldate = {2021-10-11},
	date = {2014-11},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 7525
Primary\_atype: Special Features
Publisher: Nature Publishing Group
Subject\_term: Communication;Computational biology and bioinformatics;Information technology;Publishing
Subject\_term\_id: communication;computational-biology-and-bioinformatics;information-technology;publishing},
	file = {Full Text PDF:/home/weverson/Zotero/storage/J8HE5QA7/Shen - 2014 - Interactive notebooks Sharing the code.pdf:application/pdf},
}

@article{sandve_ten_2013,
	title = {Ten Simple Rules for Reproducible Computational Research},
	volume = {9},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003285},
	doi = {10.1371/journal.pcbi.1003285},
	pages = {e1003285},
	number = {10},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
	urldate = {2021-10-11},
	date = {2013-10-24},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Archives, Computer and information sciences, Computer applications, Genome analysis, Habits, Replication studies, Reproducibility, Source code},
	file = {Full Text PDF:/home/weverson/Zotero/storage/4Q3ZR4WH/Sandve et al. - 2013 - Ten Simple Rules for Reproducible Computational Re.pdf:application/pdf;Snapshot:/home/weverson/Zotero/storage/PF36H59Y/article.html:text/html},
}

@article{nordlie_towards_2009,
	title = {Towards Reproducible Descriptions of Neuronal Network Models},
	volume = {5},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000456},
	doi = {10.1371/journal.pcbi.1000456},
	abstract = {Progress in science depends on the effective exchange of ideas among scientists. New ideas can be assessed and criticized in a meaningful manner only if they are formulated precisely. This applies to simulation studies as well as to experiments and theories. But after more than 50 years of neuronal network simulations, we still lack a clear and common understanding of the role of computational models in neuroscience as well as established practices for describing network models in publications. This hinders the critical evaluation of network models as well as their re-use. We analyze here 14 research papers proposing neuronal network models of different complexity and find widely varying approaches to model descriptions, with regard to both the means of description and the ordering and placement of material. We further observe great variation in the graphical representation of networks and the notation used in equations. Based on our observations, we propose a good model description practice, composed of guidelines for the organization of publications, a checklist for model descriptions, templates for tables presenting model structure, and guidelines for diagrams of networks. The main purpose of this good practice is to trigger a debate about the communication of neuronal network models in a manner comprehensible to humans, as opposed to machine-readable model description languages. We believe that the good model description practice proposed here, together with a number of other recent initiatives on data-, model-, and software-sharing, may lead to a deeper and more fruitful exchange of ideas among computational neuroscientists in years to come. We further hope that work on standardized ways of describing—and thinking about—complex neuronal networks will lead the scientific community to a clearer understanding of high-level concepts in network dynamics, and will thus lead to deeper insights into the function of the brain.},
	pages = {e1000456},
	number = {8},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Nordlie, Eilen and Gewaltig, Marc-Oliver and Plesser, Hans Ekkehard},
	urldate = {2021-10-11},
	date = {2009-08-07},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Computational neuroscience, Computer software, Neurons, Network analysis, Neural networks, Scientists, Simulation and modeling, Synapses},
	file = {Full Text PDF:/home/weverson/Zotero/storage/QIAAMBCZ/Nordlie et al. - 2009 - Towards Reproducible Descriptions of Neuronal Netw.pdf:application/pdf},
}

@article{wang_computational_2020,
	title = {Computational neuroscience: a frontier of the 21st century},
	volume = {7},
	issn = {2095-5138},
	url = {https://doi.org/10.1093/nsr/nwaa129},
	doi = {10.1093/nsr/nwaa129},
	shorttitle = {Computational neuroscience},
	pages = {1418--1422},
	number = {9},
	journaltitle = {National Science Review},
	shortjournal = {National Science Review},
	author = {Wang, Xiao-Jing and Hu, Hailan and Huang, Chengcheng and Kennedy, Henry and Li, Chengyu Tony and Logothetis, Nikos and Lu, Zhong-Lin and Luo, Qingming and Poo, Mu-ming and Tsao, Doris and Wu, Si and Wu, Zhaohui and Zhang, Xu and Zhou, Douglas},
	urldate = {2021-10-11},
	date = {2020-09-12},
	file = {Full Text PDF:/home/weverson/Zotero/storage/KAD5UVQU/Wang et al. - 2020 - Computational neuroscience a frontier of the 21st.pdf:application/pdf},
}

@book{borges_python_2014,
	location = {São Paulo},
	title = {Python para Desenvolvedores: Aborda Python 3.3},
	rights = {Novatec Editora Ltda.},
	isbn = {978-85-7522-405-2},
	url = {https://novatec.com.br/livros/python-para-desenvolvedores/},
	abstract = {Entenda os principais recursos da linguagem de programação Python, desde os conceitos mais básicos até recursos avançados, como a integração com outras linguagens. Indicado para desenvolvedores que já têm experiência e desejam um curso rápido e prático de Python sem ter de consultar volumosos livros e manuais.},
	pagetotal = {320},
	publisher = {Novatec Editora},
	author = {Borges, Luiz Eduardo},
	urldate = {2021-10-11},
	date = {2014},
	langid = {portuguese},
	file = {Snapshot:/home/weverson/Zotero/storage/Q2V5VKK7/python-para-desenvolvedores.html:text/html},
}

@article{crisp_models_2019,
	title = {Models for Spiking Neurons: Integrate-and-Fire Units and Relaxation Oscillators},
	volume = {17},
	issn = {1544-2896},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6650253/},
	shorttitle = {Models for Spiking Neurons},
	abstract = {Relaxation oscillators are nonlinear electronic circuits that produce a repetitive non-sinusoidal waveform when sufficient voltage is applied. In this fashion, they are reminiscent of integrate-and-fire neuron models, except that they also include components with hysteresis, and thus require no threshold rule to determine when an impulse has occurred or to return the voltage to its reset value. Here, I discuss the pros and cons of teaching elementary neurophysiology using first-order linear integrate-and-fire neurons versus relaxation oscillator circuits. I suggest that the shortcomings of both types of models are useful in order to foster a critical understanding of the neurophysiology underlying the firing dynamics of biological neurons.},
	pages = {E7--E12},
	number = {2},
	journaltitle = {Journal of Undergraduate Neuroscience Education},
	shortjournal = {J Undergrad Neurosci Educ},
	author = {Crisp, Kevin},
	urldate = {2021-10-11},
	date = {2019-06-30},
	pmid = {31360134},
	pmcid = {PMC6650253},
	file = {PubMed Central Full Text PDF:/home/weverson/Zotero/storage/DE2Y2JKP/Crisp - 2019 - Models for Spiking Neurons Integrate-and-Fire Uni.pdf:application/pdf},
}

@article{virgilio_g_spiking_2020,
	title = {Spiking Neural Networks applied to the classification of motor tasks in {EEG} signals},
	volume = {122},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608019303193},
	doi = {10.1016/j.neunet.2019.09.037},
	abstract = {Motivated by the recent progress of Spiking Neural Network ({SNN}) models in pattern recognition, we report on the development and evaluation of brain signal classifiers based on {SNNs}. The work shows the capabilities of this type of Spiking Neurons in the recognition of motor imagery tasks from {EEG} signals and compares their performance with other traditional classifiers commonly used in this application. This work includes two stages: the first stage consists of comparing the performance of the {SNN} models against some traditional neural network models. The second stage, compares the {SNN} models performance in two input conditions: input features with constant values and input features with temporal information. The {EEG} signals employed in this work represent five motor imagery tasks: i.e. rest, left hand, right hand, foot and tongue movements. These {EEG} signals were obtained from a public database provided by the Technological University of Graz (Brunner et al., 2008). The feature extraction stage was performed by applying two algorithms: power spectral density and wavelet decomposition. Likewise, this work uses raw {EEG} signals for the second stage of the problem solution. All of the models were evaluated in the classification between two motor imagery tasks. This work demonstrates that with a smaller number of Spiking neurons, simple problems can be solved. Better results are obtained by using patterns with temporal information, thereby exploiting the capabilities of the {SNNs}.},
	pages = {130--143},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Virgilio G., Carlos D. and Sossa A., Juan H. and Antelis, Javier M. and Falcón, Luis E.},
	urldate = {2021-10-11},
	date = {2020-02-01},
	langid = {english},
	keywords = {{EEG} signals, Izhikevich model, Motor imagery, Power Spectral Density, Spiking Neural Network, Wavelet Decomposition},
}

@book{montgomery_estatistica_2018,
	location = {Rio de Janeiro},
	edition = {6},
	title = {Estatística aplicada e probabilidade para engenheiros},
	rights = {{LTC} - Livros Técnicos e Científicos Editora Ltda.},
	isbn = {978-85-216-3241-2},
	publisher = {{LTC}},
	author = {Montgomery, Douglas C. and Runger, George C.},
	date = {2018},
	langid = {portuguese},
}

@article{stimberg_brian_2019,
	title = {Brian 2, an intuitive and efficient neural simulator},
	volume = {8},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.47314},
	doi = {10.7554/eLife.47314},
	abstract = {Brian 2 allows scientists to simply and efficiently simulate spiking neural network models. These models can feature novel dynamical equations, their interactions with the environment, and experimental protocols. To preserve high performance when defining new models, most simulators offer two options: low-level programming or description languages. The first option requires expertise, is prone to errors, and is problematic for reproducibility. The second option cannot describe all aspects of a computational experiment, such as the potentially complex logic of a stimulation protocol. Brian addresses these issues using runtime code generation. Scientists write code with simple and concise high-level descriptions, and Brian transforms them into efficient low-level code that can run interleaved with their code. We illustrate this with several challenging examples: a plastic model of the pyloric network, a closed-loop sensorimotor model, a programmatic exploration of a neuron model, and an auditory model with real-time input.},
	pages = {e47314},
	journaltitle = {{eLife}},
	author = {Stimberg, Marcel and Brette, Romain and Goodman, Dan {FM}},
	editor = {Skinner, Frances K and Calabrese, Ronald L and Skinner, Frances K and Zeldenrust, Fleur and Gerkin, Richard C},
	urldate = {2021-10-05},
	date = {2019-08-20},
	note = {Publisher: {eLife} Sciences Publications, Ltd},
	keywords = {computational neuroscience, simulation, software},
	file = {Full Text PDF:/home/weverson/Zotero/storage/48RJHJ9E/Stimberg et al. - 2019 - Brian 2, an intuitive and efficient neural simulat.pdf:application/pdf},
}

@book{carnevale_neuron_2006,
	location = {Cambridge},
	title = {The {NEURON} Book},
	isbn = {978-0-521-84321-8},
	url = {https://www.cambridge.org/core/books/neuron-book/7C8D9BD861D288E658BEB652F593F273},
	abstract = {The authoritative reference on {NEURON}, the simulation environment for modeling biological neurons and neural networks that enjoys wide use in the experimental and computational neuroscience communities. This book shows how to use {NEURON} to construct and apply empirically based models. Written primarily for neuroscience investigators, teachers, and students, it assumes no previous knowledge of computer programming or numerical methods. Readers with a background in the physical sciences or mathematics, who have some knowledge about brain cells and circuits and are interested in computational modeling, will also find it helpful. The {NEURON} Book covers material that ranges from the inner workings of this program, to practical considerations involved in specifying the anatomical and biophysical properties that are to be represented in models. It uses a problem-solving approach, with many working examples that readers can try for themselves.},
	publisher = {Cambridge University Press},
	author = {Carnevale, Nicholas T. and Hines, Michael L.},
	urldate = {2021-10-05},
	date = {2006},
	doi = {10.1017/CBO9780511541612},
}

@article{syahid_simulation_2021,
	title = {Simulation of spiking activities neuron models using the Euler method},
	volume = {1951},
	issn = {1742-6596},
	url = {https://doi.org/10.1088/1742-6596/1951/1/012065},
	doi = {10.1088/1742-6596/1951/1/012065},
	abstract = {Simulation of neuron spiking activity models has been carried out using the Euler method. This study aims to simulate spiking activity in a neuron model. The neuron model used is the Hodgkin-Huxley neuron model, Integrate and Fire neuron model, Wilson neuron model, and Izhikevich neuron model. The research was conducted by implementing the mathematical equations of each neuron model used and then recording the membrane potential changes from time to time using the Euler method in {MATLAB}. The different forms of spiking activity were done by varying the variable’s value in each mathematical equation of a neuron model that describes the processing of action potentials (spikes) influenced by ion channel activity. The results showed that the Integrate and Fire neuron models produce regular spiking ({RS}), Hodgkin-Huxley neuron models have regular spiking ({RS}) forms, Wilson neuron models produce regular spiking ({RS}), fast-spiking ({FS}), and intrinsic bursting ({IB}), Izhikevich neuron model produces regular spiking ({RS}), fast-spiking ({FS}), intrinsic bursting ({IB}), chattering neurons ({CH}), and low threshold spiking ({LTS}). The complexity of the variables used and the spiking activity generated by each neuron model can provide an overview of computational efficiency and proximity to actual biological neurons.},
	pages = {012065},
	number = {1},
	journaltitle = {Journal of Physics: Conference Series},
	shortjournal = {J. Phys.: Conf. Ser.},
	author = {Syahid, A. and Yuniati, A.},
	urldate = {2021-10-05},
	date = {2021-06},
	langid = {english},
	note = {Publisher: {IOP} Publishing},
	file = {IOP Full Text PDF:/home/weverson/Zotero/storage/WXEHRCFW/Syahid e Yuniati - 2021 - Simulation of spiking activities neuron models usi.pdf:application/pdf},
}

@article{yuste_cortex_2005,
	title = {The cortex as a central pattern generator},
	volume = {6},
	rights = {2005 Nature Publishing Group},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/nrn1686},
	doi = {10.1038/nrn1686},
	abstract = {Vertebrate spinal cord and brainstem central pattern generator ({CPG}) circuits share profound similarities with neocortical circuits. {CPGs} can produce meaningful functional output in the absence of sensory inputs. Neocortical circuits could be considered analogous to {CPGs} as they have rich spontaneous dynamics that, similar to {CPGs}, are powerfully modulated or engaged by sensory inputs, but can also generate output in their absence. We find compelling evidence for this argument at the anatomical, biophysical, developmental, dynamic and pathological levels of analysis. Although it is possible that cortical circuits are particularly plastic types of {CPG} ('learning {CPGs}'), we argue that present knowledge about {CPGs} is likely to foretell the basic principles of the organization and dynamic function of cortical circuits.},
	pages = {477--483},
	number = {6},
	journaltitle = {Nature Reviews Neuroscience},
	shortjournal = {Nat Rev Neurosci},
	author = {Yuste, Rafael and {MacLean}, Jason N. and Smith, Jeffrey and Lansner, Anders},
	urldate = {2021-10-05},
	date = {2005-06},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 6
Primary\_atype: Reviews
Publisher: Nature Publishing Group},
	file = {Full Text PDF:/home/weverson/Zotero/storage/VXKD82QN/Yuste et al. - 2005 - The cortex as a central pattern generator.pdf:application/pdf},
}

@article{prat-ortega_flexible_2021,
	title = {Flexible categorization in perceptual decision making},
	volume = {12},
	rights = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-21501-z},
	doi = {10.1038/s41467-021-21501-z},
	abstract = {Perceptual decisions rely on accumulating sensory evidence. This computation has been studied using either drift diffusion models or neurobiological network models exhibiting winner-take-all attractor dynamics. Although both models can account for a large amount of data, it remains unclear whether their dynamics are qualitatively equivalent. Here we show that in the attractor model, but not in the drift diffusion model, an increase in the stimulus fluctuations or the stimulus duration promotes transitions between decision states. The increase in the number of transitions leads to a crossover between weighting mostly early evidence (primacy) to weighting late evidence (recency), a prediction we validate with psychophysical data. Between these two limiting cases, we found a novel flexible categorization regime, in which fluctuations can reverse initially-incorrect categorizations. This reversal asymmetry results in a non-monotonic psychometric curve, a distinctive feature of the attractor model. Our findings point to correcting decision reversals as an important feature of perceptual decision making.},
	pages = {1283},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Prat-Ortega, Genís and Wimmer, Klaus and Roxin, Alex and de la Rocha, Jaime},
	urldate = {2021-10-01},
	date = {2021-02-24},
	langid = {english},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Cognitive neuroscience;Computational neuroscience;Network models;Neural circuits;Sensory processing
Subject\_term\_id: cognitive-neuroscience;computational-neuroscience;network-models;neural-circuit;sensory-processing},
	file = {Full Text PDF:/home/weverson/Zotero/storage/HPWDR37R/Prat-Ortega et al. - 2021 - Flexible categorization in perceptual decision mak.pdf:application/pdf},
}

@article{herz_modeling_2006,
	title = {Modeling Single-Neuron Dynamics and Computations: A Balance of Detail and Abstraction},
	volume = {314},
	url = {https://www.science.org/doi/10.1126/science.1127240},
	doi = {10.1126/science.1127240},
	shorttitle = {Modeling Single-Neuron Dynamics and Computations},
	pages = {80--85},
	number = {5796},
	journaltitle = {Science},
	author = {Herz, Andreas V. M. and Gollisch, Tim and Machens, Christian K. and Jaeger, Dieter},
	urldate = {2021-10-01},
	date = {2006-10-06},
	note = {Publisher: American Association for the Advancement of Science},
	file = {Herz et al. - 2006 - Modeling Single-Neuron Dynamics and Computations .pdf:/home/weverson/Zotero/storage/7LSPDVAL/Herz et al. - 2006 - Modeling Single-Neuron Dynamics and Computations .pdf:application/pdf},
}

@article{ratcliff_diffusion_2016,
	title = {Diffusion Decision Model: Current Issues and History},
	volume = {20},
	issn = {1364-6613},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661316000255},
	doi = {10.1016/j.tics.2016.01.007},
	shorttitle = {Diffusion Decision Model},
	abstract = {There is growing interest in diffusion models to represent the cognitive and neural processes of speeded decision making. Sequential-sampling models like the diffusion model have a long history in psychology. They view decision making as a process of noisy accumulation of evidence from a stimulus. The standard model assumes that evidence accumulates at a constant rate during the second or two it takes to make a decision. This process can be linked to the behaviors of populations of neurons and to theories of optimality. Diffusion models have been used successfully in a range of cognitive tasks and as psychometric tools in clinical research to examine individual differences. In this review, we relate the models to both earlier and more recent research in psychology.},
	pages = {260--281},
	number = {4},
	journaltitle = {Trends in Cognitive Sciences},
	shortjournal = {Trends in Cognitive Sciences},
	author = {Ratcliff, Roger and Smith, Philip L. and Brown, Scott D. and {McKoon}, Gail},
	urldate = {2021-10-01},
	date = {2016-04-01},
	langid = {english},
	keywords = {diffusion model, nonstationarity, optimality, response time},
	file = {ScienceDirect Full Text PDF:/home/weverson/Zotero/storage/X4M6I8J2/Ratcliff et al. - 2016 - Diffusion Decision Model Current Issues and Histo.pdf:application/pdf},
}

@article{neymotin_ketamine_2011,
	title = {Ketamine Disrupts Theta Modulation of Gamma in a Computer Model of Hippocampus},
	volume = {31},
	rights = {Copyright © 2011 the authors 0270-6474/11/3111733-11\$15.00/0},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/31/32/11733},
	doi = {10.1523/JNEUROSCI.0501-11.2011},
	abstract = {Abnormalities in oscillations have been suggested to play a role in schizophrenia. We studied theta-modulated gamma oscillations in a computer model of hippocampal {CA}3 in vivo with and without simulated application of ketamine, an {NMDA} receptor antagonist and psychotomimetic. Networks of 1200 multicompartment neurons [pyramidal, basket, and oriens-lacunosum moleculare ({OLM}) cells] generated theta and gamma oscillations from intrinsic network dynamics: basket cells primarily generated gamma and amplified theta, while {OLM} cells strongly contributed to theta. Extrinsic medial septal inputs paced theta and amplified both theta and gamma oscillations. Exploration of {NMDA} receptor reduction across all location combinations demonstrated that the experimentally observed ketamine effect occurred only with isolated reduction of {NMDA} receptors on {OLMs}. In the ketamine simulations, lower {OLM} activity reduced theta power and disinhibited pyramidal cells, resulting in increased basket cell activation and gamma power. Our simulations predict the following: (1) ketamine increases firing rates; (2) oscillations can be generated by intrinsic hippocampal circuits; (3) medial-septum inputs pace and augment oscillations; (4) pyramidal cells lead basket cells at the gamma peak but lag at trough; (5) basket cells amplify theta rhythms; (6) ketamine alters oscillations due to primary blockade at {OLM} {NMDA} receptors; (7) ketamine alters phase relationships of cell firing; (8) ketamine reduces network responsivity to the environment; (9) ketamine effect could be reversed by providing a continuous inward current to {OLM} cells. We suggest that this last prediction has implications for a possible novel treatment for cognitive deficits of schizophrenia by targeting {OLM} cells.},
	pages = {11733--11743},
	number = {32},
	journaltitle = {Journal of Neuroscience},
	shortjournal = {J. Neurosci.},
	author = {Neymotin, Samuel A. and Lazarewicz, Maciej T. and Sherif, Mohamed and Contreras, Diego and Finkel, Leif H. and Lytton, William W.},
	urldate = {2021-10-01},
	date = {2011-08-10},
	langid = {english},
	pmid = {21832203},
	note = {Publisher: Society for Neuroscience
Section: Articles},
	file = {Full Text PDF:/home/weverson/Zotero/storage/2YJFAKU7/Neymotin et al. - 2011 - Ketamine Disrupts Theta Modulation of Gamma in a C.pdf:application/pdf},
}

@article{heusser_episodic_2016,
	title = {Episodic sequence memory is supported by a theta–gamma phase code},
	volume = {19},
	rights = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn.4374},
	doi = {10.1038/nn.4374},
	abstract = {A core aspect of human episodic memory is the ability to recall events in the order that they were experienced. The authors found that successful memory for order is related to the precise timing of high frequency brain activity with respect to slower underlying rhythms.},
	pages = {1374--1380},
	number = {10},
	journaltitle = {Nature Neuroscience},
	shortjournal = {Nat Neurosci},
	author = {Heusser, Andrew C. and Poeppel, David and Ezzyat, Youssef and Davachi, Lila},
	urldate = {2021-10-01},
	date = {2016-10},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 10
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Cognitive neuroscience;Learning and memory;Psychology
Subject\_term\_id: cognitive-neuroscience;learning-and-memory;psychology},
	file = {Full Text PDF:/home/weverson/Zotero/storage/HQ87DS3H/Heusser et al. - 2016 - Episodic sequence memory is supported by a theta–g.pdf:application/pdf},
}

@article{tully_spike-based_2016,
	title = {Spike-Based Bayesian-Hebbian Learning of Temporal Sequences},
	volume = {12},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004954},
	doi = {10.1371/journal.pcbi.1004954},
	abstract = {Many cognitive and motor functions are enabled by the temporal representation and processing of stimuli, but it remains an open issue how neocortical microcircuits can reliably encode and replay such sequences of information. To better understand this, a modular attractor memory network is proposed in which meta-stable sequential attractor transitions are learned through changes to synaptic weights and intrinsic excitabilities via the spike-based Bayesian Confidence Propagation Neural Network ({BCPNN}) learning rule. We find that the formation of distributed memories, embodied by increased periods of firing in pools of excitatory neurons, together with asymmetrical associations between these distinct network states, can be acquired through plasticity. The model’s feasibility is demonstrated using simulations of adaptive exponential integrate-and-fire model neurons ({AdEx}). We show that the learning and speed of sequence replay depends on a confluence of biophysically relevant parameters including stimulus duration, level of background noise, ratio of synaptic currents, and strengths of short-term depression and adaptation. Moreover, sequence elements are shown to flexibly participate multiple times in the sequence, suggesting that spiking attractor networks of this type can support an efficient combinatorial code. The model provides a principled approach towards understanding how multiple interacting plasticity mechanisms can coordinate hetero-associative learning in unison.},
	pages = {e1004954},
	number = {5},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Tully, Philip J. and Lindén, Henrik and Hennig, Matthias H. and Lansner, Anders},
	urldate = {2021-10-01},
	date = {2016-05-23},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Learning, Neurons, Neural networks, Synapses, Action potentials, Memory recall, Neuronal plasticity, Synaptic plasticity},
	file = {Full Text PDF:/home/weverson/Zotero/storage/8LARUG83/Tully et al. - 2016 - Spike-Based Bayesian-Hebbian Learning of Temporal .pdf:application/pdf},
}

@article{boerlin_spike-based_2011,
	title = {Spike-Based Population Coding and Working Memory},
	volume = {7},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1001080},
	doi = {10.1371/journal.pcbi.1001080},
	abstract = {Compelling behavioral evidence suggests that humans can make optimal decisions despite the uncertainty inherent in perceptual or motor tasks. A key question in neuroscience is how populations of spiking neurons can implement such probabilistic computations. In this article, we develop a comprehensive framework for optimal, spike-based sensory integration and working memory in a dynamic environment. We propose that probability distributions are inferred spike-per-spike in recurrently connected networks of integrate-and-fire neurons. As a result, these networks can combine sensory cues optimally, track the state of a time-varying stimulus and memorize accumulated evidence over periods much longer than the time constant of single neurons. Importantly, we propose that population responses and persistent working memory states represent entire probability distributions and not only single stimulus values. These memories are reflected by sustained, asynchronous patterns of activity which make relevant information available to downstream neurons within their short time window of integration. Model neurons act as predictive encoders, only firing spikes which account for new information that has not yet been signaled. Thus, spike times signal deterministically a prediction error, contrary to rate codes in which spike times are considered to be random samples of an underlying firing rate. As a consequence of this coding scheme, a multitude of spike patterns can reliably encode the same information. This results in weakly correlated, Poisson-like spike trains that are sensitive to initial conditions but robust to even high levels of external neural noise. This spike train variability reproduces the one observed in cortical sensory spike trains, but cannot be equated to noise. On the contrary, it is a consequence of optimal spike-based inference. In contrast, we show that rate-based models perform poorly when implemented with stochastically spiking neurons.},
	pages = {e1001080},
	number = {2},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Boerlin, Martin and Denève, Sophie},
	urldate = {2021-10-01},
	date = {2011-02-17},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Neurons, Neural networks, Action potentials, Memory, Neuronal tuning, Sensory cues, Sensory perception, Working memory},
	file = {Full Text PDF:/home/weverson/Zotero/storage/YNGVFNJS/Boerlin e Denève - 2011 - Spike-Based Population Coding and Working Memory.pdf:application/pdf},
}

@article{okamoto_three-dimensional_2004,
	title = {Three-dimensional probabilistic anatomical cranio-cerebral correlation via the international 10–20 system oriented for transcranial functional brain mapping},
	volume = {21},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811903005366},
	doi = {10.1016/j.neuroimage.2003.08.026},
	abstract = {The recent advent of multichannel near-infrared spectroscopy ({NIRS}) has expanded its technical potential for human brain mapping. However, {NIRS} measurement has a technical drawback in that it measures cortical activities from the head surface without anatomical information of the object to be measured. This problem is also found in transcranial magnetic stimulation ({TMS}) that transcranially activates or inactivates the cortical surface. To overcome this drawback, we examined cranio-cerebral correlation using magnetic resonance imaging ({MRI}) via the guidance of the international 10–20 system for electrode placement, which had originally been developed for electroencephalography. We projected the 10–20 standard cranial positions over the cerebral cortical surface. After examining the cranio-cerebral correspondence for 17 healthy adults, we normalized the 10–20 cortical projection points of the subjects to the standard Montreal Neurological Institute ({MNI}) and Talairach stereotactic coordinates and obtained their probabilistic distributions. We also expressed the anatomical structures for the 10–20 cortical projection points probabilistically. Next, we examined the distance between the cortical surface and the head surface along the scalp and created a cortical surface depth map. We found that the locations of 10–20 cortical projection points in the standard {MNI} or Talairach space could be estimated with an average standard deviation of 8 mm. This study provided an initial step toward establishing a three-dimensional probabilistic anatomical platform that enables intra- and intermodal comparisons of {NIRS} and {TMS} brain imaging data.},
	pages = {99--111},
	number = {1},
	journaltitle = {{NeuroImage}},
	shortjournal = {{NeuroImage}},
	author = {Okamoto, Masako and Dan, Haruka and Sakamoto, Kuniko and Takeo, Kazuhiro and Shimizu, Koji and Kohno, Satoru and Oda, Ichiro and Isobe, Seiichiro and Suzuki, Tateo and Kohyama, Kaoru and Dan, Ippeita},
	urldate = {2021-10-01},
	date = {2004-01-01},
	langid = {english},
	keywords = {Human brain mapping, Near-infrared spectroscopy, Probabilistic anatomical platform, Transcranial magnetic stimulation},
	file = {ScienceDirect Full Text PDF:/home/weverson/Zotero/storage/YLWKEJP5/Okamoto et al. - 2004 - Three-dimensional probabilistic anatomical cranio-.pdf:application/pdf},
}

@article{kaya_large_2018,
	title = {A large electroencephalographic motor imagery dataset for electroencephalographic brain computer interfaces},
	volume = {5},
	rights = {2018 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata2018211},
	doi = {10.1038/sdata.2018.211},
	abstract = {Recent advancements in brain computer interfaces ({BCI}) have demonstrated control of robotic systems by mental processes alone. Together with invasive {BCI}, electroencephalographic ({EEG}) {BCI} represent an important direction in the development of {BCI} systems. In the context of {EEG} {BCI}, the processing of {EEG} data is the key challenge. Unfortunately, advances in that direction have been complicated by a lack of large and uniform datasets that could be used to design and evaluate different data processing approaches. In this work, we release a large set of {EEG} {BCI} data collected during the development of a slow cortical potentials-based {EEG} {BCI}. The dataset contains 60 h of {EEG} recordings, 13 participants, 75 recording sessions, 201 individual {EEG} {BCI} interaction session-segments, and over 60 000 examples of motor imageries in 4 interaction paradigms. The current dataset presents one of the largest {EEG} {BCI} datasets publically available to date.},
	pages = {180211},
	number = {1},
	journaltitle = {Scientific Data},
	shortjournal = {Sci Data},
	author = {Kaya, Murat and Binli, Mustafa Kemal and Ozbay, Erkan and Yanar, Hilmi and Mishchenko, Yuriy},
	urldate = {2021-10-01},
	date = {2018-10-16},
	langid = {english},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_publicdomain
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Biomedical engineering;Computational neuroscience
Subject\_term\_id: biomedical-engineering;computational-neuroscience},
	file = {Full Text PDF:/home/weverson/Zotero/storage/WDWSQS7A/Kaya et al. - 2018 - A large electroencephalographic motor imagery data.pdf:application/pdf},
}

@article{thorpe_spike-based_2001,
	title = {Spike-based strategies for rapid processing},
	volume = {14},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608001000831},
	doi = {10.1016/S0893-6080(01)00083-1},
	abstract = {Most experimental and theoretical studies of brain function assume that neurons transmit information as a rate code, but recent studies on the speed of visual processing impose temporal constraints that appear incompatible with such a coding scheme. Other coding schemes that use the pattern of spikes across a population a neurons may be much more efficient. For example, since strongly activated neurons tend to fire first, one can use the order of firing as a code. We argue that Rank Order Coding is not only very efficient, but also easy to implement in biological hardware: neurons can be made sensitive to the order of activation of their inputs by including a feed-forward shunting inhibition mechanism that progressively desensitizes the neuronal population during a wave of afferent activity. In such a case, maximum activation will only be produced when the afferent inputs are activated in the order of their synaptic weights.},
	pages = {715--725},
	number = {6},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Thorpe, Simon and Delorme, Arnaud and Van Rullen, Rufin},
	urldate = {2021-10-01},
	date = {2001-07-09},
	langid = {english},
	keywords = {Information, Latency, Rank Order Coding, Rapid visual processing, Retina, Spikes},
	file = {ScienceDirect Full Text PDF:/home/weverson/Zotero/storage/JZHBCNAD/Thorpe et al. - 2001 - Spike-based strategies for rapid processing.pdf:application/pdf},
}

@article{lee_enabling_2020,
	title = {Enabling Spike-Based Backpropagation for Training Deep Neural Network Architectures},
	volume = {14},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/article/10.3389/fnins.2020.00119},
	doi = {10.3389/fnins.2020.00119},
	abstract = {Spiking Neural Networks ({SNNs}) have recently emerged as a prominent neural computing paradigm. However, the typical shallow {SNN} architectures have limited capacity for expressing complex representations while training deep {SNNs} using input spikes has not been successful so far. Diverse methods have been proposed to get around this issue such as converting off-the-shelf trained deep Artificial Neural Networks ({ANNs}) to {SNNs}. However, the {ANN}-{SNN} conversion scheme fails to capture the temporal dynamics of a spiking system. On the other hand, it is still a difficult problem to directly train deep {SNNs} using input spike events due to the discontinuous, non-differentiable nature of the spike generation function. To overcome this problem, we propose an approximate derivative method that accounts for the leaky behavior of {LIF} neurons. This method enables training deep convolutional {SNNs} directly (with input spike events) using spike-based backpropagation. Our experiments show the effectiveness of the proposed spike-based learning on deep networks ({VGG} and Residual architectures) by achieving the best classification accuracies in {MNIST}, {SVHN}, and {CIFAR}-10 datasets compared to other {SNNs} trained with a spike-based learning. Moreover, we analyze sparse event-based computations to demonstrate the efficacy of the proposed {SNN} training method for inference operation in the spiking domain.},
	pages = {119},
	journaltitle = {Frontiers in Neuroscience},
	author = {Lee, Chankyu and Sarwar, Syed Shakib and Panda, Priyadarshini and Srinivasan, Gopalakrishnan and Roy, Kaushik},
	urldate = {2021-10-01},
	date = {2020},
	file = {Full Text PDF:/home/weverson/Zotero/storage/U27JIEMK/Lee et al. - 2020 - Enabling Spike-Based Backpropagation for Training .pdf:application/pdf},
}

@article{teramae_optimal_2012,
	title = {Optimal spike-based communication in excitable networks with strong-sparse and weak-dense links},
	volume = {2},
	rights = {2012 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/srep00485},
	doi = {10.1038/srep00485},
	abstract = {The connectivity of complex networks and functional implications has been attracting much interest in many physical, biological and social systems. However, the significance of the weight distributions of network links remains largely unknown except for uniformly- or Gaussian-weighted links. Here, we show analytically and numerically, that recurrent neural networks can robustly generate internal noise optimal for spike transmission between neurons with the help of a long-tailed distribution in the weights of recurrent connections. The structure of spontaneous activity in such networks involves weak-dense connections that redistribute excitatory activity over the network as noise sources to optimally enhance the responses of individual neurons to input at sparse-strong connections, thus opening multiple signal transmission pathways. Electrophysiological experiments confirm the importance of a highly broad connectivity spectrum supported by the model. Our results identify a simple network mechanism for internal noise generation by highly inhomogeneous connection strengths supporting both stability and optimal communication.},
	pages = {485},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Teramae, Jun-nosuke and Tsubo, Yasuhiro and Fukai, Tomoki},
	urldate = {2021-10-01},
	date = {2012-07-02},
	langid = {english},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_y
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Physics;Statistical physics, thermodynamics and nonlinear dynamics;Synaptic transmission
Subject\_term\_id: physics;statistical-physics-thermodynamics-and-nonlinear-dynamics;synaptic-transmission},
	file = {Full Text PDF:/home/weverson/Zotero/storage/T6CHXXA5/Teramae et al. - 2012 - Optimal spike-based communication in excitable net.pdf:application/pdf},
}

@article{brette_philosophy_2015,
	title = {Philosophy of the Spike: Rate-Based vs. Spike-Based Theories of the Brain},
	volume = {9},
	issn = {1662-5137},
	url = {https://www.frontiersin.org/article/10.3389/fnsys.2015.00151},
	doi = {10.3389/fnsys.2015.00151},
	shorttitle = {Philosophy of the Spike},
	abstract = {Does the brain use a firing rate code or a spike timing code? Considering this controversial question from an epistemological perspective, I argue that progress has been hampered by its problematic phrasing. It takes the perspective of an external observer looking at whether those two observables vary with stimuli, and thereby misses the relevant question: which one has a causal role in neural activity? When rephrased in a more meaningful way, the rate-based view appears as an ad hoc methodological postulate, one that is practical but with virtually no empirical or theoretical support.},
	pages = {151},
	journaltitle = {Frontiers in Systems Neuroscience},
	author = {Brette, Romain},
	urldate = {2021-10-01},
	date = {2015},
	file = {Full Text PDF:/home/weverson/Zotero/storage/XZCCFUA8/Brette - 2015 - Philosophy of the Spike Rate-Based vs. Spike-Base.pdf:application/pdf},
}

@article{huh_gradient_2017,
	title = {Gradient Descent for Spiking Neural Networks},
	url = {http://arxiv.org/abs/1706.04698},
	abstract = {Much of studies on neural computation are based on network models of static neurons that produce analog output, despite the fact that information processing in the brain is predominantly carried out by dynamic neurons that produce discrete pulses called spikes. Research in spike-based computation has been impeded by the lack of efﬁcient supervised learning algorithm for spiking networks. Here, we present a gradient descent method for optimizing spiking network models by introducing a differentiable formulation of spiking networks and deriving the exact gradient calculation. For demonstration, we trained recurrent spiking networks on two dynamic tasks: one that requires optimizing fast (≈ millisecond) spike-based interactions for efﬁcient encoding of information, and a delayed-memory {XOR} task over extended duration (≈ second). The results show that our method indeed optimizes the spiking network dynamics on the time scale of individual spikes as well as the behavioral time scales. In conclusion, our result offers a general purpose supervised learning algorithm for spiking neural networks, thus advancing further investigations on spike-based computation.},
	journaltitle = {{arXiv}:1706.04698 [cs, q-bio, stat]},
	author = {Huh, Dongsung and Sejnowski, Terrence J.},
	urldate = {2021-10-01},
	date = {2017-06-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1706.04698},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
	file = {Huh e Sejnowski - 2017 - Gradient Descent for Spiking Neural Networks.pdf:/home/weverson/Zotero/storage/WTHYTYSC/Huh e Sejnowski - 2017 - Gradient Descent for Spiking Neural Networks.pdf:application/pdf},
}

@inproceedings{indiveri_spike-based_2007,
	title = {Spike-based learning in {VLSI} networks of integrate-and-fire neurons},
	doi = {10.1109/ISCAS.2007.378290},
	abstract = {As the number of {VLSI} implementations of spike-based neural networks is steadily increasing, and the development of spike-based multi-chip systems is becoming more popular it is important to design spike-based learning algorithms and circuits, compatible with existing solutions, that endow these systems with adaptation and classification capabilities. We propose a spike-based learning algorithm that is highly effective in classifying complex patterns in semi-supervised fashion, and present neuromorphic circuits that support its {VLSI} implementation. We describe the architecture of a spike-based learning neural network, the analog circuits that implement the synaptic learning mechanism, and present results from a prototype {VLSI} chip comprising a full network of integrate-and-fire neurons and plastic synapses. We demonstrate how the {VLSI} circuits proposed reproduce the learning model's properties and fulfil its basic requirements for classifying complex patterns of mean firing rates.},
	eventtitle = {2007 {IEEE} International Symposium on Circuits and Systems},
	pages = {3371--3374},
	booktitle = {2007 {IEEE} International Symposium on Circuits and Systems},
	author = {Indiveri, Giacomo and Fusi, Stefano},
	date = {2007-05},
	note = {{ISSN}: 2158-1525},
	keywords = {Large-scale systems, Neurons, Neural networks, Circuits, Learning systems, Protection, Sensor arrays, Silicon, Timing, Very large scale integration},
	file = {IEEE Xplore Full Text PDF:/home/weverson/Zotero/storage/DDMHHQVH/Indiveri e Fusi - 2007 - Spike-based learning in VLSI networks of integrate.pdf:application/pdf},
}

@article{galanis_efficient_2021,
	title = {Efficient Deployment of Spiking Neural Networks on {SpiNNaker} Neuromorphic Platform},
	volume = {68},
	issn = {1558-3791},
	doi = {10.1109/TCSII.2020.3047425},
	abstract = {Spiking Neural Networks ({SNNs}) have emerged as serious competitors of the traditional Convolutional Neural Networks ({CNNs}), as they unlock new potential of implementing less complex and more energy efficient neural networks. Current deep {CNNs} can be converted to {SNNs} for fast deployment on neuromorphic devices, however existing methods do not investigate the impact of hardware-related parameters that directly affect the accuracy of an {SNN}. In this brief, we target the {SpiNNaker} neuromorphic platform and we demonstrate a fast exploration framework that effectively decides the configuration of the target board, in order to achieve the highest possible accuracy. Experimental results show that our method reaches 98.85\% {SNN} accuracy on {MNIST} dataset, while reducing the exploration time by a factor of \$3{\textbackslash}times \$ compared to exhaustive search.},
	pages = {1937--1941},
	number = {6},
	journaltitle = {{IEEE} Transactions on Circuits and Systems {II}: Express Briefs},
	author = {Galanis, Ioannis and Anagnostopoulos, Iraklis and Nguyen, Chinh and Bares, Guillermo},
	date = {2021-06},
	note = {Conference Name: {IEEE} Transactions on Circuits and Systems {II}: Express Briefs},
	keywords = {Neurons, Synapses, Computer architecture, neural network hardware, Artificial neural networks, Biological neural networks, neuromorphics, Neuromorphics, Routing, Time factors},
	file = {IEEE Xplore Full Text PDF:/home/weverson/Zotero/storage/B7HQF8YI/Galanis et al. - 2021 - Efficient Deployment of Spiking Neural Networks on.pdf:application/pdf},
}

@article{lu_autonomous_2021,
	title = {An autonomous learning mobile robot using biological reward modulate {STDP}},
	volume = {458},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231221009310},
	doi = {10.1016/j.neucom.2021.06.027},
	abstract = {Recent studies have shown that biologically inspired Spiking Neural Networks ({SNNs}) has potentials for the mobile robot controls. Based on {SNNs}, an autonomous learning paradigm for controlling mobile robots is proposed in this work, which can learn specific tasks autonomously. A reward modulated spike-timing-dependent plasticity (R-{STDP}) learning algorithm is designed to aid implementing the autonomous learning paradigm. It can train the {SNN} under different environmental states and conditions. The obstacle avoidance in the synthetic and real environments is used as a robotic task example to verify the effectiveness of the proposed paradigm. Results show that the mobile robot can learn autonomously under different environmental conditions and is able to avoid obstacles after learning processes complete.},
	pages = {308--318},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Lu, Hao and Liu, Junxiu and Luo, Yuling and Hua, Yifan and Qiu, Senhui and Huang, Yongchuang},
	urldate = {2021-10-01},
	date = {2021-10-11},
	langid = {english},
	keywords = {Autonomous learning, Reinforcement learning, Reward, Spike-timing-dependent plasticity, Spiking neural networks},
	file = {Lu et al. - 2021 - An autonomous learning mobile robot using biologic.pdf:/home/weverson/Zotero/storage/PN277HHQ/Lu et al. - 2021 - An autonomous learning mobile robot using biologic.pdf:application/pdf},
}

@article{kumarasinghe_brain-inspired_2021,
	title = {Brain-inspired spiking neural networks for decoding and understanding muscle activity and kinematics from electroencephalography signals during hand movements},
	volume = {11},
	rights = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-81805-4},
	doi = {10.1038/s41598-021-81805-4},
	abstract = {Compared to the abilities of the animal brain, many Artificial Intelligence systems have limitations which emphasise the need for a Brain-Inspired Artificial Intelligence paradigm. This paper proposes a novel Brain-Inspired Spiking Neural Network ({BI}-{SNN}) model for incremental learning of spike sequences. {BI}-{SNN} maps spiking activity from input channels into a high dimensional source-space which enhances the evolution of polychronising spiking neural populations. We applied the {BI}-{SNN} to predict muscle activity and kinematics from electroencephalography signals during upper limb functional movements. The {BI}-{SNN} extends our previously proposed {eSPANNet} computational model by integrating it with the ‘{NeuCube}’ brain-inspired {SNN} architecture. We show that {BI}-{SNN} can successfully predict continuous muscle activity and kinematics of upper-limb. The experimental results confirmed that the {BI}-{SNN} resulted in strongly correlated population activity and demonstrated the feasibility for real-time prediction. In contrast to the majority of Brain–Computer Interfaces ({BCIs}) that constitute a ‘black box’, {BI}-{SNN} provide quantitative and visual feedback about the related brain activity. This study is one of the first attempts to examine the feasibility of finding neural correlates of muscle activity and kinematics from electroencephalography using a brain-inspired computational paradigm. The findings suggest that {BI}-{SNN} is a better neural decoder for non-invasive {BCI}.},
	pages = {2486},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Kumarasinghe, Kaushalya and Kasabov, Nikola and Taylor, Denise},
	urldate = {2021-10-01},
	date = {2021-01-28},
	langid = {english},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Brain–machine interface;Computer science;Electroencephalography – {EEG};Learning algorithms;Network models;Neural decoding;Sensorimotor processing
Subject\_term\_id: brain-machine-interface;computer-science;electroencephalography-eeg;learning-algorithms;network-models;neural-decoding;sensorimotor-processing},
	file = {Full Text PDF:/home/weverson/Zotero/storage/3327MBD9/Kumarasinghe et al. - 2021 - Brain-inspired spiking neural networks for decodin.pdf:application/pdf},
}

@inproceedings{kumarasinghe_espannet_2019,
	title = {{eSPANNet}: Evolving Spike Pattern Association Neural Network for Spike-based Supervised Incremental Learning and Its Application for Single-trial Brain Computer Interfaces},
	doi = {10.1109/IJCNN.2019.8852213},
	shorttitle = {{eSPANNet}},
	abstract = {Objective:Due to the non-stationarity and high trialto-trial variability, online event prediction from biomedical signals is challenging. This is more significant when it is applied to neurological rehabilitation where the person incrementally learns to regain the control of movement. {eSPANNet} is a computational model inspired by the incremental learning for motor control in living nervous systems. It is inspired by the concept of 'population vectors' which have been experimentally proven by several computational neuroscience studies. In this paper, we present a proof-of-concept study on the proposed computational model. Our goal is to utilize the polychronization effect of Spiking Neural Networks to develop a better neural decoder for Brain-Computer Interfaces. Methods: The {eSPANNet} model contains a network of Spike Pattern Association Neurons, a spiking neuron model which is able to emit spikes at the desired time-point. Results: The proposed approach was experimentally validated using the finger flexion prediction dataset from the fourth {BCI} competition. The results show that {eSPANNet} results in 1) a higher classification accuracy, sensitivity and F1 score compared to several other multi-class classifiers and, 2) a better approximation of the actual movement compared to several regression analysis based approaches. Conclusion and Significance: The novelty of our algorithm is the ability to learn which inputs to focus on in an online manner. We suggest that the {eSPANNet} is a better {BCI} decoder due to its i) incremental and life-long learning, ii) compatibility with the neuromorphic platforms and, iii) ability to address the non-stationarity of brain data.},
	eventtitle = {2019 International Joint Conference on Neural Networks ({IJCNN})},
	pages = {1--8},
	booktitle = {2019 International Joint Conference on Neural Networks ({IJCNN})},
	author = {Kumarasinghe, Kaushalya and Taylor, Denise and Kasabov, Nikola},
	date = {2019-07},
	note = {{ISSN}: 2161-4407},
	keywords = {Statistics, Brain modeling, Computational modeling, Neurons, Biological neural networks, Sociology, Training},
	file = {IEEE Xplore Full Text PDF:/home/weverson/Zotero/storage/4I4DPVG4/Kumarasinghe et al. - 2019 - eSPANNet Evolving Spike Pattern Association Neura.pdf:application/pdf},
}

@article{tan_spiking_2020,
	title = {Spiking Neural Networks: Background, Recent Development and the {NeuCube} Architecture},
	volume = {52},
	issn = {1573-773X},
	url = {https://doi.org/10.1007/s11063-020-10322-8},
	doi = {10.1007/s11063-020-10322-8},
	shorttitle = {Spiking Neural Networks},
	abstract = {This paper reviews recent developments in the still-off-the-mainstream information and data processing area of spiking neural networks ({SNN})—the third generation of artificial neural networks. We provide background information about the functioning of biological neurons, discussing the most important and commonly used mathematical neural models. Most relevant information processing techniques, learning algorithms, and applications of spiking neurons are described and discussed, focusing on feasibility and biological plausibility of the methods. Specifically, we describe in detail the functioning and organization of the latest version of a 3D spatio-temporal {SNN}-based data machine framework called {NeuCube}, as well as it’s {SNN}-related submodules. All described submodules are accompanied with formal algorithmic formulations. The architecture is highly relevant for the analysis and interpretation of various types of spatio-temporal brain data ({STBD}), like {EEG}, {NIRS}, {fMRI}, but we highlight some of the recent both {STBD}- and non-{STBD}-based applications. Finally, we summarise and discuss some open research problems that can be addressed in the future. These include, but are not limited to: application in the area of {EEG}-based {BCI} through transfer learning; application in the area of affective computing through the extension of the {NeuCube} framework which would allow for a biologically plausible {SNN}-based integration of central and peripheral nervous system measures. Matlab implementation of the {NeuCube}’s {SNN}-related module is available for research and teaching purposes.},
	pages = {1675--1701},
	number = {2},
	journaltitle = {Neural Processing Letters},
	shortjournal = {Neural Process Lett},
	author = {Tan, Clarence and Šarlija, Marko and Kasabov, Nikola},
	urldate = {2021-10-01},
	date = {2020-10-01},
	langid = {english},
	file = {Springer Full Text PDF:/home/weverson/Zotero/storage/WF3XYKVN/Tan et al. - 2020 - Spiking Neural Networks Background, Recent Develo.pdf:application/pdf},
}

@incollection{hosseini_introductory_2018,
	title = {Introductory Chapter: Cognitive and Computational Neuroscience - Principles, Algorithms, and Applications},
	isbn = {978-1-78923-188-5 978-1-78923-189-2},
	url = {http://www.intechopen.com/books/cognitive-and-computational-neuroscience-principles-algorithms-and-applications/introductory-chapter-cognitive-and-computational-neuroscience-principles-algorithms-and-applications},
	shorttitle = {Introductory Chapter},
	booktitle = {Cognitive and Computational Neuroscience - Principles, Algorithms and Applications},
	publisher = {{InTech}},
	author = {Hosseini, Seyyed Abed},
	editor = {Hosseini, Seyyed Abed},
	urldate = {2023-09-23},
	date = {2018-05-30},
	langid = {english},
	doi = {10.5772/intechopen.72824},
	file = {Texto completo:/home/weverson/Zotero/storage/QYY5M23A/Hosseini - 2018 - Introductory Chapter Cognitive and Computational .pdf:application/pdf},
}

@incollection{abdurakhmonov_data_2018,
	title = {Data Models in Neuroinformatics},
	isbn = {978-1-78923-268-4 978-1-78923-269-1},
	url = {http://www.intechopen.com/books/bioinformatics-in-the-era-of-post-genomics-and-big-data/data-models-in-neuroinformatics},
	booktitle = {Bioinformatics in the Era of Post Genomics and Big Data},
	publisher = {{InTech}},
	author = {Tsur, Elishai Ezra},
	editor = {Abdurakhmonov, Ibrokhim Y.},
	urldate = {2023-09-23},
	date = {2018-06-20},
	langid = {english},
	doi = {10.5772/intechopen.73516},
	file = {Texto completo:/home/weverson/Zotero/storage/FPDKRGHY/Tsur - 2018 - Data Models in Neuroinformatics.pdf:application/pdf},
}

@incollection{lopez-ruiz_simulation_2016,
	title = {Simulation of Neural Behavior},
	isbn = {978-953-51-2564-8 978-953-51-2565-5},
	url = {http://www.intechopen.com/books/numerical-simulation-from-brain-imaging-to-turbulent-flows/simulation-of-neural-behavior},
	booktitle = {Numerical Simulation - From Brain Imaging to Turbulent Flows},
	publisher = {{InTech}},
	author = {Kitajima, Tatsuo and Feng, Zonggang and Azhim, Azran},
	editor = {Lopez-Ruiz, Ricardo},
	urldate = {2023-09-23},
	date = {2016-08-24},
	langid = {english},
	doi = {10.5772/64028},
	file = {Texto completo:/home/weverson/Zotero/storage/ZB8QEMZH/Kitajima et al. - 2016 - Simulation of Neural Behavior.pdf:application/pdf},
}

@incollection{hutchison_supervised_2010,
	location = {Berlin, Heidelberg},
	title = {Supervised Associative Learning in Spiking Neural Network},
	volume = {6352},
	isbn = {978-3-642-15818-6 978-3-642-15819-3},
	url = {http://link.springer.com/10.1007/978-3-642-15819-3_30},
	pages = {224--229},
	booktitle = {Artificial Neural Networks – {ICANN} 2010},
	publisher = {Springer Berlin Heidelberg},
	author = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Yusoff, Nooraini and Grüning, André},
	editor = {Diamantaras, Konstantinos and Duch, Wlodek and Iliadis, Lazaros S.},
	urldate = {2023-09-23},
	date = {2010},
	doi = {10.1007/978-3-642-15819-3_30},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Texto completo:/home/weverson/Zotero/storage/Z8Y3NSUK/Hutchison et al. - 2010 - Supervised Associative Learning in Spiking Neural .pdf:application/pdf},
}

@article{hille_ionic_1992,
	title = {Ionic channels of excitable membranes},
	volume = {306},
	rights = {{FEBS} Letters 306 (1992) 1873-3468 © 2015 Federation of European Biochemical Societies},
	issn = {1873-3468},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1016/0014-5793%2892%2981020-M},
	doi = {10.1016/0014-5793(92)81020-M},
	pages = {277--278},
	number = {2},
	journaltitle = {{FEBS} Letters},
	author = {Hille, Bertil},
	urldate = {2023-11-22},
	date = {1992},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1016/0014-5793\%2892\%2981020-M},
	file = {Full Text PDF:/home/weverson/Zotero/storage/JIAJFFEP/Apps - 1992 - Ionic channels of excitable membranes (second edit.pdf:application/pdf},
}

@article{hodgkin_measurement_1952,
	title = {Measurement of current-voltage relations in the membrane of the giant axon of Loligo},
	volume = {116},
	issn = {0022-3751},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1392219/},
	abstract = {Images
null},
	pages = {424--448},
	number = {4},
	journaltitle = {The Journal of Physiology},
	shortjournal = {J Physiol},
	author = {Hodgkin, A. L. and Huxley, A. F. and Katz, B.},
	urldate = {2023-11-27},
	date = {1952-04-28},
	pmid = {14946712},
	pmcid = {PMC1392219},
	file = {PubMed Central Full Text PDF:/home/weverson/Zotero/storage/8JGQV564/Hodgkin et al. - 1952 - Measurement of current-voltage relations in the me.pdf:application/pdf},
}

@article{ijspeert_central_2008,
	title = {Central pattern generators for locomotion control in animals and robots: A review},
	volume = {21},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608008000804},
	doi = {10.1016/j.neunet.2008.03.014},
	series = {Robotics and Neuroscience},
	shorttitle = {Central pattern generators for locomotion control in animals and robots},
	abstract = {The problem of controlling locomotion is an area in which neuroscience and robotics can fruitfully interact. In this article, I will review research carried out on locomotor central pattern generators ({CPGs}), i.e. neural circuits capable of producing coordinated patterns of high-dimensional rhythmic output signals while receiving only simple, low-dimensional, input signals. The review will first cover neurobiological observations concerning locomotor {CPGs} and their numerical modelling, with a special focus on vertebrates. It will then cover how {CPG} models implemented as neural networks or systems of coupled oscillators can be used in robotics for controlling the locomotion of articulated robots. The review also presents how robots can be used as scientific tools to obtain a better understanding of the functioning of biological {CPGs}. Finally, various methods for designing {CPGs} to control specific modes of locomotion will be briefly reviewed. In this process, I will discuss different types of {CPG} models, the pros and cons of using {CPGs} with robots, and the pros and cons of using robots as scientific tools. Open research topics both in biology and in robotics will also be discussed.},
	pages = {642--653},
	number = {4},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Ijspeert, Auke Jan},
	urldate = {2023-11-28},
	date = {2008-05-01},
	keywords = {Robots, Dynamical systems, Neural networks, Locomotion, Central pattern generators, Computational models, Systems of coupled oscillators},
}

@article{cain_computational_2012,
	title = {Computational models of decision making: integration, stability, and noise},
	volume = {22},
	issn = {0959-4388},
	url = {https://www.sciencedirect.com/science/article/pii/S0959438812000785},
	doi = {10.1016/j.conb.2012.04.013},
	series = {Decision making},
	shorttitle = {Computational models of decision making},
	abstract = {Decision making demands the accumulation of sensory evidence over time. Questions remain about how this occurs, but recent years have seen progress on several fronts. The first concerns when optimal accumulation of evidence coincides with the simplest method of accumulating neural activity: summation over time. The second involves what computations the brain might perform when summation is difficult due to imprecision in neural circuits or is suboptimal due to uncertainty or variability in how evidence arrives. Finally, the third concerns sources of noise in decision circuits. Empirical studies have better constrained the extent of this noise, and modeling work is helping to clarify its possible origins.},
	pages = {1047--1053},
	number = {6},
	journaltitle = {Current Opinion in Neurobiology},
	shortjournal = {Current Opinion in Neurobiology},
	author = {Cain, Nicholas and Shea-Brown, Eric},
	urldate = {2023-11-28},
	date = {2012-12-01},
}

@article{morris_do_1999,
	title = {D.O. Hebb: The Organization of Behavior, Wiley: New York; 1949},
	volume = {50},
	issn = {0361-9230},
	doi = {10.1016/s0361-9230(99)00182-3},
	shorttitle = {D.O. Hebb},
	pages = {437},
	number = {5},
	journaltitle = {Brain Research Bulletin},
	shortjournal = {Brain Res Bull},
	author = {Morris, R. G.},
	date = {1999},
	pmid = {10643472},
	keywords = {Neurosciences, Animals, Behavior, Animal, Cognitive Science, History, 20th Century, Publishing},
}

@article{ben-ari_excitatory_2002,
	title = {Excitatory actions of gaba during development: the nature of the nurture},
	volume = {3},
	rights = {2002 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/nrn920},
	doi = {10.1038/nrn920},
	shorttitle = {Excitatory actions of gaba during development},
	abstract = {In the immature brain, {GABA} (γ-aminobutyric acid) is excitatory, and {GABA}-releasing synapses are formed before glutamatergic contacts in a wide range of species and structures. {GABA} becomes inhibitory by the delayed expression of a chloride exporter, leading to a negative shift in the reversal potential for choride ions. I propose that this mechanism provides a solution to the problem of how to excite developing neurons to promote growth and synapse formation while avoiding the potentially toxic effects of a mismatch between {GABA}-mediated inhibition and glutamatergic excitation. As key elements of this cascade are activity dependent, the formation of inhibition adds an element of nurture to the construction of cortical networks.},
	pages = {728--739},
	number = {9},
	journaltitle = {Nature Reviews Neuroscience},
	shortjournal = {Nat Rev Neurosci},
	author = {Ben-Ari, Yehezkel},
	urldate = {2023-11-28},
	date = {2002-09},
	langid = {english},
	note = {Number: 9
Publisher: Nature Publishing Group},
	file = {Full Text PDF:/home/weverson/Zotero/storage/QF8YXT6X/Ben-Ari - 2002 - Excitatory actions of gaba during development the.pdf:application/pdf},
}

@article{arena_central_2000,
	title = {The Central Pattern Generator: a paradigm for artificial locomotion},
	volume = {4},
	issn = {1432-7643},
	url = {https://doi.org/10.1007/s005000000051},
	doi = {10.1007/s005000000051},
	shorttitle = {The Central Pattern Generator},
	abstract = {In this paper the fundamental aspects that unify the basic programs for locomotion generation in both vertebrates and invertebrates are examined. Subsequently it is shown that these laws that are commonly included under the biological stereotype of the so-called Central Pattern Generator ({CPG}), can be efficiently represented and reproduced by means of low-cost, locally interacting nonlinear systems and programmed by soft-computing techniques. To demonstrate the suitability of the approach some simple Central Pattern Generators are implemented on biologically inspired robotic prototypes.},
	pages = {251--266},
	number = {4},
	journaltitle = {Soft Computing},
	shortjournal = {Soft Computing},
	author = {Arena, P.},
	urldate = {2023-11-28},
	date = {2000-12-01},
	langid = {english},
	keywords = {Keywords Central Pattern Generator, Cellular neural networks, Reaction diffusion equations, Autowaves},
	file = {Full Text PDF:/home/weverson/Zotero/storage/9AVHCIKR/Arena - 2000 - The Central Pattern Generator a paradigm for arti.pdf:application/pdf},
}

@book{hebb_organization_2005,
	edition = {0},
	title = {The Organization of Behavior},
	isbn = {978-1-4106-1240-3},
	url = {https://www.taylorfrancis.com/books/9781135631918},
	publisher = {Psychology Press},
	author = {Hebb, D.O.},
	urldate = {2023-11-28},
	date = {2005-04-11},
	langid = {english},
	doi = {10.4324/9781410612403},
}

@article{parhi_brain-inspired_2020,
	title = {Brain-Inspired Computing: Models and Architectures},
	volume = {1},
	issn = {2644-1225},
	url = {https://ieeexplore.ieee.org/document/9238476},
	doi = {10.1109/OJCAS.2020.3032092},
	shorttitle = {Brain-Inspired Computing},
	abstract = {With an exponential increase in the amount of data collected per day, the fields of artificial intelligence and machine learning continue to progress at a rapid pace with respect to algorithms, models, applications, and hardware. In particular, deep neural networks have revolutionized these fields by providing unprecedented human-like performance in solving many real-world problems such as image or speech recognition. There is also significant research aimed at unraveling the principles of computation in large biological neural networks and, in particular, biologically plausible spiking neural networks. This article presents an overview of the brain-inspired computing models starting with the development of the perceptron and multi-layer perceptron followed by convolutional neural networks ({CNNs}) and recurrent neural networks ({RNNs}). This article also briefly reviews other neural network models such as Hopfield neural networks and Boltzmann machines. Other models such as spiking neural networks ({SNNs}) and hyperdimensional computing are then briefly reviewed. Recent advances in these neural networks and graph related neural networks are then described.},
	pages = {185--204},
	journaltitle = {{IEEE} Open Journal of Circuits and Systems},
	author = {Parhi, Keshab K. and Unnikrishnan, Nanda K.},
	urldate = {2023-11-30},
	date = {2020},
	note = {Conference Name: {IEEE} Open Journal of Circuits and Systems},
	file = {IEEE Xplore Full Text PDF:/home/weverson/Zotero/storage/9HBJJ9Z5/Parhi e Unnikrishnan - 2020 - Brain-Inspired Computing Models and Architectures.pdf:application/pdf},
}

@article{shrestha_review_2019,
	title = {Review of Deep Learning Algorithms and Architectures},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8694781},
	doi = {10.1109/ACCESS.2019.2912200},
	abstract = {Deep learning ({DL}) is playing an increasingly important role in our lives. It has already made a huge impact in areas, such as cancer diagnosis, precision medicine, self-driving cars, predictive forecasting, and speech recognition. The painstakingly handcrafted feature extractors used in traditional learning, classification, and pattern recognition systems are not scalable for large-sized data sets. In many cases, depending on the problem complexity, {DL} can also overcome the limitations of earlier shallow networks that prevented efficient training and abstractions of hierarchical representations of multi-dimensional training data. Deep neural network ({DNN}) uses multiple (deep) layers of units with highly optimized algorithms and architectures. This paper reviews several optimization methods to improve the accuracy of the training and to reduce training time. We delve into the math behind training algorithms used in recent deep networks. We describe current shortcomings, enhancements, and implementations. The review also covers different types of deep architectures, such as deep convolution networks, deep residual networks, recurrent neural networks, reinforcement learning, variational autoencoders, and others.},
	pages = {53040--53065},
	journaltitle = {{IEEE} Access},
	author = {Shrestha, Ajay and Mahmood, Ausif},
	urldate = {2023-11-30},
	date = {2019},
	note = {Conference Name: {IEEE} Access},
	file = {IEEE Xplore Full Text PDF:/home/weverson/Zotero/storage/NYEVGJUY/Shrestha e Mahmood - 2019 - Review of Deep Learning Algorithms and Architectur.pdf:application/pdf},
}

@article{van_gerven_computational_2017,
	title = {Computational Foundations of Natural Intelligence},
	volume = {11},
	issn = {1662-5188},
	url = {https://www.frontiersin.org/articles/10.3389/fncom.2017.00112},
	abstract = {New developments in {AI} and neuroscience are revitalizing the quest to understanding natural intelligence, offering insight about how to equip machines with human-like capabilities. This paper reviews some of the computational principles relevant for understanding natural intelligence and, ultimately, achieving strong {AI}. After reviewing basic principles, a variety of computational modeling approaches is discussed. Subsequently, I concentrate on the use of artificial neural networks as a framework for modeling cognitive processes. This paper ends by outlining some of the challenges that remain to fulfill the promise of machines that show human-like intelligence.},
	journaltitle = {Frontiers in Computational Neuroscience},
	author = {van Gerven, Marcel},
	urldate = {2023-11-30},
	date = {2017},
	file = {Full Text PDF:/home/weverson/Zotero/storage/ZQEAJGSY/van Gerven - 2017 - Computational Foundations of Natural Intelligence.pdf:application/pdf},
}

@article{ivanov_neuromorphic_2022,
	title = {Neuromorphic artificial intelligence systems},
	volume = {16},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2022.959626},
	abstract = {Modern artificial intelligence ({AI}) systems, based on von Neumann architecture and classical neural networks, have a number of fundamental limitations in comparison with the mammalian brain. In this article we discuss these limitations and ways to mitigate them. Next, we present an overview of currently available neuromorphic {AI} projects in which these limitations are overcome by bringing some brain features into the functioning and organization of computing systems ({TrueNorth}, Loihi, Tianjic, {SpiNNaker}, {BrainScaleS}, {NeuronFlow}, {DYNAP}, Akida, Mythic). Also, we present the principle of classifying neuromorphic {AI} systems by the brain features they use: connectionism, parallelism, asynchrony, impulse nature of information transfer, on-device-learning, local learning, sparsity, analog, and in-memory computing. In addition to reviewing new architectural approaches used by neuromorphic devices based on existing silicon microelectronics technologies, we also discuss the prospects for using a new memristor element base. Examples of recent advances in the use of memristors in neuromorphic applications are also given.},
	journaltitle = {Frontiers in Neuroscience},
	author = {Ivanov, Dmitry and Chezhegov, Aleksandr and Kiselev, Mikhail and Grunin, Andrey and Larionov, Denis},
	urldate = {2023-11-30},
	date = {2022},
	file = {Full Text PDF:/home/weverson/Zotero/storage/GAXXQWX2/Ivanov et al. - 2022 - Neuromorphic artificial intelligence systems.pdf:application/pdf},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	rights = {2015 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	pages = {436--444},
	number = {7553},
	journaltitle = {Nature},
	author = {{LeCun}, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	urldate = {2023-11-30},
	date = {2015-05},
	langid = {english},
	note = {Number: 7553
Publisher: Nature Publishing Group},
	keywords = {Computer science, Mathematics and computing},
	file = {Full Text PDF:/home/weverson/Zotero/storage/MPJBISNX/LeCun et al. - 2015 - Deep learning.pdf:application/pdf},
}

@article{asnicar_machine_2023,
	title = {Machine learning for microbiologists},
	rights = {2023 Springer Nature Limited},
	issn = {1740-1534},
	url = {https://www.nature.com/articles/s41579-023-00984-1},
	doi = {10.1038/s41579-023-00984-1},
	abstract = {Machine learning is increasingly important in microbiology where it is used for tasks such as predicting antibiotic resistance and associating human microbiome features with complex host diseases. The applications in microbiology are quickly expanding and the machine learning tools frequently used in basic and clinical research range from classification and regression to clustering and dimensionality reduction. In this Review, we examine the main machine learning concepts, tasks and applications that are relevant for experimental and clinical microbiologists. We provide the minimal toolbox for a microbiologist to be able to understand, interpret and use machine learning in their experimental and translational activities.},
	pages = {1--15},
	journaltitle = {Nature Reviews Microbiology},
	shortjournal = {Nat Rev Microbiol},
	author = {Asnicar, Francesco and Thomas, Andrew Maltez and Passerini, Andrea and Waldron, Levi and Segata, Nicola},
	urldate = {2023-11-30},
	date = {2023-11-15},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Machine learning, Computational models, Microbiology},
	file = {Full Text PDF:/home/weverson/Zotero/storage/Q3BPZBSF/Asnicar et al. - 2023 - Machine learning for microbiologists.pdf:application/pdf},
}

@article{rosenblatt_perceptron_1958,
	title = {The perceptron: A probabilistic model for information storage and organization in the brain},
	volume = {65},
	issn = {1939-1471},
	doi = {10.1037/h0042519},
	shorttitle = {The perceptron},
	abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {386--408},
	number = {6},
	journaltitle = {Psychological Review},
	author = {Rosenblatt, F.},
	date = {1958},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Memory, Cognition, Brain, Nervous System},
	file = {Full Text PDF:/home/weverson/Zotero/storage/FG689CKB/Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf:application/pdf},
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	pages = {115--133},
	number = {4},
	journaltitle = {The bulletin of mathematical biophysics},
	shortjournal = {Bulletin of Mathematical Biophysics},
	author = {{McCulloch}, Warren S. and Pitts, Walter},
	urldate = {2023-12-01},
	date = {1943-12-01},
	langid = {english},
	keywords = {Excitatory Synapse, Inhibitory Synapse, Nervous Activity, Spatial Summation, Temporal Summation},
	file = {Full Text PDF:/home/weverson/Zotero/storage/MB47TB7G/McCulloch e Pitts - 1943 - A logical calculus of the ideas immanent in nervou.pdf:application/pdf},
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the {ImageNet} {LSVRC}-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient {GPU} implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the {ILSVRC}-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	pages = {84--90},
	number = {6},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	urldate = {2023-12-01},
	date = {2017-05-24},
	file = {Full Text PDF:/home/weverson/Zotero/storage/Z2HK4IG4/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf},
}

@article{haenlein_brief_2019,
	title = {A Brief History of Artificial Intelligence: On the Past, Present, and Future of Artificial Intelligence},
	volume = {61},
	issn = {0008-1256},
	url = {https://doi.org/10.1177/0008125619864925},
	doi = {10.1177/0008125619864925},
	shorttitle = {A Brief History of Artificial Intelligence},
	abstract = {This introduction to this special issue discusses artificial intelligence ({AI}), commonly defined as “a system’s ability to interpret external data correctly, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.” It summarizes seven articles published in this special issue that present a wide variety of perspectives on {AI}, authored by several of the world’s leading experts and specialists in {AI}. It concludes by offering a comprehensive outlook on the future of {AI}, drawing on micro-, meso-, and macro-perspectives.},
	pages = {5--14},
	number = {4},
	journaltitle = {California Management Review},
	author = {Haenlein, Michael and Kaplan, Andreas},
	urldate = {2023-12-01},
	date = {2019-08-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:/home/weverson/Zotero/storage/D4NLEUEB/Haenlein e Kaplan - 2019 - A Brief History of Artificial Intelligence On the.pdf:application/pdf},
}

@article{louridas_machine_2016,
	title = {Machine Learning},
	volume = {33},
	issn = {1937-4194},
	url = {https://ieeexplore.ieee.org/document/7548905},
	doi = {10.1109/MS.2016.114},
	abstract = {In machine learning, a computer first learns to perform a task by studying a training set of examples. The computer then performs the same task with data it hasn't encountered before. This article presents a brief overview of machine-learning technologies, with a concrete case study from code analysis.},
	pages = {110--115},
	number = {5},
	journaltitle = {{IEEE} Software},
	author = {Louridas, Panos and Ebert, Christof},
	urldate = {2023-12-01},
	date = {2016-09},
	note = {Conference Name: {IEEE} Software},
	file = {Texto completo:/home/weverson/Zotero/storage/IKG2LK2Q/Louridas e Ebert - 2016 - Machine Learning.pdf:application/pdf},
}

@incollection{drenick_applications_1982,
	location = {Berlin/Heidelberg},
	title = {Applications of advances in nonlinear sensitivity analysis},
	volume = {38},
	isbn = {978-3-540-11691-2},
	url = {http://link.springer.com/10.1007/BFb0006203},
	pages = {762--770},
	booktitle = {System Modeling and Optimization},
	publisher = {Springer-Verlag},
	author = {Werbos, Paul J.},
	editor = {Drenick, R. F. and Kozin, F.},
	urldate = {2023-12-02},
	date = {1982},
	langid = {english},
	doi = {10.1007/BFb0006203},
	note = {Series Title: Lecture Notes in Control and Information Sciences},
	file = {Texto completo:/home/weverson/Zotero/storage/AH7RJMCY/Werbos - 1982 - Applications of advances in nonlinear sensitivity .pdf:application/pdf},
}

@book{minsky_perceptrons_2017,
	title = {Perceptrons: An Introduction to Computational Geometry},
	isbn = {978-0-262-34393-0},
	url = {https://direct.mit.edu/books/book/3132/perceptronsan-introduction-to-computational},
	shorttitle = {Perceptrons},
	publisher = {The {MIT} Press},
	author = {Minsky, Marvin and Papert, Seymour A.},
	urldate = {2023-12-05},
	date = {2017},
	langid = {english},
	doi = {10.7551/mitpress/11301.001.0001},
}

@thesis{werbos_beyond_1974,
	location = {Cambridge, {MA}},
	title = {Beyond regression: new tools for prediction and analysis in the behavioral sciences},
	pagetotal = {453},
	institution = {Harvard University},
	type = {phdthesis},
	author = {Werbos, Paul John},
	date = {1974-08},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {00189219},
	url = {http://ieeexplore.ieee.org/document/726791/},
	doi = {10.1109/5.726791},
	pages = {2278--2324},
	number = {11},
	journaltitle = {Proceedings of the {IEEE}},
	shortjournal = {Proc. {IEEE}},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	urldate = {2023-12-08},
	date = {1998-11},
	file = {Texto completo:/home/weverson/Zotero/storage/KL4B2PVK/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf:application/pdf},
}

@article{elman_finding_1990,
	title = {Finding Structure in Time},
	volume = {14},
	issn = {0364-0213, 1551-6709},
	url = {https://onlinelibrary.wiley.com/doi/10.1207/s15516709cog1402_1},
	doi = {10.1207/s15516709cog1402_1},
	abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves: the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of {XOR}) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context‐dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
	pages = {179--211},
	number = {2},
	journaltitle = {Cognitive Science},
	shortjournal = {Cognitive Science},
	author = {Elman, Jeffrey L.},
	urldate = {2023-12-08},
	date = {1990-03},
	langid = {english},
	file = {Texto completo:/home/weverson/Zotero/storage/YB3JNEQR/Elman - 1990 - Finding Structure in Time.pdf:application/pdf},
}

@article{hinton_reducing_2006,
	title = {Reducing the Dimensionality of Data with Neural Networks},
	volume = {313},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.1127647},
	doi = {10.1126/science.1127647},
	abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
	pages = {504--507},
	number = {5786},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Hinton, G. E. and Salakhutdinov, R. R.},
	urldate = {2023-12-08},
	date = {2006-07-28},
	langid = {english},
}

@article{hochreiter_long_1997,
	title = {Long Short-Term Memory},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory ({LSTM}). Truncating the gradient where this does not do harm, {LSTM} can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. {LSTM} is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, {LSTM} leads to many more successful runs, and learns much faster. {LSTM} also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	pages = {1735--1780},
	number = {8},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	urldate = {2023-12-08},
	date = {1997-11-01},
	langid = {english},
}

@article{von_neumann_first_1993,
	title = {First draft of a report on the {EDVAC}},
	volume = {15},
	issn = {1058-6180},
	url = {http://ieeexplore.ieee.org/document/238389/},
	doi = {10.1109/85.238389},
	pages = {27--75},
	number = {4},
	journaltitle = {{IEEE} Annals of the History of Computing},
	shortjournal = {{IEEE} Annals Hist. Comput.},
	author = {Von Neumann, J.},
	urldate = {2023-12-08},
	date = {1993},
	file = {Texto completo:/home/weverson/Zotero/storage/XJM4CE5K/Von Neumann - 1993 - First draft of a report on the EDVAC.pdf:application/pdf},
}

@article{schuman_opportunities_2022,
	title = {Opportunities for neuromorphic computing algorithms and applications},
	volume = {2},
	issn = {2662-8457},
	url = {https://www.nature.com/articles/s43588-021-00184-y},
	doi = {10.1038/s43588-021-00184-y},
	pages = {10--19},
	number = {1},
	journaltitle = {Nature Computational Science},
	shortjournal = {Nat Comput Sci},
	author = {Schuman, Catherine D. and Kulkarni, Shruti R. and Parsa, Maryam and Mitchell, J. Parker and Date, Prasanna and Kay, Bill},
	urldate = {2023-12-10},
	date = {2022-01-31},
	langid = {english},
	file = {Texto completo:/home/weverson/Zotero/storage/NLGX6XZ5/Schuman et al. - 2022 - Opportunities for neuromorphic computing algorithm.pdf:application/pdf},
}

@article{indiveri_memory_2015,
	title = {Memory and Information Processing in Neuromorphic Systems},
	volume = {103},
	issn = {0018-9219, 1558-2256},
	url = {http://ieeexplore.ieee.org/document/7159144/},
	doi = {10.1109/JPROC.2015.2444094},
	pages = {1379--1397},
	number = {8},
	journaltitle = {Proceedings of the {IEEE}},
	shortjournal = {Proc. {IEEE}},
	author = {Indiveri, Giacomo and Liu, Shih-Chii},
	urldate = {2023-12-10},
	date = {2015-08},
	file = {Versão submetida:/home/weverson/Zotero/storage/D5HM2I6W/Indiveri e Liu - 2015 - Memory and Information Processing in Neuromorphic .pdf:application/pdf},
}

@article{frenkel_bottom-up_2023,
	title = {Bottom-Up and Top-Down Approaches for the Design of Neuromorphic Processing Systems: Tradeoffs and Synergies Between Natural and Artificial Intelligence},
	volume = {111},
	issn = {0018-9219, 1558-2256},
	url = {https://ieeexplore.ieee.org/document/10144567/},
	doi = {10.1109/JPROC.2023.3273520},
	shorttitle = {Bottom-Up and Top-Down Approaches for the Design of Neuromorphic Processing Systems},
	pages = {623--652},
	number = {6},
	journaltitle = {Proceedings of the {IEEE}},
	shortjournal = {Proc. {IEEE}},
	author = {Frenkel, Charlotte and Bol, David and Indiveri, Giacomo},
	urldate = {2023-12-10},
	date = {2023-06},
	file = {Versão submetida:/home/weverson/Zotero/storage/9B9GIAR9/Frenkel et al. - 2023 - Bottom-Up and Top-Down Approaches for the Design o.pdf:application/pdf},
}

@article{hodgkin_quantitative_1952,
	title = {A quantitative description of membrane current and its application to conduction and excitation in nerve},
	volume = {117},
	issn = {0022-3751, 1469-7793},
	url = {https://physoc.onlinelibrary.wiley.com/doi/10.1113/jphysiol.1952.sp004764},
	doi = {10.1113/jphysiol.1952.sp004764},
	pages = {500--544},
	number = {4},
	journaltitle = {The Journal of Physiology},
	shortjournal = {The Journal of Physiology},
	author = {Hodgkin, A. L. and Huxley, A. F.},
	urldate = {2023-12-10},
	date = {1952-08-28},
	langid = {english},
	file = {Texto completo:/home/weverson/Zotero/storage/K76R5Z9L/Hodgkin e Huxley - 1952 - A quantitative description of membrane current and.pdf:application/pdf},
}

@article{furber_spinnaker_2014,
	title = {The {SpiNNaker} Project},
	volume = {102},
	issn = {0018-9219, 1558-2256},
	url = {https://ieeexplore.ieee.org/document/6750072/},
	doi = {10.1109/JPROC.2014.2304638},
	pages = {652--665},
	number = {5},
	journaltitle = {Proceedings of the {IEEE}},
	shortjournal = {Proc. {IEEE}},
	author = {Furber, Steve B. and Galluppi, Francesco and Temple, Steve and Plana, Luis A.},
	urldate = {2023-12-10},
	date = {2014-05},
	keywords = {Brain modeling, Computational modeling, Neural networks, Computer architecture, multicast algorithms, multiprocessor interconnection networks, Multitasking, neural network hardware, Neuroscience, parallel programming, Parallel programming, Program processors},
	file = {Texto completo:/home/weverson/Zotero/storage/5F7GR4BM/Furber et al. - 2014 - The SpiNNaker Project.pdf:application/pdf},
}

@inproceedings{schemmel_wafer-scale_2010,
	location = {Paris, France},
	title = {A wafer-scale neuromorphic hardware system for large-scale neural modeling},
	isbn = {978-1-4244-5308-5},
	url = {http://ieeexplore.ieee.org/document/5536970/},
	doi = {10.1109/ISCAS.2010.5536970},
	eventtitle = {2010 {IEEE} International Symposium on Circuits and Systems - {ISCAS} 2010},
	pages = {1947--1950},
	booktitle = {Proceedings of 2010 {IEEE} International Symposium on Circuits and Systems},
	publisher = {{IEEE}},
	author = {Schemmel, Johannes and Briiderle, Daniel and Griibl, Andreas and Hock, Matthias and Meier, Karlheinz and Millner, Sebastian},
	urldate = {2023-12-10},
	date = {2010-05},
	file = {Texto completo:/home/weverson/Zotero/storage/6S6ZUZQL/Schemmel et al. - 2010 - A wafer-scale neuromorphic hardware system for lar.pdf:application/pdf},
}

@inproceedings{aamir_lif_2017,
	location = {Torino},
	title = {From {LIF} to {AdEx} neuron models: Accelerated analog 65 nm {CMOS} implementation},
	isbn = {978-1-5090-5803-7},
	url = {http://ieeexplore.ieee.org/document/8325167/},
	doi = {10.1109/BIOCAS.2017.8325167},
	shorttitle = {From {LIF} to {AdEx} neuron models},
	eventtitle = {2017 {IEEE} Biomedical Circuits and Systems Conference ({BioCAS})},
	pages = {1--4},
	booktitle = {2017 {IEEE} Biomedical Circuits and Systems Conference ({BioCAS})},
	publisher = {{IEEE}},
	author = {Aamir, Syed Ahmed and Muller, Paul and Kriener, Laura and Kiene, Gerd and Schemmel, Johannes and Meier, Karlheinz},
	urldate = {2023-12-10},
	date = {2017-10},
	file = {Texto completo:/home/weverson/Zotero/storage/I8QJCYJK/Aamir et al. - 2017 - From LIF to AdEx neuron models Accelerated analog.pdf:application/pdf},
}

@article{merolla_million_2014,
	title = {A million spiking-neuron integrated circuit with a scalable communication network and interface},
	volume = {345},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.1254642},
	doi = {10.1126/science.1254642},
	abstract = {Modeling computer chips on real brains
            
              Computers are nowhere near as versatile as our own brains. Merolla
              et al.
              applied our present knowledge of the structure and function of the brain to design a new computer chip that uses the same wiring rules and architecture. The flexible, scalable chip operated efficiently in real time, while using very little power.
            
            
              Science
              , this issue p.
              668
            
          , 
            A large-scale computer chip mimics many features of a real brain.
          , 
            Inspired by the brain’s structure, we have developed an efficient, scalable, and flexible non–von Neumann architecture that leverages contemporary silicon technology. To demonstrate, we built a 5.4-billion-transistor chip with 4096 neurosynaptic cores interconnected via an intrachip network that integrates 1 million programmable spiking neurons and 256 million configurable synapses. Chips can be tiled in two dimensions via an interchip communication interface, seamlessly scaling the architecture to a cortexlike sheet of arbitrary size. The architecture is well suited to many applications that use complex neural networks in real time, for example, multiobject detection and classification. With 400-pixel-by-240-pixel video input at 30 frames per second, the chip consumes 63 milliwatts.},
	pages = {668--673},
	number = {6197},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Merolla, Paul A. and Arthur, John V. and Alvarez-Icaza, Rodrigo and Cassidy, Andrew S. and Sawada, Jun and Akopyan, Filipp and Jackson, Bryan L. and Imam, Nabil and Guo, Chen and Nakamura, Yutaka and Brezzo, Bernard and Vo, Ivan and Esser, Steven K. and Appuswamy, Rathinakumar and Taba, Brian and Amir, Arnon and Flickner, Myron D. and Risk, William P. and Manohar, Rajit and Modha, Dharmendra S.},
	urldate = {2023-12-10},
	date = {2014-08-08},
	langid = {english},
}

@article{davies_loihi_2018,
	title = {Loihi: A Neuromorphic Manycore Processor with On-Chip Learning},
	volume = {38},
	issn = {0272-1732, 1937-4143},
	url = {https://ieeexplore.ieee.org/document/8259423/},
	doi = {10.1109/MM.2018.112130359},
	shorttitle = {Loihi},
	pages = {82--99},
	number = {1},
	journaltitle = {{IEEE} Micro},
	shortjournal = {{IEEE} Micro},
	author = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and Liao, Yuyun and Lin, Chit-Kwan and Lines, Andrew and Liu, Ruokun and Mathaikutty, Deepak and {McCoy}, Steven and Paul, Arnab and Tse, Jonathan and Venkataramanan, Guruguhanathan and Weng, Yi-Hsin and Wild, Andreas and Yang, Yoonseok and Wang, Hong},
	urldate = {2023-12-10},
	date = {2018-01},
	file = {Texto completo:/home/weverson/Zotero/storage/465CBHW6/Davies et al. - 2018 - Loihi A Neuromorphic Manycore Processor with On-C.pdf:application/pdf},
}

@article{mehonic_brain-inspired_2022,
	title = {Brain-inspired computing needs a master plan},
	volume = {604},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-021-04362-w},
	doi = {10.1038/s41586-021-04362-w},
	pages = {255--260},
	number = {7905},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Mehonic, A. and Kenyon, A. J.},
	urldate = {2023-12-10},
	date = {2022-04-14},
	langid = {english},
	file = {Texto completo:/home/weverson/Zotero/storage/RAFPNAIH/Mehonic e Kenyon - 2022 - Brain-inspired computing needs a master plan.pdf:application/pdf},
}

@article{mehonic_memristorsmemory_2020,
	title = {Memristors—From In‐Memory Computing, Deep Learning Acceleration, and Spiking Neural Networks to the Future of Neuromorphic and Bio‐Inspired Computing},
	volume = {2},
	issn = {2640-4567, 2640-4567},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/aisy.202000085},
	doi = {10.1002/aisy.202000085},
	abstract = {Machine learning, particularly in the form of deep learning ({DL}), has driven most of the recent fundamental developments in artificial intelligence ({AI}). {DL} is based on computational models that are, to a certain extent, bio‐inspired, as they rely on networks of connected simple computing units operating in parallel. The success of {DL} is supported by three factors: availability of vast amounts of data, continuous growth in computing power, and algorithmic innovations. The approaching demise of Moore's law, and the consequent expected modest improvements in computing power that can be achieved by scaling, raises the question of whether the progress will be slowed or halted due to hardware limitations. This article reviews the case for a novel beyond‐complementary metal–oxide–semiconductor ({CMOS}) technology—memristors—as a potential solution for the implementation of power‐efficient in‐memory computing, {DL} accelerators, and spiking neural networks. Central themes are the reliance on non‐von‐Neumann computing architectures and the need for developing tailored learning and inference algorithms. To argue that lessons from biology can be useful in providing directions for further progress in {AI}, an example‐based reservoir computing is briefly discussed. At the end, speculation is given on the “big picture” view of future neuromorphic and brain‐inspired computing systems.},
	pages = {2000085},
	number = {11},
	journaltitle = {Advanced Intelligent Systems},
	shortjournal = {Advanced Intelligent Systems},
	author = {Mehonic, Adnan and Sebastian, Abu and Rajendran, Bipin and Simeone, Osvaldo and Vasilaki, Eleni and Kenyon, Anthony J.},
	urldate = {2023-12-10},
	date = {2020-11},
	langid = {english},
	file = {Texto completo:/home/weverson/Zotero/storage/YUKQR4ZN/Mehonic et al. - 2020 - Memristors—From In‐Memory Computing, Deep Learning.pdf:application/pdf},
}

@article{roy_towards_2019,
	title = {Towards spike-based machine intelligence with neuromorphic computing},
	volume = {575},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1677-2},
	doi = {10.1038/s41586-019-1677-2},
	pages = {607--617},
	number = {7784},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Roy, Kaushik and Jaiswal, Akhilesh and Panda, Priyadarshini},
	urldate = {2023-12-10},
	date = {2019-11-28},
	langid = {english},
	file = {Texto completo:/home/weverson/Zotero/storage/9E7HS46R/Roy et al. - 2019 - Towards spike-based machine intelligence with neur.pdf:application/pdf},
}

@inproceedings{diehl_conversion_2016,
	location = {San Diego, {CA}, {USA}},
	title = {Conversion of artificial recurrent neural networks to spiking neural networks for low-power neuromorphic hardware},
	isbn = {978-1-5090-1370-8},
	url = {http://ieeexplore.ieee.org/document/7738691/},
	doi = {10.1109/ICRC.2016.7738691},
	eventtitle = {2016 {IEEE} International Conference on Rebooting Computing ({ICRC})},
	pages = {1--8},
	booktitle = {2016 {IEEE} International Conference on Rebooting Computing ({ICRC})},
	publisher = {{IEEE}},
	author = {Diehl, Peter U. and Zarrella, Guido and Cassidy, Andrew and Pedroni, Bruno U. and Neftci, Emre},
	urldate = {2023-12-10},
	date = {2016-10},
	file = {Versão submetida:/home/weverson/Zotero/storage/LYV2JYAZ/Diehl et al. - 2016 - Conversion of artificial recurrent neural networks.pdf:application/pdf},
}

@article{decharms_primary_1996,
	title = {Primary cortical representation of sounds by the coordination of action-potential timing},
	volume = {381},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/381610a0},
	doi = {10.1038/381610a0},
	pages = {610--613},
	number = {6583},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {{deCharms}, R. Christopher and Merzenich, Michael M.},
	urldate = {2023-12-10},
	date = {1996-06},
	langid = {english},
}

@inbook{mallot_coding_2013,
	location = {Heidelberg},
	title = {Coding and Representation},
	volume = {2},
	isbn = {978-3-319-00860-8 978-3-319-00861-5},
	url = {http://link.springer.com/10.1007/978-3-319-00861-5_5},
	pages = {113--129},
	booktitle = {Computational Neuroscience},
	publisher = {Springer International Publishing},
	author = {Mallot, Hanspeter A},
	bookauthor = {Mallot, Hanspeter A},
	urldate = {2023-12-10},
	date = {2013},
	doi = {10.1007/978-3-319-00861-5_5},
	note = {Series Title: Springer Series in Bio-/Neuroinformatics},
}

@article{bohte_evidence_2004,
	title = {The evidence for neural information processing with precise spike-times: A survey},
	volume = {3},
	issn = {1567-7818},
	url = {http://link.springer.com/10.1023/B:NACO.0000027755.02868.60},
	doi = {10.1023/B:NACO.0000027755.02868.60},
	shorttitle = {The evidence for neural information processing with precise spike-times},
	pages = {195--206},
	number = {2},
	journaltitle = {Natural Computing},
	shortjournal = {Natural Computing},
	author = {Bohte, Sander M.},
	urldate = {2023-12-10},
	date = {2004},
	langid = {english},
	file = {Versão submetida:/home/weverson/Zotero/storage/NXWXEFBD/Bohte - 2004 - The evidence for neural information processing wit.pdf:application/pdf},
}

@article{markram_history_2011,
	title = {A history of spike-timing-dependent plasticity},
	volume = {3},
	issn = {16633563},
	url = {http://journal.frontiersin.org/article/10.3389/fnsyn.2011.00004/abstract},
	doi = {10.3389/fnsyn.2011.00004},
	journaltitle = {Frontiers in Synaptic Neuroscience},
	shortjournal = {Front. Syn. Neurosci.},
	author = {Markram, Henry and Gerstner, Wulfram and Sjöstöm, r Jesper},
	urldate = {2023-12-10},
	date = {2011},
	file = {Texto completo:/home/weverson/Zotero/storage/YTE7ZNFD/Markram - 2011 - A history of spike-timing-dependent plasticity.pdf:application/pdf},
}

@article{kheradpisheh_stdp-based_2018,
	title = {{STDP}-based spiking deep convolutional neural networks for object recognition},
	volume = {99},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608017302903},
	doi = {10.1016/j.neunet.2017.12.005},
	pages = {56--67},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Kheradpisheh, Saeed Reza and Ganjtabesh, Mohammad and Thorpe, Simon J. and Masquelier, Timothée},
	urldate = {2023-12-10},
	date = {2018-03},
	langid = {english},
	file = {Texto completo:/home/weverson/Zotero/storage/HESA64Q5/Kheradpisheh et al. - 2018 - STDP-based spiking deep convolutional neural netwo.pdf:application/pdf},
}

@inproceedings{diehl_fast-classifying_2015,
	location = {Killarney, Ireland},
	title = {Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing},
	isbn = {978-1-4799-1960-4},
	url = {http://ieeexplore.ieee.org/document/7280696/},
	doi = {10.1109/IJCNN.2015.7280696},
	eventtitle = {2015 International Joint Conference on Neural Networks ({IJCNN})},
	pages = {1--8},
	booktitle = {2015 International Joint Conference on Neural Networks ({IJCNN})},
	publisher = {{IEEE}},
	author = {Diehl, Peter U. and Neil, Daniel and Binas, Jonathan and Cook, Matthew and Liu, Shih-Chii and Pfeiffer, Michael},
	urldate = {2023-12-11},
	date = {2015-07},
	file = {Texto completo:/home/weverson/Zotero/storage/AHZ84C4Q/Diehl et al. - 2015 - Fast-classifying, high-accuracy spiking deep netwo.pdf:application/pdf},
}

@article{bohte_error-backpropagation_2002,
	title = {Error-backpropagation in temporally encoded networks of spiking neurons},
	volume = {48},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231201006580},
	doi = {10.1016/S0925-2312(01)00658-0},
	pages = {17--37},
	number = {1},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Bohte, Sander M. and Kok, Joost N. and La Poutré, Han},
	urldate = {2023-12-11},
	date = {2002-10},
	langid = {english},
	file = {Versão submetida:/home/weverson/Zotero/storage/T8WTB8GA/Bohte et al. - 2002 - Error-backpropagation in temporally encoded networ.pdf:application/pdf},
}

@article{ponulak_supervised_2010,
	title = {Supervised Learning in Spiking Neural Networks with {ReSuMe}: Sequence Learning, Classification, and Spike Shifting},
	volume = {22},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/22/2/467-510/7529},
	doi = {10.1162/neco.2009.11-08-901},
	shorttitle = {Supervised Learning in Spiking Neural Networks with {ReSuMe}},
	abstract = {Learning from instructions or demonstrations is a fundamental property of our brain necessary to acquire new knowledge and develop novel skills or behavioral patterns. This type of learning is thought to be involved in most of our daily routines. Although the concept of instruction-based learning has been studied for several decades, the exact neural mechanisms implementing this process remain unrevealed. One of the central questions in this regard is, How do neurons learn to reproduce template signals (instructions) encoded in precisely timed sequences of spikes?
            Here we present a model of supervised learning for biologically plausible neurons that addresses this question. In a set of experiments, we demonstrate that our approach enables us to train spiking neurons to reproduce arbitrary template spike patterns in response to given synaptic stimuli even in the presence of various sources of noise.
            We show that the learning rule can also be used for decision-making tasks. Neurons can be trained to classify categories of input signals based on only a temporal configuration of spikes. The decision is communicated by emitting precisely timed spike trains associated with given input categories. Trained neurons can perform the classification task correctly even if stimuli and corresponding decision times are temporally separated and the relevant information is consequently highly overlapped by the ongoing neural activity.
            Finally, we demonstrate that neurons can be trained to reproduce sequences of spikes with a controllable time shift with respect to target templates. A reproduced signal can follow or even precede the targets. This surprising result points out that spiking neurons can potentially be applied to forecast the behavior (firing times) of other reference neurons or networks.},
	pages = {467--510},
	number = {2},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Ponulak, Filip and Kasiński, Andrzej},
	urldate = {2023-12-11},
	date = {2010-02},
	langid = {english},
}

@article{mohemmed_span_2012,
	title = {Span: spike pattern association neuron for learning spatio-temporal spike patterns},
	volume = {22},
	issn = {0129-0657, 1793-6462},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0129065712500128},
	doi = {10.1142/S0129065712500128},
	shorttitle = {{SPAN}},
	abstract = {Spiking Neural Networks ({SNN}) were shown to be suitable tools for the processing of spatio-temporal information. However, due to their inherent complexity, the formulation of efficient supervised learning algorithms for {SNN} is difficult and remains an important problem in the research area. This article presents {SPAN} — a spiking neuron that is able to learn associations of arbitrary spike trains in a supervised fashion allowing the processing of spatio-temporal information encoded in the precise timing of spikes. The idea of the proposed algorithm is to transform spike trains during the learning phase into analog signals so that common mathematical operations can be performed on them. Using this conversion, it is possible to apply the well-known Widrow–Hoff rule directly to the transformed spike trains in order to adjust the synaptic weights and to achieve a desired input/output spike behavior of the neuron. In the presented experimental analysis, the proposed learning algorithm is evaluated regarding its learning capabilities, its memory capacity, its robustness to noisy stimuli and its classification performance. Differences and similarities of {SPAN} regarding two related algorithms, {ReSuMe} and Chronotron, are discussed.},
	pages = {1250012},
	number = {4},
	journaltitle = {International Journal of Neural Systems},
	shortjournal = {Int. J. Neur. Syst.},
	author = {Mohemmed, Ammar and Schliebs, Stefan and Matsuda, Satoshi and Kasabov, Nikola},
	urldate = {2023-12-11},
	date = {2012-08},
	langid = {english},
	file = {Versão aceita:/home/weverson/Zotero/storage/TALHP6SY/Mohemmed et al. - 2012 - SPAN SPIKE PATTERN ASSOCIATION NEURON FOR LEARNIN.pdf:application/pdf},
}

@article{mateos-aparicio_impact_2019,
	title = {The Impact of Studying Brain Plasticity},
	volume = {13},
	issn = {1662-5102},
	url = {https://www.frontiersin.org/article/10.3389/fncel.2019.00066/full},
	doi = {10.3389/fncel.2019.00066},
	pages = {66},
	journaltitle = {Frontiers in Cellular Neuroscience},
	shortjournal = {Front. Cell. Neurosci.},
	author = {Mateos-Aparicio, Pedro and Rodríguez-Moreno, Antonio},
	urldate = {2023-12-12},
	date = {2019-02-27},
	file = {Texto completo:/home/weverson/Zotero/storage/76HB2SJH/Mateos-Aparicio e Rodríguez-Moreno - 2019 - The Impact of Studying Brain Plasticity.pdf:application/pdf},
}

@article{bliss_longlasting_1973,
	title = {Long‐lasting potentiation of synaptic transmission in the dentate area of the anaesthetized rabbit following stimulation of the perforant path},
	volume = {232},
	issn = {0022-3751, 1469-7793},
	url = {https://physoc.onlinelibrary.wiley.com/doi/10.1113/jphysiol.1973.sp010273},
	doi = {10.1113/jphysiol.1973.sp010273},
	abstract = {1. The after‐effects of repetitive stimulation of the perforant path fibres to the dentate area of the hippocampal formation have been examined with extracellular micro‐electrodes in rabbits anaesthetized with urethane.
            2. In fifteen out of eighteen rabbits the population response recorded from granule cells in the dentate area to single perforant path volleys was potentiated for periods ranging from 30 min to 10 hr after one or more conditioning trains at 10–20/sec for 10–15 sec, or 100/sec for 3–4 sec.
            3. The population response was analysed in terms of three parameters: the amplitude of the population excitatory post‐synaptic potential (e.p.s.p.), signalling the depolarization of the granule cells, and the amplitude and latency of the population spike, signalling the discharge of the granule cells.
            4. All three parameters were potentiated in 29\% of the experiments; in other experiments in which long term changes occurred, potentiation was confined to one or two of the three parameters. A reduction in the latency of the population spike was the commonest sign of potentiation, occurring in 57\% of all experiments. The amplitude of the population e.p.s.p. was increased in 43\%, and of the population spike in 40\%, of all experiments.
            5. During conditioning at 10–20/sec there was massive potentiation of the population spike (‘frequency potentiation’). The spike was suppressed during stimulation at 100/sec. Both frequencies produced long‐term potentiation.
            
              6. The results suggest that two independent mechanisms are responsible for long‐lasting potentiation: (
              a
              ) an increase in the efficiency of synaptic transmission at the perforant path synapses; (
              b
              ) an increase in the excitability of the granule cell population.},
	pages = {331--356},
	number = {2},
	journaltitle = {The Journal of Physiology},
	shortjournal = {The Journal of Physiology},
	author = {Bliss, T. V. P. and Lømo, T.},
	urldate = {2023-12-12},
	date = {1973-07},
	langid = {english},
	file = {Texto completo:/home/weverson/Zotero/storage/6TEEIL2P/Bliss e Lømo - 1973 - Long‐lasting potentiation of synaptic transmission.pdf:application/pdf},
}

@article{bear_long-term_1996,
	title = {Long-Term Depression in Hippocampus},
	volume = {19},
	issn = {0147-006X, 1545-4126},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.ne.19.030196.002253},
	doi = {10.1146/annurev.ne.19.030196.002253},
	abstract = {Long-term depression ({LTD}) is a lasting decrease in synaptic effectiveness that follows some types of electrical stimulation in the hippocampus. Two broad types of {LTD} may be distinguished. Heterosynaptic {LTD} can occur at synapses that are inactive, normally during high-frequency stimulation of a converging synaptic input. Homosynaptic {LTD} can occur at synapses that are activated, normally at low frequencies. Here we discuss the mechanisms of {LTD} and their possible relevance to hippocampal function.},
	pages = {437--462},
	number = {1},
	journaltitle = {Annual Review of Neuroscience},
	shortjournal = {Annu. Rev. Neurosci.},
	author = {Bear, Mark F. and Abraham, Wickliffe C.},
	urldate = {2023-12-12},
	date = {1996-03},
	langid = {english},
}

@article{song_competitive_2000,
	title = {Competitive Hebbian learning through spike-timing-dependent synaptic plasticity},
	volume = {3},
	issn = {1097-6256, 1546-1726},
	url = {https://www.nature.com/articles/nn0900_919},
	doi = {10.1038/78829},
	pages = {919--926},
	number = {9},
	journaltitle = {Nature Neuroscience},
	shortjournal = {Nat Neurosci},
	author = {Song, Sen and Miller, Kenneth D. and Abbott, L. F.},
	urldate = {2023-12-12},
	date = {2000-09},
	langid = {english},
	file = {Texto completo:/home/weverson/Zotero/storage/4ZLPN4AV/Song et al. - 2000 - Competitive Hebbian learning through spike-timing-.pdf:application/pdf},
}

@article{wilson_excitatory_1972,
	title = {Excitatory and Inhibitory Interactions in Localized Populations of Model Neurons},
	volume = {12},
	issn = {00063495},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006349572860685},
	doi = {10.1016/S0006-3495(72)86068-5},
	pages = {1--24},
	number = {1},
	journaltitle = {Biophysical Journal},
	shortjournal = {Biophysical Journal},
	author = {Wilson, Hugh R. and Cowan, Jack D.},
	urldate = {2023-12-13},
	date = {1972-01},
	langid = {english},
	file = {Texto completo:/home/weverson/Zotero/storage/RPSLHA2E/Wilson e Cowan - 1972 - Excitatory and Inhibitory Interactions in Localize.pdf:application/pdf},
}

@article{dale_pharmacology_1935,
	title = {Pharmacology and Nerve-Endings},
	volume = {28},
	issn = {0035-9157},
	url = {http://journals.sagepub.com/doi/10.1177/003591573502800330},
	doi = {10.1177/003591573502800330},
	abstract = {A brief account is given of the scientific career of Walter Ernest Dixon, and of the importance of his work and his influence for the development of Pharmacology in England. It is suggested that the Memorial Lecture may appropriately deal with some matter of new interest, from one of the fields of research in which Dixon himself was active. Special mention is made of his work with Brodie on the physiology and pharmacology of the bronchioles and the pulmonary blood-vessels, as probably showing the beginning of Dixon's interest in the actions of the alkaloids and organic bases which reproduce the effects of autonomic nerves.
            An account is given of Dixon's early interest in the suggestion, first made by Elliott, that autonomic nerves transmit their effects by releasing, at their endings, specific substances, which reproduce their actions; and of his attempt to obtain experimental support for this conception. After the War it was established by the experiments of O. Loewi; and it is now generally recognized that parasympathetic effects are so transmitted by release of acetylcholine, sympathetic effects by that of a substance related to adrenaline.
            Very recent evidence indicates that acetylcholine, by virtue of its other (“nicotine-like”) action, also acts as transmitter of activity at synapses in autonomic ganglia, and from motor nerve to voluntary muscle.
            The terms “cholinergic” and “adrenergic” have been introduced to describe nerve-fibres which transmit their actions by the release at their endings of acetylcholine, and of a substance related to adrenaline, respectively. It is shown that Langley and Anderson's evidence, long available, as to the kinds of peripheral efferent fibres which can replace one another in regeneration, can be summarized by the statement, that cholinergic can replace cholinergic fibres, and that adrenergic can replace adrenergic fibres; but that fibres of different chemical function cannot replace one another. The bearing of this new evidence on conceptions of the mode of action of “neuromimetic” drugs is discussed. The pharmacological problem can now be more clearly defined, and Dixon's participation in further attempts at its solution will be sadly missed.},
	pages = {319--332},
	number = {3},
	journaltitle = {Proceedings of the Royal Society of Medicine},
	shortjournal = {Proceedings of the Royal Society of Medicine},
	author = {Dale, Henry},
	urldate = {2023-12-13},
	date = {1935-01},
	langid = {english},
	file = {Texto completo:/home/weverson/Zotero/storage/D5VNY3I5/Dale - 1935 - Pharmacology and Nerve-Endings.pdf:application/pdf},
}

@article{ramezanian-panahi_generative_2022,
	title = {Generative Models of Brain Dynamics},
	volume = {5},
	issn = {2624-8212},
	url = {https://www.frontiersin.org/articles/10.3389/frai.2022.807406/full},
	doi = {10.3389/frai.2022.807406},
	abstract = {This review article gives a high-level overview of the approaches across different scales of organization and levels of abstraction. The studies covered in this paper include fundamental models in computational neuroscience, nonlinear dynamics, data-driven methods, as well as emergent practices. While not all of these models span the intersection of neuroscience, {AI}, and system dynamics, all of them do or can work in tandem as generative models, which, as we argue, provide superior properties for the analysis of neuroscientific data. We discuss the limitations and unique dynamical traits of brain data and the complementary need for hypothesis- and data-driven modeling. By way of conclusion, we present several hybrid generative models from recent literature in scientific machine learning, which can be efficiently deployed to yield interpretable models of neural dynamics.},
	pages = {807406},
	journaltitle = {Frontiers in Artificial Intelligence},
	shortjournal = {Front. Artif. Intell.},
	author = {Ramezanian-Panahi, Mahta and Abrevaya, Germán and Gagnon-Audet, Jean-Christophe and Voleti, Vikram and Rish, Irina and Dumas, Guillaume},
	urldate = {2023-12-14},
	date = {2022-07-15},
	file = {Texto completo:/home/weverson/Zotero/storage/RRVWY4UA/Ramezanian-Panahi et al. - 2022 - Generative Models of Brain Dynamics.pdf:application/pdf},
}

@inbook{bisong_google_2019,
	location = {Berkeley, {CA}},
	title = {Google Colaboratory},
	isbn = {978-1-4842-4469-2 978-1-4842-4470-8},
	url = {http://link.springer.com/10.1007/978-1-4842-4470-8_7},
	pages = {59--64},
	booktitle = {Building Machine Learning and Deep Learning Models on Google Cloud Platform},
	publisher = {Apress},
	author = {Bisong, Ekaba},
	bookauthor = {Bisong, Ekaba},
	urldate = {2023-12-14},
	date = {2019},
	langid = {english},
	doi = {10.1007/978-1-4842-4470-8_7},
}

@article{rattay_model_2001,
	title = {A model of the electrically excited human cochlear neuron},
	volume = {153},
	issn = {03785955},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0378595500002562},
	doi = {10.1016/S0378-5955(00)00256-2},
	pages = {43--63},
	number = {1},
	journaltitle = {Hearing Research},
	shortjournal = {Hearing Research},
	author = {Rattay, Frank and Lutter, Petra and Felix, Heidi},
	urldate = {2023-12-14},
	date = {2001-03},
	langid = {english},
}

@article{cavanagh_multiple_2019,
	title = {Multiple Dissociations Between Comorbid Depression and Anxiety on Reward and Punishment Processing: Evidence From Computationally Informed {EEG}},
	volume = {3},
	issn = {2379-6227},
	url = {https://cpsyjournal.org/article/10.1162/CPSY_a_00024/},
	doi = {10.1162/CPSY_a_00024},
	shorttitle = {Multiple Dissociations Between Comorbid Depression and Anxiety on Reward and Punishment Processing},
	pages = {1},
	number = {0},
	journaltitle = {Computational Psychiatry},
	author = {Cavanagh, James F. and Bismark, Andrew W. and Frank, Michael J. and Allen, John J. B.},
	urldate = {2023-12-14},
	date = {2019-01-01},
	file = {Texto completo:/home/weverson/Zotero/storage/CJS2PJW4/Cavanagh et al. - 2019 - Multiple Dissociations Between Comorbid Depression.pdf:application/pdf},
}

@article{harris_array_2020,
	title = {Array programming with {NumPy}},
	volume = {585},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2649-2},
	doi = {10.1038/s41586-020-2649-2},
	abstract = {Abstract
            
              Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. {NumPy} is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, {NumPy} was an important part of the software stack used in the discovery of gravitational waves
              1
              and in the first imaging of a black hole
              2
              . Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. {NumPy} is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own {NumPy}-like interfaces and array objects. Owing to its central position in the ecosystem, {NumPy} increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface ({API}), provides a flexible framework to support the next decade of scientific and industrial analysis.},
	pages = {357--362},
	number = {7825},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Harris, Charles R. and Millman, K. Jarrod and Van Der Walt, Stéfan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and Van Kerkwijk, Marten H. and Brett, Matthew and Haldane, Allan and Del Río, Jaime Fernández and Wiebe, Mark and Peterson, Pearu and Gérard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
	urldate = {2023-12-14},
	date = {2020-09-17},
	langid = {english},
	file = {Texto completo:/home/weverson/Zotero/storage/X4NC4UMR/Harris et al. - 2020 - Array programming with NumPy.pdf:application/pdf},
}

@article{hunter_matplotlib_2007,
	title = {Matplotlib: A 2D Graphics Environment},
	volume = {9},
	issn = {1521-9615},
	url = {http://ieeexplore.ieee.org/document/4160265/},
	doi = {10.1109/MCSE.2007.55},
	shorttitle = {Matplotlib},
	pages = {90--95},
	number = {3},
	journaltitle = {Computing in Science \& Engineering},
	shortjournal = {Comput. Sci. Eng.},
	author = {Hunter, John D.},
	urldate = {2023-12-14},
	date = {2007},
}
