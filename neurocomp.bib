
@article{tan_spiking_2020,
	title = {Spiking {Neural} {Networks}: {Background}, {Recent} {Development} and the {NeuCube} {Architecture}},
	volume = {52},
	issn = {1573-773X},
	shorttitle = {Spiking {Neural} {Networks}},
	url = {https://doi.org/10.1007/s11063-020-10322-8},
	doi = {10.1007/s11063-020-10322-8},
	abstract = {This paper reviews recent developments in the still-off-the-mainstream information and data processing area of spiking neural networks (SNN)—the third generation of artificial neural networks. We provide background information about the functioning of biological neurons, discussing the most important and commonly used mathematical neural models. Most relevant information processing techniques, learning algorithms, and applications of spiking neurons are described and discussed, focusing on feasibility and biological plausibility of the methods. Specifically, we describe in detail the functioning and organization of the latest version of a 3D spatio-temporal SNN-based data machine framework called NeuCube, as well as it’s SNN-related submodules. All described submodules are accompanied with formal algorithmic formulations. The architecture is highly relevant for the analysis and interpretation of various types of spatio-temporal brain data (STBD), like EEG, NIRS, fMRI, but we highlight some of the recent both STBD- and non-STBD-based applications. Finally, we summarise and discuss some open research problems that can be addressed in the future. These include, but are not limited to: application in the area of EEG-based BCI through transfer learning; application in the area of affective computing through the extension of the NeuCube framework which would allow for a biologically plausible SNN-based integration of central and peripheral nervous system measures. Matlab implementation of the NeuCube’s SNN-related module is available for research and teaching purposes.},
	language = {en},
	number = {2},
	urldate = {2021-10-01},
	journal = {Neural Processing Letters},
	author = {Tan, Clarence and Šarlija, Marko and Kasabov, Nikola},
	month = oct,
	year = {2020},
	pages = {1675--1701},
	file = {Springer Full Text PDF:/mnt/work-data/weverson/Zotero/storage/WF3XYKVN/Tan et al. - 2020 - Spiking Neural Networks Background, Recent Develo.pdf:application/pdf},
}

@inproceedings{kumarasinghe_espannet_2019,
	title = {{eSPANNet}: {Evolving} {Spike} {Pattern} {Association} {Neural} {Network} for {Spike}-based {Supervised} {Incremental} {Learning} and {Its} {Application} for {Single}-trial {Brain} {Computer} {Interfaces}},
	shorttitle = {{eSPANNet}},
	doi = {10.1109/IJCNN.2019.8852213},
	abstract = {Objective:Due to the non-stationarity and high trialto-trial variability, online event prediction from biomedical signals is challenging. This is more significant when it is applied to neurological rehabilitation where the person incrementally learns to regain the control of movement. eSPANNet is a computational model inspired by the incremental learning for motor control in living nervous systems. It is inspired by the concept of 'population vectors' which have been experimentally proven by several computational neuroscience studies. In this paper, we present a proof-of-concept study on the proposed computational model. Our goal is to utilize the polychronization effect of Spiking Neural Networks to develop a better neural decoder for Brain-Computer Interfaces. Methods: The eSPANNet model contains a network of Spike Pattern Association Neurons, a spiking neuron model which is able to emit spikes at the desired time-point. Results: The proposed approach was experimentally validated using the finger flexion prediction dataset from the fourth BCI competition. The results show that eSPANNet results in 1) a higher classification accuracy, sensitivity and F1 score compared to several other multi-class classifiers and, 2) a better approximation of the actual movement compared to several regression analysis based approaches. Conclusion and Significance: The novelty of our algorithm is the ability to learn which inputs to focus on in an online manner. We suggest that the eSPANNet is a better BCI decoder due to its i) incremental and life-long learning, ii) compatibility with the neuromorphic platforms and, iii) ability to address the non-stationarity of brain data.},
	booktitle = {2019 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Kumarasinghe, Kaushalya and Taylor, Denise and Kasabov, Nikola},
	month = jul,
	year = {2019},
	note = {ISSN: 2161-4407},
	keywords = {Biological neural networks, Brain modeling, Computational modeling, Neurons, Sociology, Statistics, Training},
	pages = {1--8},
	file = {IEEE Xplore Full Text PDF:/mnt/work-data/weverson/Zotero/storage/4I4DPVG4/Kumarasinghe et al. - 2019 - eSPANNet Evolving Spike Pattern Association Neura.pdf:application/pdf},
}

@article{kumarasinghe_brain-inspired_2021,
	title = {Brain-inspired spiking neural networks for decoding and understanding muscle activity and kinematics from electroencephalography signals during hand movements},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-81805-4},
	doi = {10.1038/s41598-021-81805-4},
	abstract = {Compared to the abilities of the animal brain, many Artificial Intelligence systems have limitations which emphasise the need for a Brain-Inspired Artificial Intelligence paradigm. This paper proposes a novel Brain-Inspired Spiking Neural Network (BI-SNN) model for incremental learning of spike sequences. BI-SNN maps spiking activity from input channels into a high dimensional source-space which enhances the evolution of polychronising spiking neural populations. We applied the BI-SNN to predict muscle activity and kinematics from electroencephalography signals during upper limb functional movements. The BI-SNN extends our previously proposed eSPANNet computational model by integrating it with the ‘NeuCube’ brain-inspired SNN architecture. We show that BI-SNN can successfully predict continuous muscle activity and kinematics of upper-limb. The experimental results confirmed that the BI-SNN resulted in strongly correlated population activity and demonstrated the feasibility for real-time prediction. In contrast to the majority of Brain–Computer Interfaces (BCIs) that constitute a ‘black box’, BI-SNN provide quantitative and visual feedback about the related brain activity. This study is one of the first attempts to examine the feasibility of finding neural correlates of muscle activity and kinematics from electroencephalography using a brain-inspired computational paradigm. The findings suggest that BI-SNN is a better neural decoder for non-invasive BCI.},
	language = {en},
	number = {1},
	urldate = {2021-10-01},
	journal = {Scientific Reports},
	author = {Kumarasinghe, Kaushalya and Kasabov, Nikola and Taylor, Denise},
	month = jan,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Brain–machine interface;Computer science;Electroencephalography – EEG;Learning algorithms;Network models;Neural decoding;Sensorimotor processing
Subject\_term\_id: brain-machine-interface;computer-science;electroencephalography-eeg;learning-algorithms;network-models;neural-decoding;sensorimotor-processing},
	pages = {2486},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/3327MBD9/Kumarasinghe et al. - 2021 - Brain-inspired spiking neural networks for decodin.pdf:application/pdf},
}

@article{lu_autonomous_2021,
	title = {An autonomous learning mobile robot using biological reward modulate {STDP}},
	volume = {458},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231221009310},
	doi = {10.1016/j.neucom.2021.06.027},
	abstract = {Recent studies have shown that biologically inspired Spiking Neural Networks (SNNs) has potentials for the mobile robot controls. Based on SNNs, an autonomous learning paradigm for controlling mobile robots is proposed in this work, which can learn specific tasks autonomously. A reward modulated spike-timing-dependent plasticity (R-STDP) learning algorithm is designed to aid implementing the autonomous learning paradigm. It can train the SNN under different environmental states and conditions. The obstacle avoidance in the synthetic and real environments is used as a robotic task example to verify the effectiveness of the proposed paradigm. Results show that the mobile robot can learn autonomously under different environmental conditions and is able to avoid obstacles after learning processes complete.},
	language = {en},
	urldate = {2021-10-01},
	journal = {Neurocomputing},
	author = {Lu, Hao and Liu, Junxiu and Luo, Yuling and Hua, Yifan and Qiu, Senhui and Huang, Yongchuang},
	month = oct,
	year = {2021},
	keywords = {Autonomous learning, Reinforcement learning, Reward, Spike-timing-dependent plasticity, Spiking neural networks},
	pages = {308--318},
}

@article{galanis_efficient_2021,
	title = {Efficient {Deployment} of {Spiking} {Neural} {Networks} on {SpiNNaker} {Neuromorphic} {Platform}},
	volume = {68},
	issn = {1558-3791},
	doi = {10.1109/TCSII.2020.3047425},
	abstract = {Spiking Neural Networks (SNNs) have emerged as serious competitors of the traditional Convolutional Neural Networks (CNNs), as they unlock new potential of implementing less complex and more energy efficient neural networks. Current deep CNNs can be converted to SNNs for fast deployment on neuromorphic devices, however existing methods do not investigate the impact of hardware-related parameters that directly affect the accuracy of an SNN. In this brief, we target the SpiNNaker neuromorphic platform and we demonstrate a fast exploration framework that effectively decides the configuration of the target board, in order to achieve the highest possible accuracy. Experimental results show that our method reaches 98.85\% SNN accuracy on MNIST dataset, while reducing the exploration time by a factor of \$3{\textbackslash}times \$ compared to exhaustive search.},
	number = {6},
	journal = {IEEE Transactions on Circuits and Systems II: Express Briefs},
	author = {Galanis, Ioannis and Anagnostopoulos, Iraklis and Nguyen, Chinh and Bares, Guillermo},
	month = jun,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Circuits and Systems II: Express Briefs},
	keywords = {Synapses, Biological neural networks, Neurons, Artificial neural networks, Computer architecture, neural network hardware, neuromorphics, Neuromorphics, Routing, Time factors},
	pages = {1937--1941},
	file = {IEEE Xplore Full Text PDF:/mnt/work-data/weverson/Zotero/storage/B7HQF8YI/Galanis et al. - 2021 - Efficient Deployment of Spiking Neural Networks on.pdf:application/pdf},
}

@article{furber_spinnaker_2014,
	title = {The {SpiNNaker} {Project}},
	volume = {102},
	issn = {1558-2256},
	doi = {10.1109/JPROC.2014.2304638},
	abstract = {The spiking neural network architecture (SpiNNaker) project aims to deliver a massively parallel million-core computer whose interconnect architecture is inspired by the connectivity characteristics of the mammalian brain, and which is suited to the modeling of large-scale spiking neural networks in biological real time. Specifically, the interconnect allows the transmission of a very large number of very small data packets, each conveying explicitly the source, and implicitly the time, of a single neural action potential or “spike.” In this paper, we review the current state of the project, which has already delivered systems with up to 2500 processors, and present the real-time event-driven programming model that supports flexible access to the resources of the machine and has enabled its use by a wide range of collaborators around the world.},
	number = {5},
	journal = {Proceedings of the IEEE},
	author = {Furber, Steve B. and Galluppi, Francesco and Temple, Steve and Plana, Luis A.},
	month = may,
	year = {2014},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Neuroscience, Brain modeling, Computational modeling, Computer architecture, neural network hardware, multicast algorithms, multiprocessor interconnection networks, Multitasking, Neural networks, parallel programming, Parallel programming, Program processors},
	pages = {652--665},
	file = {IEEE Xplore Full Text PDF:/mnt/work-data/weverson/Zotero/storage/RNHDAW8M/Furber et al. - 2014 - The SpiNNaker Project.pdf:application/pdf},
}

@inproceedings{indiveri_spike-based_2007,
	title = {Spike-based learning in {VLSI} networks of integrate-and-fire neurons},
	doi = {10.1109/ISCAS.2007.378290},
	abstract = {As the number of VLSI implementations of spike-based neural networks is steadily increasing, and the development of spike-based multi-chip systems is becoming more popular it is important to design spike-based learning algorithms and circuits, compatible with existing solutions, that endow these systems with adaptation and classification capabilities. We propose a spike-based learning algorithm that is highly effective in classifying complex patterns in semi-supervised fashion, and present neuromorphic circuits that support its VLSI implementation. We describe the architecture of a spike-based learning neural network, the analog circuits that implement the synaptic learning mechanism, and present results from a prototype VLSI chip comprising a full network of integrate-and-fire neurons and plastic synapses. We demonstrate how the VLSI circuits proposed reproduce the learning model's properties and fulfil its basic requirements for classifying complex patterns of mean firing rates.},
	booktitle = {2007 {IEEE} {International} {Symposium} on {Circuits} and {Systems}},
	author = {Indiveri, Giacomo and Fusi, Stefano},
	month = may,
	year = {2007},
	note = {ISSN: 2158-1525},
	keywords = {Timing, Neurons, Neural networks, Circuits, Large-scale systems, Learning systems, Protection, Sensor arrays, Silicon, Very large scale integration},
	pages = {3371--3374},
	file = {IEEE Xplore Full Text PDF:/mnt/work-data/weverson/Zotero/storage/DDMHHQVH/Indiveri e Fusi - 2007 - Spike-based learning in VLSI networks of integrate.pdf:application/pdf},
}

@article{huh_gradient_2017,
	title = {Gradient {Descent} for {Spiking} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1706.04698},
	abstract = {Much of studies on neural computation are based on network models of static neurons that produce analog output, despite the fact that information processing in the brain is predominantly carried out by dynamic neurons that produce discrete pulses called spikes. Research in spike-based computation has been impeded by the lack of efﬁcient supervised learning algorithm for spiking networks. Here, we present a gradient descent method for optimizing spiking network models by introducing a differentiable formulation of spiking networks and deriving the exact gradient calculation. For demonstration, we trained recurrent spiking networks on two dynamic tasks: one that requires optimizing fast (≈ millisecond) spike-based interactions for efﬁcient encoding of information, and a delayed-memory XOR task over extended duration (≈ second). The results show that our method indeed optimizes the spiking network dynamics on the time scale of individual spikes as well as the behavioral time scales. In conclusion, our result offers a general purpose supervised learning algorithm for spiking neural networks, thus advancing further investigations on spike-based computation.},
	language = {en},
	urldate = {2021-10-01},
	journal = {arXiv:1706.04698 [cs, q-bio, stat]},
	author = {Huh, Dongsung and Sejnowski, Terrence J.},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.04698},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
	file = {Huh e Sejnowski - 2017 - Gradient Descent for Spiking Neural Networks.pdf:/mnt/work-data/weverson/Zotero/storage/WTHYTYSC/Huh e Sejnowski - 2017 - Gradient Descent for Spiking Neural Networks.pdf:application/pdf},
}

@article{brette_philosophy_2015,
	title = {Philosophy of the {Spike}: {Rate}-{Based} vs. {Spike}-{Based} {Theories} of the {Brain}},
	volume = {9},
	issn = {1662-5137},
	shorttitle = {Philosophy of the {Spike}},
	url = {https://www.frontiersin.org/article/10.3389/fnsys.2015.00151},
	doi = {10.3389/fnsys.2015.00151},
	abstract = {Does the brain use a firing rate code or a spike timing code? Considering this controversial question from an epistemological perspective, I argue that progress has been hampered by its problematic phrasing. It takes the perspective of an external observer looking at whether those two observables vary with stimuli, and thereby misses the relevant question: which one has a causal role in neural activity? When rephrased in a more meaningful way, the rate-based view appears as an ad hoc methodological postulate, one that is practical but with virtually no empirical or theoretical support.},
	urldate = {2021-10-01},
	journal = {Frontiers in Systems Neuroscience},
	author = {Brette, Romain},
	year = {2015},
	pages = {151},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/XZCCFUA8/Brette - 2015 - Philosophy of the Spike Rate-Based vs. Spike-Base.pdf:application/pdf},
}

@article{teramae_optimal_2012,
	title = {Optimal spike-based communication in excitable networks with strong-sparse and weak-dense links},
	volume = {2},
	copyright = {2012 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/srep00485},
	doi = {10.1038/srep00485},
	abstract = {The connectivity of complex networks and functional implications has been attracting much interest in many physical, biological and social systems. However, the significance of the weight distributions of network links remains largely unknown except for uniformly- or Gaussian-weighted links. Here, we show analytically and numerically, that recurrent neural networks can robustly generate internal noise optimal for spike transmission between neurons with the help of a long-tailed distribution in the weights of recurrent connections. The structure of spontaneous activity in such networks involves weak-dense connections that redistribute excitatory activity over the network as noise sources to optimally enhance the responses of individual neurons to input at sparse-strong connections, thus opening multiple signal transmission pathways. Electrophysiological experiments confirm the importance of a highly broad connectivity spectrum supported by the model. Our results identify a simple network mechanism for internal noise generation by highly inhomogeneous connection strengths supporting both stability and optimal communication.},
	language = {en},
	number = {1},
	urldate = {2021-10-01},
	journal = {Scientific Reports},
	author = {Teramae, Jun-nosuke and Tsubo, Yasuhiro and Fukai, Tomoki},
	month = jul,
	year = {2012},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_y
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Physics;Statistical physics, thermodynamics and nonlinear dynamics;Synaptic transmission
Subject\_term\_id: physics;statistical-physics-thermodynamics-and-nonlinear-dynamics;synaptic-transmission},
	pages = {485},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/T6CHXXA5/Teramae et al. - 2012 - Optimal spike-based communication in excitable net.pdf:application/pdf},
}

@article{lee_enabling_2020,
	title = {Enabling {Spike}-{Based} {Backpropagation} for {Training} {Deep} {Neural} {Network} {Architectures}},
	volume = {14},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/article/10.3389/fnins.2020.00119},
	doi = {10.3389/fnins.2020.00119},
	abstract = {Spiking Neural Networks (SNNs) have recently emerged as a prominent neural computing paradigm. However, the typical shallow SNN architectures have limited capacity for expressing complex representations while training deep SNNs using input spikes has not been successful so far. Diverse methods have been proposed to get around this issue such as converting off-the-shelf trained deep Artificial Neural Networks (ANNs) to SNNs. However, the ANN-SNN conversion scheme fails to capture the temporal dynamics of a spiking system. On the other hand, it is still a difficult problem to directly train deep SNNs using input spike events due to the discontinuous, non-differentiable nature of the spike generation function. To overcome this problem, we propose an approximate derivative method that accounts for the leaky behavior of LIF neurons. This method enables training deep convolutional SNNs directly (with input spike events) using spike-based backpropagation. Our experiments show the effectiveness of the proposed spike-based learning on deep networks (VGG and Residual architectures) by achieving the best classification accuracies in MNIST, SVHN, and CIFAR-10 datasets compared to other SNNs trained with a spike-based learning. Moreover, we analyze sparse event-based computations to demonstrate the efficacy of the proposed SNN training method for inference operation in the spiking domain.},
	urldate = {2021-10-01},
	journal = {Frontiers in Neuroscience},
	author = {Lee, Chankyu and Sarwar, Syed Shakib and Panda, Priyadarshini and Srinivasan, Gopalakrishnan and Roy, Kaushik},
	year = {2020},
	pages = {119},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/U27JIEMK/Lee et al. - 2020 - Enabling Spike-Based Backpropagation for Training .pdf:application/pdf},
}

@article{thorpe_spike-based_2001,
	title = {Spike-based strategies for rapid processing},
	volume = {14},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608001000831},
	doi = {10.1016/S0893-6080(01)00083-1},
	abstract = {Most experimental and theoretical studies of brain function assume that neurons transmit information as a rate code, but recent studies on the speed of visual processing impose temporal constraints that appear incompatible with such a coding scheme. Other coding schemes that use the pattern of spikes across a population a neurons may be much more efficient. For example, since strongly activated neurons tend to fire first, one can use the order of firing as a code. We argue that Rank Order Coding is not only very efficient, but also easy to implement in biological hardware: neurons can be made sensitive to the order of activation of their inputs by including a feed-forward shunting inhibition mechanism that progressively desensitizes the neuronal population during a wave of afferent activity. In such a case, maximum activation will only be produced when the afferent inputs are activated in the order of their synaptic weights.},
	language = {en},
	number = {6},
	urldate = {2021-10-01},
	journal = {Neural Networks},
	author = {Thorpe, Simon and Delorme, Arnaud and Van Rullen, Rufin},
	month = jul,
	year = {2001},
	keywords = {Information, Latency, Rank Order Coding, Rapid visual processing, Retina, Spikes},
	pages = {715--725},
	file = {ScienceDirect Snapshot:/mnt/work-data/weverson/Zotero/storage/XMM4NWNP/S0893608001000831.html:text/html;ScienceDirect Full Text PDF:/mnt/work-data/weverson/Zotero/storage/JZHBCNAD/Thorpe et al. - 2001 - Spike-based strategies for rapid processing.pdf:application/pdf},
}

@article{kaya_large_2018,
	title = {A large electroencephalographic motor imagery dataset for electroencephalographic brain computer interfaces},
	volume = {5},
	copyright = {2018 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata2018211},
	doi = {10.1038/sdata.2018.211},
	abstract = {Recent advancements in brain computer interfaces (BCI) have demonstrated control of robotic systems by mental processes alone. Together with invasive BCI, electroencephalographic (EEG) BCI represent an important direction in the development of BCI systems. In the context of EEG BCI, the processing of EEG data is the key challenge. Unfortunately, advances in that direction have been complicated by a lack of large and uniform datasets that could be used to design and evaluate different data processing approaches. In this work, we release a large set of EEG BCI data collected during the development of a slow cortical potentials-based EEG BCI. The dataset contains 60 h of EEG recordings, 13 participants, 75 recording sessions, 201 individual EEG BCI interaction session-segments, and over 60 000 examples of motor imageries in 4 interaction paradigms. The current dataset presents one of the largest EEG BCI datasets publically available to date.},
	language = {en},
	number = {1},
	urldate = {2021-10-01},
	journal = {Scientific Data},
	author = {Kaya, Murat and Binli, Mustafa Kemal and Ozbay, Erkan and Yanar, Hilmi and Mishchenko, Yuriy},
	month = oct,
	year = {2018},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_publicdomain
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Biomedical engineering;Computational neuroscience
Subject\_term\_id: biomedical-engineering;computational-neuroscience},
	pages = {180211},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/WDWSQS7A/Kaya et al. - 2018 - A large electroencephalographic motor imagery data.pdf:application/pdf},
}

@article{okamoto_three-dimensional_2004,
	title = {Three-dimensional probabilistic anatomical cranio-cerebral correlation via the international 10–20 system oriented for transcranial functional brain mapping},
	volume = {21},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811903005366},
	doi = {10.1016/j.neuroimage.2003.08.026},
	abstract = {The recent advent of multichannel near-infrared spectroscopy (NIRS) has expanded its technical potential for human brain mapping. However, NIRS measurement has a technical drawback in that it measures cortical activities from the head surface without anatomical information of the object to be measured. This problem is also found in transcranial magnetic stimulation (TMS) that transcranially activates or inactivates the cortical surface. To overcome this drawback, we examined cranio-cerebral correlation using magnetic resonance imaging (MRI) via the guidance of the international 10–20 system for electrode placement, which had originally been developed for electroencephalography. We projected the 10–20 standard cranial positions over the cerebral cortical surface. After examining the cranio-cerebral correspondence for 17 healthy adults, we normalized the 10–20 cortical projection points of the subjects to the standard Montreal Neurological Institute (MNI) and Talairach stereotactic coordinates and obtained their probabilistic distributions. We also expressed the anatomical structures for the 10–20 cortical projection points probabilistically. Next, we examined the distance between the cortical surface and the head surface along the scalp and created a cortical surface depth map. We found that the locations of 10–20 cortical projection points in the standard MNI or Talairach space could be estimated with an average standard deviation of 8 mm. This study provided an initial step toward establishing a three-dimensional probabilistic anatomical platform that enables intra- and intermodal comparisons of NIRS and TMS brain imaging data.},
	language = {en},
	number = {1},
	urldate = {2021-10-01},
	journal = {NeuroImage},
	author = {Okamoto, Masako and Dan, Haruka and Sakamoto, Kuniko and Takeo, Kazuhiro and Shimizu, Koji and Kohno, Satoru and Oda, Ichiro and Isobe, Seiichiro and Suzuki, Tateo and Kohyama, Kaoru and Dan, Ippeita},
	month = jan,
	year = {2004},
	keywords = {Human brain mapping, Near-infrared spectroscopy, Probabilistic anatomical platform, Transcranial magnetic stimulation},
	pages = {99--111},
	file = {ScienceDirect Full Text PDF:/mnt/work-data/weverson/Zotero/storage/YLWKEJP5/Okamoto et al. - 2004 - Three-dimensional probabilistic anatomical cranio-.pdf:application/pdf;ScienceDirect Snapshot:/mnt/work-data/weverson/Zotero/storage/VZQJ9RWA/S1053811903005366.html:text/html},
}

@article{boerlin_spike-based_2011,
	title = {Spike-{Based} {Population} {Coding} and {Working} {Memory}},
	volume = {7},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1001080},
	doi = {10.1371/journal.pcbi.1001080},
	abstract = {Compelling behavioral evidence suggests that humans can make optimal decisions despite the uncertainty inherent in perceptual or motor tasks. A key question in neuroscience is how populations of spiking neurons can implement such probabilistic computations. In this article, we develop a comprehensive framework for optimal, spike-based sensory integration and working memory in a dynamic environment. We propose that probability distributions are inferred spike-per-spike in recurrently connected networks of integrate-and-fire neurons. As a result, these networks can combine sensory cues optimally, track the state of a time-varying stimulus and memorize accumulated evidence over periods much longer than the time constant of single neurons. Importantly, we propose that population responses and persistent working memory states represent entire probability distributions and not only single stimulus values. These memories are reflected by sustained, asynchronous patterns of activity which make relevant information available to downstream neurons within their short time window of integration. Model neurons act as predictive encoders, only firing spikes which account for new information that has not yet been signaled. Thus, spike times signal deterministically a prediction error, contrary to rate codes in which spike times are considered to be random samples of an underlying firing rate. As a consequence of this coding scheme, a multitude of spike patterns can reliably encode the same information. This results in weakly correlated, Poisson-like spike trains that are sensitive to initial conditions but robust to even high levels of external neural noise. This spike train variability reproduces the one observed in cortical sensory spike trains, but cannot be equated to noise. On the contrary, it is a consequence of optimal spike-based inference. In contrast, we show that rate-based models perform poorly when implemented with stochastically spiking neurons.},
	language = {en},
	number = {2},
	urldate = {2021-10-01},
	journal = {PLOS Computational Biology},
	author = {Boerlin, Martin and Denève, Sophie},
	month = feb,
	year = {2011},
	note = {Publisher: Public Library of Science},
	keywords = {Sensory perception, Memory, Working memory, Neurons, Neural networks, Action potentials, Neuronal tuning, Sensory cues},
	pages = {e1001080},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/YNGVFNJS/Boerlin e Denève - 2011 - Spike-Based Population Coding and Working Memory.pdf:application/pdf;Snapshot:/mnt/work-data/weverson/Zotero/storage/J3FNM6DA/article.html:text/html},
}

@article{tully_spike-based_2016,
	title = {Spike-{Based} {Bayesian}-{Hebbian} {Learning} of {Temporal} {Sequences}},
	volume = {12},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004954},
	doi = {10.1371/journal.pcbi.1004954},
	abstract = {Many cognitive and motor functions are enabled by the temporal representation and processing of stimuli, but it remains an open issue how neocortical microcircuits can reliably encode and replay such sequences of information. To better understand this, a modular attractor memory network is proposed in which meta-stable sequential attractor transitions are learned through changes to synaptic weights and intrinsic excitabilities via the spike-based Bayesian Confidence Propagation Neural Network (BCPNN) learning rule. We find that the formation of distributed memories, embodied by increased periods of firing in pools of excitatory neurons, together with asymmetrical associations between these distinct network states, can be acquired through plasticity. The model’s feasibility is demonstrated using simulations of adaptive exponential integrate-and-fire model neurons (AdEx). We show that the learning and speed of sequence replay depends on a confluence of biophysically relevant parameters including stimulus duration, level of background noise, ratio of synaptic currents, and strengths of short-term depression and adaptation. Moreover, sequence elements are shown to flexibly participate multiple times in the sequence, suggesting that spiking attractor networks of this type can support an efficient combinatorial code. The model provides a principled approach towards understanding how multiple interacting plasticity mechanisms can coordinate hetero-associative learning in unison.},
	language = {en},
	number = {5},
	urldate = {2021-10-01},
	journal = {PLOS Computational Biology},
	author = {Tully, Philip J. and Lindén, Henrik and Hennig, Matthias H. and Lansner, Anders},
	month = may,
	year = {2016},
	note = {Publisher: Public Library of Science},
	keywords = {Learning, Synapses, Neurons, Neural networks, Action potentials, Memory recall, Neuronal plasticity, Synaptic plasticity},
	pages = {e1004954},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/8LARUG83/Tully et al. - 2016 - Spike-Based Bayesian-Hebbian Learning of Temporal .pdf:application/pdf},
}

@article{heusser_episodic_2016,
	title = {Episodic sequence memory is supported by a theta–gamma phase code},
	volume = {19},
	copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn.4374},
	doi = {10.1038/nn.4374},
	abstract = {A core aspect of human episodic memory is the ability to recall events in the order that they were experienced. The authors found that successful memory for order is related to the precise timing of high frequency brain activity with respect to slower underlying rhythms.},
	language = {en},
	number = {10},
	urldate = {2021-10-01},
	journal = {Nature Neuroscience},
	author = {Heusser, Andrew C. and Poeppel, David and Ezzyat, Youssef and Davachi, Lila},
	month = oct,
	year = {2016},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 10
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Cognitive neuroscience;Learning and memory;Psychology
Subject\_term\_id: cognitive-neuroscience;learning-and-memory;psychology},
	pages = {1374--1380},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/HQ87DS3H/Heusser et al. - 2016 - Episodic sequence memory is supported by a theta–g.pdf:application/pdf},
}

@article{neymotin_ketamine_2011,
	title = {Ketamine {Disrupts} {Theta} {Modulation} of {Gamma} in a {Computer} {Model} of {Hippocampus}},
	volume = {31},
	copyright = {Copyright © 2011 the authors 0270-6474/11/3111733-11\$15.00/0},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/31/32/11733},
	doi = {10.1523/JNEUROSCI.0501-11.2011},
	abstract = {Abnormalities in oscillations have been suggested to play a role in schizophrenia. We studied theta-modulated gamma oscillations in a computer model of hippocampal CA3 in vivo with and without simulated application of ketamine, an NMDA receptor antagonist and psychotomimetic. Networks of 1200 multicompartment neurons [pyramidal, basket, and oriens-lacunosum moleculare (OLM) cells] generated theta and gamma oscillations from intrinsic network dynamics: basket cells primarily generated gamma and amplified theta, while OLM cells strongly contributed to theta. Extrinsic medial septal inputs paced theta and amplified both theta and gamma oscillations. Exploration of NMDA receptor reduction across all location combinations demonstrated that the experimentally observed ketamine effect occurred only with isolated reduction of NMDA receptors on OLMs. In the ketamine simulations, lower OLM activity reduced theta power and disinhibited pyramidal cells, resulting in increased basket cell activation and gamma power. Our simulations predict the following: (1) ketamine increases firing rates; (2) oscillations can be generated by intrinsic hippocampal circuits; (3) medial-septum inputs pace and augment oscillations; (4) pyramidal cells lead basket cells at the gamma peak but lag at trough; (5) basket cells amplify theta rhythms; (6) ketamine alters oscillations due to primary blockade at OLM NMDA receptors; (7) ketamine alters phase relationships of cell firing; (8) ketamine reduces network responsivity to the environment; (9) ketamine effect could be reversed by providing a continuous inward current to OLM cells. We suggest that this last prediction has implications for a possible novel treatment for cognitive deficits of schizophrenia by targeting OLM cells.},
	language = {en},
	number = {32},
	urldate = {2021-10-01},
	journal = {Journal of Neuroscience},
	author = {Neymotin, Samuel A. and Lazarewicz, Maciej T. and Sherif, Mohamed and Contreras, Diego and Finkel, Leif H. and Lytton, William W.},
	month = aug,
	year = {2011},
	pmid = {21832203},
	note = {Publisher: Society for Neuroscience
Section: Articles},
	pages = {11733--11743},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/2YJFAKU7/Neymotin et al. - 2011 - Ketamine Disrupts Theta Modulation of Gamma in a C.pdf:application/pdf},
}

@article{ratcliff_diffusion_2016,
	title = {Diffusion {Decision} {Model}: {Current} {Issues} and {History}},
	volume = {20},
	issn = {1364-6613},
	shorttitle = {Diffusion {Decision} {Model}},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661316000255},
	doi = {10.1016/j.tics.2016.01.007},
	abstract = {There is growing interest in diffusion models to represent the cognitive and neural processes of speeded decision making. Sequential-sampling models like the diffusion model have a long history in psychology. They view decision making as a process of noisy accumulation of evidence from a stimulus. The standard model assumes that evidence accumulates at a constant rate during the second or two it takes to make a decision. This process can be linked to the behaviors of populations of neurons and to theories of optimality. Diffusion models have been used successfully in a range of cognitive tasks and as psychometric tools in clinical research to examine individual differences. In this review, we relate the models to both earlier and more recent research in psychology.},
	language = {en},
	number = {4},
	urldate = {2021-10-01},
	journal = {Trends in Cognitive Sciences},
	author = {Ratcliff, Roger and Smith, Philip L. and Brown, Scott D. and McKoon, Gail},
	month = apr,
	year = {2016},
	keywords = {diffusion model, nonstationarity, optimality, response time},
	pages = {260--281},
	file = {ScienceDirect Full Text PDF:/mnt/work-data/weverson/Zotero/storage/X4M6I8J2/Ratcliff et al. - 2016 - Diffusion Decision Model Current Issues and Histo.pdf:application/pdf},
}

@article{herz_modeling_2006,
	title = {Modeling {Single}-{Neuron} {Dynamics} and {Computations}: {A} {Balance} of {Detail} and {Abstraction}},
	volume = {314},
	shorttitle = {Modeling {Single}-{Neuron} {Dynamics} and {Computations}},
	url = {https://www.science.org/doi/10.1126/science.1127240},
	doi = {10.1126/science.1127240},
	number = {5796},
	urldate = {2021-10-01},
	journal = {Science},
	author = {Herz, Andreas V. M. and Gollisch, Tim and Machens, Christian K. and Jaeger, Dieter},
	month = oct,
	year = {2006},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {80--85},
	file = {Herz et al. - 2006 - Modeling Single-Neuron Dynamics and Computations .pdf:/mnt/work-data/weverson/Zotero/storage/7LSPDVAL/Herz et al. - 2006 - Modeling Single-Neuron Dynamics and Computations .pdf:application/pdf},
}

@article{prat-ortega_flexible_2021,
	title = {Flexible categorization in perceptual decision making},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-21501-z},
	doi = {10.1038/s41467-021-21501-z},
	abstract = {Perceptual decisions rely on accumulating sensory evidence. This computation has been studied using either drift diffusion models or neurobiological network models exhibiting winner-take-all attractor dynamics. Although both models can account for a large amount of data, it remains unclear whether their dynamics are qualitatively equivalent. Here we show that in the attractor model, but not in the drift diffusion model, an increase in the stimulus fluctuations or the stimulus duration promotes transitions between decision states. The increase in the number of transitions leads to a crossover between weighting mostly early evidence (primacy) to weighting late evidence (recency), a prediction we validate with psychophysical data. Between these two limiting cases, we found a novel flexible categorization regime, in which fluctuations can reverse initially-incorrect categorizations. This reversal asymmetry results in a non-monotonic psychometric curve, a distinctive feature of the attractor model. Our findings point to correcting decision reversals as an important feature of perceptual decision making.},
	language = {en},
	number = {1},
	urldate = {2021-10-01},
	journal = {Nature Communications},
	author = {Prat-Ortega, Genís and Wimmer, Klaus and Roxin, Alex and de la Rocha, Jaime},
	month = feb,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Cognitive neuroscience;Computational neuroscience;Network models;Neural circuits;Sensory processing
Subject\_term\_id: cognitive-neuroscience;computational-neuroscience;network-models;neural-circuit;sensory-processing},
	pages = {1283},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/HPWDR37R/Prat-Ortega et al. - 2021 - Flexible categorization in perceptual decision mak.pdf:application/pdf},
}

@book{miller_introductory_2018,
	address = {Cambridge, MA},
	series = {Computational neuroscience series},
	title = {An {Introductory} {Course} in {Computational} {Neuroscience}},
	copyright = {2018 Massachussets Institute of Technology},
	isbn = {978-0-262-03825-6},
	url = {https://mitpress.ublish.com/book/intro-computational-neuroscience},
	language = {en},
	urldate = {2021-10-05},
	publisher = {The MIT PRess},
	author = {Miller, Paul},
	year = {2018},
	file = {An Introductory Course in Computational Neuroscience | The MIT Press:/mnt/work-data/weverson/Zotero/storage/4WE73PLQ/intro-computational-neuroscience.html:text/html},
}

@book{adkins_ordinary_2012,
	address = {New York},
	edition = {1},
	series = {Undergraduate {Texts} in {Mathematics}},
	title = {Ordinary {Differential} {Equations}},
	copyright = {Springer Science+Business Media, LLC, part of Springer Nature},
	isbn = {978-1-4614-3618-8},
	url = {https://www.springer.com/gp/book/9781461436171},
	abstract = {Unlike most texts in differential equations, this textbook gives an early presentation of the Laplace transform, which is then used to motivate and develop many of the remaining differential equation concepts for which it is particularly well suited. For example, the standard solution methods for constant coefficient linear differential equations are immediate and simplified, and solution methods for constant coefficient systems are streamlined. By introducing the Laplace transform early in the text, students become proficient in its use while at the same time learning the standard topics in differential equations. The text also includes proofs of several important theorems that are not usually given in introductory texts. These include a proof of the injectivity of the Laplace transform and a proof of the existence and uniqueness theorem for linear constant coefficient differential equations.Along with its unique traits, this text contains all the topics needed for a standard three- or four-hour, sophomore-level differential equations course for students majoring in science or engineering. These topics include: first order differential equations, general linear differential equations with constant coefficients, second order linear differential equations with variable coefficients, power series methods, and linear systems of differential equations. It is assumed that the reader has had the equivalent of a one-year course in college calculus.},
	language = {en},
	urldate = {2021-10-05},
	publisher = {Springer-Verlag},
	author = {Adkins, William and Davidson, Mark G.},
	year = {2012},
	doi = {10.1007/978-1-4614-3618-8},
	file = {Snapshot:/mnt/work-data/weverson/Zotero/storage/RCGSFG8B/9781461436171.html:text/html},
}

@book{montgomery_estatistica_2018,
	address = {Rio de Janeiro},
	edition = {6},
	title = {Estatística aplicada e probabilidade para engenheiros},
	copyright = {LTC - Livros Técnicos e Científicos Editora Ltda.},
	isbn = {978-85-216-3241-2},
	language = {pt-br},
	publisher = {LTC},
	author = {Montgomery, Douglas C. and Runger, George C.},
	year = {2018},
}

@incollection{moschovakis_what_2001,
	address = {Berlin, Heidelberg},
	title = {What {Is} an {Algorithm}?},
	isbn = {978-3-642-56478-9},
	url = {https://doi.org/10.1007/978-3-642-56478-9_46},
	abstract = {When algorithms are defined rigorously in Computer Science literature (which only happens rarely), they are generally identified with abstract machines, mathematical models of computers, sometimes idealized by allowing access to “unbounded memory”.1 My aims here are to argue that this does not square with our intuitions about algorithms and the way we interpret and apply results about them; to promote the problem of defining algorithms correctly; and to describe briefly a plausible solution, by which algorithms are recursive definitions while machines model implementations, a special kind of algorithms.},
	language = {en},
	urldate = {2021-10-05},
	booktitle = {Mathematics {Unlimited} — 2001 and {Beyond}},
	publisher = {Springer},
	author = {Moschovakis, Yiannis N.},
	editor = {Engquist, Björn and Schmid, Wilfried},
	year = {2001},
	doi = {10.1007/978-3-642-56478-9_46},
	pages = {919--936},
}

@article{yuste_cortex_2005,
	title = {The cortex as a central pattern generator},
	volume = {6},
	copyright = {2005 Nature Publishing Group},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/nrn1686},
	doi = {10.1038/nrn1686},
	abstract = {Vertebrate spinal cord and brainstem central pattern generator (CPG) circuits share profound similarities with neocortical circuits. CPGs can produce meaningful functional output in the absence of sensory inputs. Neocortical circuits could be considered analogous to CPGs as they have rich spontaneous dynamics that, similar to CPGs, are powerfully modulated or engaged by sensory inputs, but can also generate output in their absence. We find compelling evidence for this argument at the anatomical, biophysical, developmental, dynamic and pathological levels of analysis. Although it is possible that cortical circuits are particularly plastic types of CPG ('learning CPGs'), we argue that present knowledge about CPGs is likely to foretell the basic principles of the organization and dynamic function of cortical circuits.},
	language = {en},
	number = {6},
	urldate = {2021-10-05},
	journal = {Nature Reviews Neuroscience},
	author = {Yuste, Rafael and MacLean, Jason N. and Smith, Jeffrey and Lansner, Anders},
	month = jun,
	year = {2005},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 6
Primary\_atype: Reviews
Publisher: Nature Publishing Group},
	pages = {477--483},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/VXKD82QN/Yuste et al. - 2005 - The cortex as a central pattern generator.pdf:application/pdf;Snapshot:/mnt/work-data/weverson/Zotero/storage/6PFR9FBZ/nrn1686.html:text/html},
}

@article{roy_towards_2019,
	title = {Towards spike-based machine intelligence with neuromorphic computing},
	volume = {575},
	copyright = {2019 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1677-2},
	doi = {10.1038/s41586-019-1677-2},
	abstract = {Guided by brain-like ‘spiking’ computational frameworks, neuromorphic computing—brain-inspired computing for machine intelligence—promises to realize artificial intelligence while reducing the energy requirements of computing platforms. This interdisciplinary field began with the implementation of silicon circuits for biological neural routines, but has evolved to encompass the hardware implementation of algorithms with spike-based encoding and event-driven representations. Here we provide an overview of the developments in neuromorphic computing for both algorithms and hardware and highlight the fundamentals of learning and hardware frameworks. We discuss the main challenges and the future prospects of neuromorphic computing, with emphasis on algorithm–hardware codesign.},
	language = {en},
	number = {7784},
	urldate = {2021-10-05},
	journal = {Nature},
	author = {Roy, Kaushik and Jaiswal, Akhilesh and Panda, Priyadarshini},
	month = nov,
	year = {2019},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 7784
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Electrical and electronic engineering;Nanoscience and technology
Subject\_term\_id: electrical-and-electronic-engineering;nanoscience-and-technology},
	pages = {607--617},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/GREZG9WP/Roy et al. - 2019 - Towards spike-based machine intelligence with neur.pdf:application/pdf;Snapshot:/mnt/work-data/weverson/Zotero/storage/5R8X9XAH/s41586-019-1677-2.html:text/html},
}

@article{syahid_simulation_2021,
	title = {Simulation of spiking activities neuron models using the {Euler} method},
	volume = {1951},
	issn = {1742-6596},
	url = {https://doi.org/10.1088/1742-6596/1951/1/012065},
	doi = {10.1088/1742-6596/1951/1/012065},
	abstract = {Simulation of neuron spiking activity models has been carried out using the Euler method. This study aims to simulate spiking activity in a neuron model. The neuron model used is the Hodgkin-Huxley neuron model, Integrate and Fire neuron model, Wilson neuron model, and Izhikevich neuron model. The research was conducted by implementing the mathematical equations of each neuron model used and then recording the membrane potential changes from time to time using the Euler method in MATLAB. The different forms of spiking activity were done by varying the variable’s value in each mathematical equation of a neuron model that describes the processing of action potentials (spikes) influenced by ion channel activity. The results showed that the Integrate and Fire neuron models produce regular spiking (RS), Hodgkin-Huxley neuron models have regular spiking (RS) forms, Wilson neuron models produce regular spiking (RS), fast-spiking (FS), and intrinsic bursting (IB), Izhikevich neuron model produces regular spiking (RS), fast-spiking (FS), intrinsic bursting (IB), chattering neurons (CH), and low threshold spiking (LTS). The complexity of the variables used and the spiking activity generated by each neuron model can provide an overview of computational efficiency and proximity to actual biological neurons.},
	language = {en},
	number = {1},
	urldate = {2021-10-05},
	journal = {Journal of Physics: Conference Series},
	author = {Syahid, A. and Yuniati, A.},
	month = jun,
	year = {2021},
	note = {Publisher: IOP Publishing},
	pages = {012065},
	file = {IOP Full Text PDF:/mnt/work-data/weverson/Zotero/storage/WXEHRCFW/Syahid e Yuniati - 2021 - Simulation of spiking activities neuron models usi.pdf:application/pdf},
}

@book{carnevale_neuron_2006,
	address = {Cambridge},
	title = {The {NEURON} {Book}},
	isbn = {978-0-521-84321-8},
	url = {https://www.cambridge.org/core/books/neuron-book/7C8D9BD861D288E658BEB652F593F273},
	abstract = {The authoritative reference on NEURON, the simulation environment for modeling biological neurons and neural networks that enjoys wide use in the experimental and computational neuroscience communities. This book shows how to use NEURON to construct and apply empirically based models. Written primarily for neuroscience investigators, teachers, and students, it assumes no previous knowledge of computer programming or numerical methods. Readers with a background in the physical sciences or mathematics, who have some knowledge about brain cells and circuits and are interested in computational modeling, will also find it helpful. The NEURON Book covers material that ranges from the inner workings of this program, to practical considerations involved in specifying the anatomical and biophysical properties that are to be represented in models. It uses a problem-solving approach, with many working examples that readers can try for themselves.},
	urldate = {2021-10-05},
	publisher = {Cambridge University Press},
	author = {Carnevale, Nicholas T. and Hines, Michael L.},
	year = {2006},
	doi = {10.1017/CBO9780511541612},
	file = {Snapshot:/mnt/work-data/weverson/Zotero/storage/UJPFJVKF/7C8D9BD861D288E658BEB652F593F273.html:text/html},
}

@article{stimberg_brian_2019,
	title = {Brian 2, an intuitive and efficient neural simulator},
	volume = {8},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.47314},
	doi = {10.7554/eLife.47314},
	abstract = {Brian 2 allows scientists to simply and efficiently simulate spiking neural network models. These models can feature novel dynamical equations, their interactions with the environment, and experimental protocols. To preserve high performance when defining new models, most simulators offer two options: low-level programming or description languages. The first option requires expertise, is prone to errors, and is problematic for reproducibility. The second option cannot describe all aspects of a computational experiment, such as the potentially complex logic of a stimulation protocol. Brian addresses these issues using runtime code generation. Scientists write code with simple and concise high-level descriptions, and Brian transforms them into efficient low-level code that can run interleaved with their code. We illustrate this with several challenging examples: a plastic model of the pyloric network, a closed-loop sensorimotor model, a programmatic exploration of a neuron model, and an auditory model with real-time input.},
	urldate = {2021-10-05},
	journal = {eLife},
	author = {Stimberg, Marcel and Brette, Romain and Goodman, Dan FM},
	editor = {Skinner, Frances K and Calabrese, Ronald L and Skinner, Frances K and Zeldenrust, Fleur and Gerkin, Richard C},
	month = aug,
	year = {2019},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {computational neuroscience, simulation, software},
	pages = {e47314},
	file = {Full Text PDF:/mnt/work-data/weverson/Zotero/storage/48RJHJ9E/Stimberg et al. - 2019 - Brian 2, an intuitive and efficient neural simulat.pdf:application/pdf},
}
